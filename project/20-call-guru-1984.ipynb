{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d2ab9a-c74b-49d4-9575-f2c322f99d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:28.238511Z",
     "iopub.status.busy": "2024-12-22T22:35:28.238329Z",
     "iopub.status.idle": "2024-12-22T22:35:28.246238Z",
     "shell.execute_reply": "2024-12-22T22:35:28.245639Z",
     "shell.execute_reply.started": "2024-12-22T22:35:28.238491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e0fce24-f1db-4f5a-ab5e-69e660e58d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:52.577194Z",
     "iopub.status.busy": "2024-12-22T22:35:52.576949Z",
     "iopub.status.idle": "2024-12-22T22:35:52.582481Z",
     "shell.execute_reply": "2024-12-22T22:35:52.581466Z",
     "shell.execute_reply.started": "2024-12-22T22:35:52.577171Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*use_reentrant parameter should be passed explicitly.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\",\n",
    ")\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a54cb76-21db-4e02-a4e0-db0d6d088db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:53.197328Z",
     "iopub.status.busy": "2024-12-22T22:35:53.197124Z",
     "iopub.status.idle": "2024-12-22T22:35:53.203673Z",
     "shell.execute_reply": "2024-12-22T22:35:53.203118Z",
     "shell.execute_reply.started": "2024-12-22T22:35:53.197312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.3\n",
      "peft version: 0.12.0\n",
      "accelerate version: 0.34.2\n",
      "datasets version: 2.21.0\n",
      "trl version: 0.10.1\n",
      "transformers version: 4.46.1\n",
      "Device name: 'NVIDIA GeForce RTX 4060 Ti'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Ti', major=8, minor=9, total_memory=16059MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(\n",
    "    f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\"\n",
    ")\n",
    "print(\n",
    "    \"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f831d8d-8c62-4334-979e-373b87a632e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:53.738001Z",
     "iopub.status.busy": "2024-12-22T22:35:53.737055Z",
     "iopub.status.idle": "2024-12-22T22:35:54.464243Z",
     "shell.execute_reply": "2024-12-22T22:35:54.462434Z",
     "shell.execute_reply.started": "2024-12-22T22:35:53.737924Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, PeftModel, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d88dfac-07e1-4471-a7c0-99d2491baacf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:54.468056Z",
     "iopub.status.busy": "2024-12-22T22:35:54.467165Z",
     "iopub.status.idle": "2024-12-22T22:35:54.475253Z",
     "shell.execute_reply": "2024-12-22T22:35:54.474741Z",
     "shell.execute_reply.started": "2024-12-22T22:35:54.468001Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages, max_new_tokens=2000, **kwargs):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].to(\n",
    "            model_inputs[\"input_ids\"].device\n",
    "        )\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs[\"attention_mask\"],\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b041d8-6c65-464b-9687-d08afb9d5065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:54.697917Z",
     "iopub.status.busy": "2024-12-22T22:35:54.696278Z",
     "iopub.status.idle": "2024-12-22T22:35:54.710624Z",
     "shell.execute_reply": "2024-12-22T22:35:54.709015Z",
     "shell.execute_reply.started": "2024-12-22T22:35:54.697839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    # compute_dtype = torch.bfloat16\n",
    "    # attn_implementation = \"flash_attention_2\"\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    # attn_implementation = 'eager'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f8d8dd-6ae1-463f-8478-0bf1df3e2846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:35:55.536957Z",
     "iopub.status.busy": "2024-12-22T22:35:55.536101Z",
     "iopub.status.idle": "2024-12-22T22:36:01.763437Z",
     "shell.execute_reply": "2024-12-22T22:36:01.762885Z",
     "shell.execute_reply.started": "2024-12-22T22:35:55.536884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab53b2c0876c41889d4d9677146d6d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c375f206fdce45178a119ad760dfb08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/guru1984-v2\"\n",
    "commit_hash = None\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    revision=commit_hash,\n",
    ")\n",
    "\n",
    "# # adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=commit_hash)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = set_tokenizer(model_id)\n",
    "\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84943477-803b-4d05-8951-c4f70b1ce68f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:36:01.764467Z",
     "iopub.status.busy": "2024-12-22T22:36:01.764290Z",
     "iopub.status.idle": "2024-12-22T22:36:02.589665Z",
     "shell.execute_reply": "2024-12-22T22:36:02.589234Z",
     "shell.execute_reply.started": "2024-12-22T22:36:01.764454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O personagem principal é Winston.\n",
      "CPU times: user 776 ms, sys: 39.9 ms, total: 816 ms\n",
      "Wall time: 822 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "context = \"\"\"\n",
    "Quem é o personagem principal?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"content\": f\"{context}\", \"role\": \"user\"},\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    result = llm.generate(messages, temperature=0.01)\n",
    "\n",
    "\n",
    "try:\n",
    "    print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))\n",
    "except:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972d3f4-4dca-4da0-aa3e-3221dfbc5f82",
   "metadata": {},
   "source": [
    "# Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "741df4c1-9cfb-41c1-81c9-43cc3f2df970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:36:47.472701Z",
     "iopub.status.busy": "2024-09-14T04:36:47.472171Z",
     "iopub.status.idle": "2024-09-14T04:36:47.475220Z",
     "shell.execute_reply": "2024-09-14T04:36:47.474780Z",
     "shell.execute_reply.started": "2024-09-14T04:36:47.472679Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Descreva resumidamente como era a organização social em 1984. Explique o papel do indivíduo e do Estado nessa configuração social.\",\n",
    "    \"Escolha um personagem da ficção que tenha lhe marcado e descreva-o em profundidade, ressaltando características psicológicas e físicas. Explique o porquê da escolha.\",\n",
    "    \"Explique a finalidade das permanentes guerras entre os continentes em 1984.\",\n",
    "    \"Reflita sobre  o  papel  da  Novafala.  Por  que  a  linguagem  era  constantemente  alterada e  o  que  essa alteração provocava?\",\n",
    "    \"Como se davam as relações familiares e românticas em 1984?  Por que, na sua  opinião, as relações eram desta forma?\",\n",
    "    \"Imagine que o jornal de sua cidade, descobrindo que você leu a obra de Orwell, tenha lhe pedido uma apreciação crítica. Dessa forma, em 15 a 20 linhas escreva por que a obra merece (ou não) ser lida e no  que  a  sua  leitura  pode  auxiliar  para  entender  a  vida  social  atual  (faça  relações  com  o  mundo contemporâneo).  Não  se  esqueça  de  mencionar,  em  linhas  bem  gerais,  as  principais  temáticas abordadas na obra.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02485abf-875e-46a8-9ee6-cf64d2b0d7f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:39:27.445894Z",
     "iopub.status.busy": "2024-09-14T04:39:27.445709Z",
     "iopub.status.idle": "2024-09-14T04:39:41.752236Z",
     "shell.execute_reply": "2024-09-14T04:39:41.751768Z",
     "shell.execute_reply.started": "2024-09-14T04:39:27.445879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Descreva resumidamente como era a organização social em 1984. Explique o papel do indivíduo e do Estado nessa configuração social."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: A sociedade era estruturada em torno de um indivíduo que era o centro de todas as atividades, e o Partido era o único poder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Escolha um personagem da ficção que tenha lhe marcado e descreva-o em profundidade, ressaltando características psicológicas e físicas. Explique o porquê da escolha."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: Eu escolhi Winston, porque ele é um personagem que se destacou na minha memória."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Explique a finalidade das permanentes guerras entre os continentes em 1984."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: As guerras entre os continentes eram apenas uma parte da guerra geral, que incluía a guerra interna, a guerra psicológica e a guerra de informações."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Reflita sobre  o  papel  da  Novafala.  Por  que  a  linguagem  era  constantemente  alterada e  o  que  essa alteração provocava?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: A linguagem era alterada constantemente, e isso provocava confusão e dificultava a compreensão."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Como se davam as relações familiares e românticas em 1984?  Por que, na sua  opinião, as relações eram desta forma?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: As relações eram de amor, mas não de amor verdadeiro."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Pergunta**: Imagine que o jornal de sua cidade, descobrindo que você leu a obra de Orwell, tenha lhe pedido uma apreciação crítica. Dessa forma, em 15 a 20 linhas escreva por que a obra merece (ou não) ser lida e no  que  a  sua  leitura  pode  auxiliar  para  entender  a  vida  social  atual  (faça  relações  com  o  mundo contemporâneo).  Não  se  esqueça  de  mencionar,  em  linhas  bem  gerais,  as  principais  temáticas abordadas na obra."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resposta**: A obra de Orwell merece ser lida porque é um comentário profundo sobre a vida social atual, que é caracterizada por uma hierarquia de castas, uma guerra constante e uma sociedade onde a maioria é submetida a trabalho árduo e privação. A leitura pode ajudar a entender essas realidades e a se questionar sobre a justificação dessas estruturas sociais."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "for q in questions:\n",
    "    r = llm.generate([{\"content\": f\"{q}\", \"role\": \"user\"}], temperature=0.01)\n",
    "    display(Markdown(f\"---\"))\n",
    "    display(Markdown(f\"**Pergunta**: {q}\"))\n",
    "    display(Markdown(f\"**Resposta**: {r}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43681518-e158-4c85-a555-45fa9e26668b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
