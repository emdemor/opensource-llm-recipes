{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3542f3a-9559-4f28-bea4-ab41a74d7b05",
   "metadata": {},
   "source": [
    "# Natural Language Processing Demystified | Seq2Seq and Attention\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified<br><br>\n",
    "\n",
    "Course module for this demo: https://www.nlpdemystified.org/course/seq2seq-and-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14acbb-3ad1-4b0a-acaa-6aa75c657f4b",
   "metadata": {},
   "source": [
    "# Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99037dc9-8332-4b56-8f52-d58483b2d219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:06:11.398225Z",
     "iopub.status.busy": "2024-09-16T04:06:11.398037Z",
     "iopub.status.idle": "2024-09-16T04:06:13.206423Z",
     "shell.execute_reply": "2024-09-16T04:06:13.205831Z",
     "shell.execute_reply.started": "2024-09-16T04:06:11.398209Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce6adb3-1c4e-4180-8ab9-a4e10da798d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:01:27.022389Z",
     "iopub.status.busy": "2024-09-16T04:01:27.021944Z",
     "iopub.status.idle": "2024-09-16T04:01:27.067387Z",
     "shell.execute_reply": "2024-09-16T04:01:27.066878Z",
     "shell.execute_reply.started": "2024-09-16T04:01:27.022365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9cde05-6a41-4faa-899b-5ae1188176ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:02:38.196825Z",
     "iopub.status.busy": "2024-09-16T04:02:38.195957Z",
     "iopub.status.idle": "2024-09-16T04:02:39.232155Z",
     "shell.execute_reply": "2024-09-16T04:02:39.230839Z",
     "shell.execute_reply.started": "2024-09-16T04:02:38.196746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "Device name: 'NVIDIA GeForce RTX 4060 Ti'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Ti', major=8, minor=9, total_memory=16059MB, multi_processor_count=34)'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551268a-a7a1-4e9f-924f-46520caa6a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b25e23f-31aa-49dd-ba64-63c20aa0237c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046eeba4-6800-4ce9-bf96-af3493a6c3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:39:52.621991Z",
     "iopub.status.busy": "2024-09-16T03:39:52.621128Z",
     "iopub.status.idle": "2024-09-16T03:39:54.247006Z",
     "shell.execute_reply": "2024-09-16T03:39:54.244272Z",
     "shell.execute_reply.started": "2024-09-16T03:39:52.621918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-16 03:39:52--  https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5518306 (5.3M) [text/plain]\n",
      "Saving to: ‘hun_eng_pairs_train.txt’\n",
      "\n",
      "hun_eng_pairs_train 100%[===================>]   5.26M  14.3MB/s    in 0.4s    \n",
      "\n",
      "2024-09-16 03:39:54 (14.3 MB/s) - ‘hun_eng_pairs_train.txt’ saved [5518306/5518306]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/hun_eng_pairs/hun_eng_pairs_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efd1e5f-378b-4388-8823-2aa209aec1ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:40:00.690821Z",
     "iopub.status.busy": "2024-09-16T03:40:00.690568Z",
     "iopub.status.idle": "2024-09-16T03:40:00.720975Z",
     "shell.execute_reply": "2024-09-16T03:40:00.720407Z",
     "shell.execute_reply.started": "2024-09-16T03:40:00.690804Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('hun_eng_pairs_train.txt') as file:\n",
    "  train = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d4a82-abb5-4ef7-98c0-80c9b3976ab9",
   "metadata": {},
   "source": [
    "Cada entrada consiste em uma sentença em hungaro seguida por uma sentença em inglês:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32039775-8913-4d0e-b6b4-167001a3ebbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:40:52.489759Z",
     "iopub.status.busy": "2024-09-16T03:40:52.489511Z",
     "iopub.status.idle": "2024-09-16T03:40:52.495014Z",
     "shell.execute_reply": "2024-09-16T03:40:52.494543Z",
     "shell.execute_reply.started": "2024-09-16T03:40:52.489738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Teszek rá, mit mondasz!<sep>I don't care what you say.\",\n",
       " 'Több olyan ember kell nekünk a csapatba, mint amilyen te vagy.<sep>We need more people like you on our team.',\n",
       " 'Vigyázz a gyerekeimre!<sep>Take care of my children.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff46fb-1d00-4826-b419-dff801eee5eb",
   "metadata": {},
   "source": [
    "Esse é um dataset relativamente pequeno para uma task de traduçãode linguagem. Mas vamos ver quão longe conseguimos ir com ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a07b27-1e55-4c99-8252-64012a2fb01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:42:04.585532Z",
     "iopub.status.busy": "2024-09-16T03:42:04.584655Z",
     "iopub.status.idle": "2024-09-16T03:42:04.589622Z",
     "shell.execute_reply": "2024-09-16T03:42:04.589238Z",
     "shell.execute_reply.started": "2024-09-16T03:42:04.585458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88647"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848113b4-c49f-42d9-899b-f80810833292",
   "metadata": {},
   "source": [
    "Agora, vamos separar as sentenças em listas de acordo com os idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863dd35f-1446-4796-abaf-5804c6ed0633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:42:26.605449Z",
     "iopub.status.busy": "2024-09-16T03:42:26.605228Z",
     "iopub.status.idle": "2024-09-16T03:42:26.849610Z",
     "shell.execute_reply": "2024-09-16T03:42:26.849098Z",
     "shell.execute_reply.started": "2024-09-16T03:42:26.605434Z"
    }
   },
   "outputs": [],
   "source": [
    "SEPARATOR = '<sep>'\n",
    "train_input, train_target = map(list, zip(*[pair.split(SEPARATOR) for pair in train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8cabd-30cd-4ed9-b0c0-52e9215d3703",
   "metadata": {},
   "source": [
    "Como estamos lidando com uma linguagem de origem que usa caracteres acentuados, é importante aplicar a normalização Unicode.\n",
    "\n",
    "No exemplo abaixo, dois conjuntos diferentes de Unicode produzem o mesmo caractere visualmente. O primeiro Unicode é para um 'a' acentuado, enquanto o segundo Unicode combina um 'a' com um acento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a692c00-0f69-443e-8156-3d49961dbec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:45:27.949713Z",
     "iopub.status.busy": "2024-09-16T03:45:27.949308Z",
     "iopub.status.idle": "2024-09-16T03:45:27.954053Z",
     "shell.execute_reply": "2024-09-16T03:45:27.953675Z",
     "shell.execute_reply.started": "2024-09-16T03:45:27.949680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "á á\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u00E1\", \"\\u0061\\u0301\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56fbb9-ebd1-4ac5-a79a-06b8aae7afdc",
   "metadata": {},
   "source": [
    "Embora esses caracteres pareçam os mesmos para nós que os lemos, eles serão tratados de forma diferente por um modelo. Então, para evitar isso, a função a seguir normaliza quaisquer caracteres acentuados no mesmo conjunto de Unicode e, em seguida, os substitui por seus equivalentes ASCII.\n",
    "https://docs.python.org/3/library/unicodedata.html\n",
    "\n",
    "Aqui está um artigo informativo sobre a importância da normalização Unicode e como fazê-lo (incluindo o que NFD significa):\n",
    "https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8fb9c0-9554-4c9f-8261-f42c9978b0df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:46:12.696266Z",
     "iopub.status.busy": "2024-09-16T03:46:12.695859Z",
     "iopub.status.idle": "2024-09-16T03:46:12.702403Z",
     "shell.execute_reply": "2024-09-16T03:46:12.701003Z",
     "shell.execute_reply.started": "2024-09-16T03:46:12.696232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unicode normalization\n",
    "def normalize_unicode(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89146702-62a5-408a-93ff-db1275e30db0",
   "metadata": {},
   "source": [
    "Estamos construindo um modelo de tradução baseado em palavras , mas ainda queremos manter a pontuação e tratá-las como tokens separados, então inseriremos um espaço entre qualquer pontuação relevante e os caracteres ao redor delas. Dessa forma, nosso tokenizador (que não filtrará a pontuação) produzirá sinais de pontuação como tokens separados.\n",
    "\n",
    "Esta função faz isso e a normalização unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489f0a8a-33a9-444b-8d32-35870af3297a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:46:51.156284Z",
     "iopub.status.busy": "2024-09-16T03:46:51.154968Z",
     "iopub.status.idle": "2024-09-16T03:46:51.158746Z",
     "shell.execute_reply": "2024-09-16T03:46:51.158331Z",
     "shell.execute_reply.started": "2024-09-16T03:46:51.156268Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(s):\n",
    "  s = normalize_unicode(s)\n",
    "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "  s = re.sub(r'[\" \"]+', \" \", s)\n",
    "  s = s.strip()\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b889f631-ab6f-49df-88e1-cfe6194f6bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:46:58.127568Z",
     "iopub.status.busy": "2024-09-16T03:46:58.125883Z",
     "iopub.status.idle": "2024-09-16T03:46:59.428799Z",
     "shell.execute_reply": "2024-09-16T03:46:59.428336Z",
     "shell.execute_reply.started": "2024-09-16T03:46:58.127488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess both the source and target sentences.\n",
    "train_preprocessed_input = [preprocess_sentence(s) for s in train_input]\n",
    "train_preprocessed_target = [preprocess_sentence(s) for s in train_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340046c-a7b7-4336-b596-639071878446",
   "metadata": {},
   "source": [
    "Após o pré-processamento, o unicode deve ser normalizado e deve haver espaços em ambos os lados de qualquer pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c7444a-e768-442c-8c5e-bc06888ed95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:47:23.930614Z",
     "iopub.status.busy": "2024-09-16T03:47:23.930342Z",
     "iopub.status.idle": "2024-09-16T03:47:23.933865Z",
     "shell.execute_reply": "2024-09-16T03:47:23.933450Z",
     "shell.execute_reply.started": "2024-09-16T03:47:23.930597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Teszek ra , mit mondasz !',\n",
       " 'Tobb olyan ember kell nekunk a csapatba , mint amilyen te vagy .',\n",
       " 'Vigyazz a gyerekeimre !']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preprocessed_input[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64dc182-8254-4c17-8ad8-f910a50eff18",
   "metadata": {},
   "source": [
    "Usaremos o **Teacher Forcing** com nosso modelo de tradução (especificamente, o decodificador). O primeiro passo dessa implementação consiste em colocar uma tag de início de frase ( \\<sos\\> ) e uma tag de fim de frase ( \\<eos\\> ) no início e no fim de cada frase-alvo, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5f89a0-1bde-475c-9925-d13fa0c64a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:48:53.878660Z",
     "iopub.status.busy": "2024-09-16T03:48:53.878387Z",
     "iopub.status.idle": "2024-09-16T03:48:53.881324Z",
     "shell.execute_reply": "2024-09-16T03:48:53.880860Z",
     "shell.execute_reply.started": "2024-09-16T03:48:53.878638Z"
    }
   },
   "outputs": [],
   "source": [
    "def tag_target_sentences(sentences):\n",
    "  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)\n",
    "  return list(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5101161a-c429-43cb-8fb0-07e176ba48a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:49:59.407787Z",
     "iopub.status.busy": "2024-09-16T03:49:59.407531Z",
     "iopub.status.idle": "2024-09-16T03:49:59.425618Z",
     "shell.execute_reply": "2024-09-16T03:49:59.425112Z",
     "shell.execute_reply.started": "2024-09-16T03:49:59.407772Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d2c0650-f3b9-4d0b-90a0-1235800c0871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:50:07.401008Z",
     "iopub.status.busy": "2024-09-16T03:50:07.400666Z",
     "iopub.status.idle": "2024-09-16T03:50:07.403988Z",
     "shell.execute_reply": "2024-09-16T03:50:07.403618Z",
     "shell.execute_reply.started": "2024-09-16T03:50:07.400991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<sos> I don't care what you say . <eos>\",\n",
       " '<sos> We need more people like you on our team . <eos>',\n",
       " '<sos> Take care of my children . <eos>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_preprocessed_target[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e6398-616c-4384-815d-59191f8ac4a3",
   "metadata": {},
   "source": [
    "A seguir, tokenizaremos nossas sentenças de entrada e de destino, tomando cuidado para manter a pontuação relevante.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "Observe que também estamos incluindo um token fora do vocabulário (\\<unk\\>) na inicialização do Tokenizer. No momento da inferência, se o tokenizer encontrar uma palavra que não viu durante o ajuste inicial nos dados de treinamento, essa palavra será substituída por \\<unk\\> e o sistema de tradução precisará lidar com isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96a49910-de96-4f0d-a925-e31e32105c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:54:30.492221Z",
     "iopub.status.busy": "2024-09-16T03:54:30.491377Z",
     "iopub.status.idle": "2024-09-16T03:54:31.041487Z",
     "shell.execute_reply": "2024-09-16T03:54:31.040987Z",
     "shell.execute_reply.started": "2024-09-16T03:54:30.492149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer for the Hungarian input sentences. Note how we're not filtering punctuation.\n",
    "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "source_tokenizer.fit_on_texts(train_preprocessed_input)\n",
    "# source_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "105d0f64-6475-4865-92e6-2ad71b3239d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:54:39.987140Z",
     "iopub.status.busy": "2024-09-16T03:54:39.986856Z",
     "iopub.status.idle": "2024-09-16T03:54:39.990194Z",
     "shell.execute_reply": "2024-09-16T03:54:39.989743Z",
     "shell.execute_reply.started": "2024-09-16T03:54:39.987124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38539\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "print(source_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c271585-3ba7-4a0f-aaaa-e4017af7de5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:54:59.286426Z",
     "iopub.status.busy": "2024-09-16T03:54:59.285670Z",
     "iopub.status.idle": "2024-09-16T03:54:59.886385Z",
     "shell.execute_reply": "2024-09-16T03:54:59.885887Z",
     "shell.execute_reply.started": "2024-09-16T03:54:59.286374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer for the English target sentences.\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n')\n",
    "target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)\n",
    "# target_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c049dad-ab5b-433d-b80e-2cd7a124cf24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:55:05.305823Z",
     "iopub.status.busy": "2024-09-16T03:55:05.304244Z",
     "iopub.status.idle": "2024-09-16T03:55:05.317402Z",
     "shell.execute_reply": "2024-09-16T03:55:05.314588Z",
     "shell.execute_reply.started": "2024-09-16T03:55:05.305717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10556\n"
     ]
    }
   ],
   "source": [
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d63aa-a263-42ae-a86a-cc5f8abf5cf2",
   "metadata": {},
   "source": [
    "Em seguida, vetorizaremos as frases de entrada e de destino, assim como fizemos nas últimas demonstrações.\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "115bc5d5-9db3-4db2-b8ec-46e19cc3c944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T03:56:00.998414Z",
     "iopub.status.busy": "2024-09-16T03:56:00.998224Z",
     "iopub.status.idle": "2024-09-16T03:56:01.471611Z",
     "shell.execute_reply": "2024-09-16T03:56:01.471043Z",
     "shell.execute_reply.started": "2024-09-16T03:56:00.998400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376efc02-8047-409d-bbb5-7881ab49653b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6581ae-38f1-451e-bfb7-603758d871c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf6d59-e803-40dc-8bfa-bdcc08ebf7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c7afb-1e01-462f-8ac7-898479247a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924d45c-ab85-429f-89c6-a090adb04ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af64b3a-de38-424c-8af1-daf027435d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efbda4-c88b-4424-a5a3-85c7a4c35fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499f019-2187-4498-8941-5b3427dae1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37615b96-99ee-4529-9856-8b00e1f1a3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23334c53-5a48-4698-9555-0c4a486eb2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a1572-ea98-4925-9568-666299beddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d2450-1b73-47fb-aca7-bc3f6cfc0669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47f927-8de9-465d-8f3d-93648890f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3f301-2644-4f12-a0ee-fdf9d4532169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8679a-83e3-418e-bafa-a8f244905b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3936f01-62df-4cc0-8d2d-2e52ad2cc82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901bab3-004c-4e1e-95b2-87472aea98b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f5993-a9cc-4557-b049-e092896f05c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
