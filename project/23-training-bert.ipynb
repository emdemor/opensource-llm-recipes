{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e169b797-a444-4b4f-b450-cb176cf43c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:27:19.272961Z",
     "iopub.status.busy": "2025-02-13T02:27:19.272015Z",
     "iopub.status.idle": "2025-02-13T02:27:29.073938Z",
     "shell.execute_reply": "2025-02-13T02:27:29.072812Z",
     "shell.execute_reply.started": "2025-02-13T02:27:19.272885Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==4.48.3 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b1be7-4720-4164-8e97-b4d79fd57094",
   "metadata": {},
   "source": [
    "# Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945fa295-0933-455e-a954-30ce1ac47118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d344e9-1ba3-4930-8607-82b270859f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:02:30.150092Z",
     "iopub.status.busy": "2025-02-14T03:02:30.148662Z",
     "iopub.status.idle": "2025-02-14T03:02:32.400247Z",
     "shell.execute_reply": "2025-02-14T03:02:32.399658Z",
     "shell.execute_reply.started": "2025-02-14T03:02:30.149960Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#  baixar arquivo de https://github.com/emdemor/News-of-the-Brazilian-Newspaper/blob/main/data/brazilian-news.parquet\n",
    "df = pd.read_parquet(\"data/brazilian-news.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89393c4e-f209-421b-b3a0-5e8a1f0b1e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:02:32.401327Z",
     "iopub.status.busy": "2025-02-14T03:02:32.401145Z",
     "iopub.status.idle": "2025-02-14T03:02:32.422172Z",
     "shell.execute_reply": "2025-02-14T03:02:32.421667Z",
     "shell.execute_reply.started": "2025-02-14T03:02:32.401313Z"
    }
   },
   "outputs": [],
   "source": [
    "temp = df.sample(10000)\n",
    "texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "\n",
    "texts = [x[:1000] for x in texts if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d26eea-29ef-4285-8436-d5ee4f139947",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modelo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568a3ca-9d9e-48a7-9e43-f7054c4ccc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb700d93-d7b2-4e67-94fa-090ee0bd732f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T16:06:36.213713Z",
     "iopub.status.busy": "2025-02-13T16:06:36.213275Z",
     "iopub.status.idle": "2025-02-13T16:07:09.374367Z",
     "shell.execute_reply": "2025-02-13T16:07:09.373885Z",
     "shell.execute_reply.started": "2025-02-13T16:06:36.213676Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer.json\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233f07abd3a64d5a872e98f8085eaeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\n",
      "Model config ModernBertConfig {\n",
      "  \"_name_or_path\": \"answerdotai/ModernBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"ModernBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 50281,\n",
      "  \"classifier_activation\": \"gelu\",\n",
      "  \"classifier_bias\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"classifier_pooling\": \"mean\",\n",
      "  \"cls_token_id\": 50281,\n",
      "  \"decoder_bias\": true,\n",
      "  \"deterministic_flash_attn\": false,\n",
      "  \"embedding_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50282,\n",
      "  \"global_attn_every_n_layers\": 3,\n",
      "  \"global_rope_theta\": 160000.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_activation\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_cutoff_factor\": 2.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1152,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"local_attention\": 128,\n",
      "  \"local_rope_theta\": 10000.0,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"mlp_dropout\": 0.0,\n",
      "  \"model_type\": \"modernbert\",\n",
      "  \"norm_bias\": false,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"pad_token_id\": 50283,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"reference_compile\": null,\n",
      "  \"repad_logits_with_grad\": false,\n",
      "  \"sep_token_id\": 50282,\n",
      "  \"sparse_pred_ignore_index\": -100,\n",
      "  \"sparse_prediction\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\n",
      "Instantiating ModernBertForMaskedLM model under default dtype torch.float16.\n",
      "All model checkpoint weights were used when initializing ModernBertForMaskedLM.\n",
      "\n",
      "All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `ModernBertForMaskedLM.forward` and have been ignored: text. If text are not expected by `ModernBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20\n",
      "  Number of trainable parameters = 149,655,232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-10\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-10/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-10/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-10] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-20\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-20/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-20/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-20] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/model.safetensors\n",
      "tokenizer config file saved in ./ModernBERT-portuguese-adapted/tokenizer_config.json\n",
      "Special tokens file saved in ./ModernBERT-portuguese-adapted/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./ModernBERT-portuguese-adapted/tokenizer_config.json',\n",
       " './ModernBERT-portuguese-adapted/special_tokens_map.json',\n",
       " './ModernBERT-portuguese-adapted/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging\n",
    "from datasets import Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Desativa o paralelismo dos tokenizers para evitar avisos\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Ajusta a verbosidade para INFO para ver mais detalhes nos logs\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# 1. Criação de um corpus fictício com algumas frases em português\n",
    "texts = [\n",
    "    \"O gato dorme no sofá.\",\n",
    "    \"A inteligência artificial está transformando o mundo.\",\n",
    "    \"Eu gosto de programar em Python.\",\n",
    "    \"Este é um exemplo de corpus em português.\"\n",
    "]\n",
    "\n",
    "# Converte a lista de textos em um Dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# 2. Carrega o tokenizador do modelo \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "\n",
    "# Função para tokenizar o dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "# Aplica a tokenização no dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. Configura um data collator para treinamento com masked language modeling (MLM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# 4. Carrega o modelo \"answerdotai/ModernBERT-base\" para MLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    device_map=\"auto\",   # ou device_map={\"\": \"cuda:0\"}\n",
    "    torch_dtype=torch.float16  # ou o dtype desejado\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Define os argumentos de treinamento com logging_steps para imprimir logs a cada 5 passos\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ModernBERT-portuguese-adapted\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,             # Utilize mais épocas para um treinamento real\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,                # Log a cada 5 passos\n",
    "    logging_dir=\"./logs\",           # Diretório para os logs (opcional)\n",
    ")\n",
    "\n",
    "# 6. Configura o Trainer com o modelo, os argumentos, o dataset e o data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7. Inicia o treinamento; você verá os logs de avanço no console.\n",
    "trainer.train()\n",
    "\n",
    "# 8. Salva o modelo e o tokenizador adaptados\n",
    "model.save_pretrained(\"./ModernBERT-portuguese-adapted\")\n",
    "tokenizer.save_pretrained(\"./ModernBERT-portuguese-adapted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c904ce-c86d-4321-86bc-213382042231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T16:07:31.267263Z",
     "iopub.status.busy": "2025-02-13T16:07:31.266910Z",
     "iopub.status.idle": "2025-02-13T16:07:38.686319Z",
     "shell.execute_reply": "2025-02-13T16:07:38.685877Z",
     "shell.execute_reply.started": "2025-02-13T16:07:31.267235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file ./ModernBERT-portuguese-adapted/config.json\n",
      "Model config ModernBertConfig {\n",
      "  \"_name_or_path\": \"./ModernBERT-portuguese-adapted\",\n",
      "  \"architectures\": [\n",
      "    \"ModernBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 50281,\n",
      "  \"classifier_activation\": \"gelu\",\n",
      "  \"classifier_bias\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"classifier_pooling\": \"mean\",\n",
      "  \"cls_token_id\": 50281,\n",
      "  \"decoder_bias\": true,\n",
      "  \"deterministic_flash_attn\": false,\n",
      "  \"embedding_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50282,\n",
      "  \"global_attn_every_n_layers\": 3,\n",
      "  \"global_rope_theta\": 160000.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_activation\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_cutoff_factor\": 2.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1152,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"local_attention\": 128,\n",
      "  \"local_rope_theta\": 10000.0,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"mlp_dropout\": 0.0,\n",
      "  \"model_type\": \"modernbert\",\n",
      "  \"norm_bias\": false,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"pad_token_id\": 50283,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"reference_compile\": true,\n",
      "  \"repad_logits_with_grad\": false,\n",
      "  \"sep_token_id\": 50282,\n",
      "  \"sparse_pred_ignore_index\": -100,\n",
      "  \"sparse_prediction\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n",
      "loading weights file ./ModernBERT-portuguese-adapted/model.safetensors\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "All model checkpoint weights were used when initializing ModernBertForMaskedLM.\n",
      "\n",
      "All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at ./ModernBERT-portuguese-adapted.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: \" | Score: nan\n",
      "Token: # | Score: nan\n",
      "Token: <|padding|> | Score: nan\n",
      "Token: |||IP_ADDRESS||| | Score: nan\n",
      "Token: ! | Score: nan\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Carrega o tokenizador e o modelo da pasta onde foram salvos\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./ModernBERT-portuguese-adapted\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./ModernBERT-portuguese-adapted\")\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "# Cria um pipeline de \"fill-mask\" para a tarefa de preenchimento de máscara\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Exemplo de frase com token de máscara (geralmente [MASK])\n",
    "sentence = \"O gato [MASK] no sofá.\"\n",
    "\n",
    "# Executa o pipeline para obter as predições para o token mascarado\n",
    "results = fill_mask(sentence)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Token: {result['token_str']} | Score: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab787601-4215-4c0d-bf2a-900ae058f16f",
   "metadata": {},
   "source": [
    "# Mudar o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2df9ef-670a-412b-87cd-4fc719023dc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:02:43.827502Z",
     "iopub.status.busy": "2025-02-14T03:02:43.827198Z",
     "iopub.status.idle": "2025-02-14T03:02:47.708130Z",
     "shell.execute_reply": "2025-02-14T03:02:47.707609Z",
     "shell.execute_reply.started": "2025-02-14T03:02:43.827483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging\n",
    "from datasets import Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76656801-d244-4128-a3c6-ee9b7e9f1ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:02:50.589435Z",
     "iopub.status.busy": "2025-02-14T03:02:50.589037Z",
     "iopub.status.idle": "2025-02-14T03:02:50.660615Z",
     "shell.execute_reply": "2025-02-14T03:02:50.660090Z",
     "shell.execute_reply.started": "2025-02-14T03:02:50.589417Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 1. Criação de um corpus fictício com algumas frases em português\n",
    "# texts = [\n",
    "#     \"O gato dorme no sofá.\",\n",
    "#     \"A inteligência artificial está transformando o mundo.\",\n",
    "#     \"Eu gosto de programar em Python.\",\n",
    "#     \"Este é um exemplo de corpus em português.\"\n",
    "# ]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b7174e-0a95-4f15-8901-e304fc67bb09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:03:09.544352Z",
     "iopub.status.busy": "2025-02-14T03:03:09.544086Z",
     "iopub.status.idle": "2025-02-14T03:03:10.081255Z",
     "shell.execute_reply": "2025-02-14T03:03:10.080778Z",
     "shell.execute_reply.started": "2025-02-14T03:03:09.544331Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2100ba08-0df7-4873-96b3-ea18918ec313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T03:03:30.926097Z",
     "iopub.status.busy": "2025-02-14T03:03:30.925671Z",
     "iopub.status.idle": "2025-02-14T03:03:30.928339Z",
     "shell.execute_reply": "2025-02-14T03:03:30.927927Z",
     "shell.execute_reply.started": "2025-02-14T03:03:30.926080Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801481f1-8ee3-447c-89d9-af266f275e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T16:32:48.158459Z",
     "iopub.status.busy": "2025-02-13T16:32:48.158214Z",
     "iopub.status.idle": "2025-02-13T16:33:04.427813Z",
     "shell.execute_reply": "2025-02-13T16:33:04.427055Z",
     "shell.execute_reply.started": "2025-02-13T16:32:48.158436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--neuralmind--bert-base-portuguese-cased/snapshots/94d69c95f98f7d5b2a8700c420230ae10def0baa/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"neuralmind/bert-base-portuguese-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 29794\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd18bc8c4be4724abb0540914edb7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/config.json\n",
      "Model config ModernBertConfig {\n",
      "  \"_name_or_path\": \"answerdotai/ModernBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"ModernBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 50281,\n",
      "  \"classifier_activation\": \"gelu\",\n",
      "  \"classifier_bias\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"classifier_pooling\": \"mean\",\n",
      "  \"cls_token_id\": 50281,\n",
      "  \"decoder_bias\": true,\n",
      "  \"deterministic_flash_attn\": false,\n",
      "  \"embedding_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50282,\n",
      "  \"global_attn_every_n_layers\": 3,\n",
      "  \"global_rope_theta\": 160000.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_activation\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_cutoff_factor\": 2.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1152,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"local_attention\": 128,\n",
      "  \"local_rope_theta\": 10000.0,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"mlp_dropout\": 0.0,\n",
      "  \"model_type\": \"modernbert\",\n",
      "  \"norm_bias\": false,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"pad_token_id\": 50283,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"reference_compile\": null,\n",
      "  \"repad_logits_with_grad\": false,\n",
      "  \"sep_token_id\": 50282,\n",
      "  \"sparse_pred_ignore_index\": -100,\n",
      "  \"sparse_prediction\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/models/models--answerdotai--ModernBERT-base/snapshots/8949b909ec900327062f0ebf497f51aef5e6f0c8/model.safetensors\n",
      "Instantiating ModernBertForMaskedLM model under default dtype torch.float16.\n",
      "All model checkpoint weights were used when initializing ModernBertForMaskedLM.\n",
      "\n",
      "All the weights of ModernBertForMaskedLM were initialized from the model checkpoint at answerdotai/ModernBERT-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ModernBertForMaskedLM for predictions without further training.\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 29794. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída do modelo com o novo tokenizador: MaskedLMOutput(loss=None, logits=tensor([[[  4.3398,   1.9277,   6.0703,  ...,   0.6348,   2.3750,  -0.5469],\n",
      "         [  3.0078,  -1.8711,   4.4531,  ...,   2.8477,   5.6719,  -0.8979],\n",
      "         [  1.5713,  -0.5005,  -3.9258,  ...,  -1.5059,   5.1719,  -6.7070],\n",
      "         ...,\n",
      "         [-12.4062,  -1.8916,   6.3242,  ...,   1.7012,  13.8750,  -7.0977],\n",
      "         [  8.8438,   2.2246,  10.6094,  ...,  -5.7734,   3.0801,  -1.6514],\n",
      "         [  6.4961,   4.0195,  13.4062,  ...,  -1.3145,  -0.0541,  -7.0078]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `ModernBertForMaskedLM.forward` and have been ignored: token_type_ids, text. If token_type_ids, text are not expected by `ModernBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 19,955\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 99,780\n",
      "  Number of trainable parameters = 133,833,826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='99780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   31/99780 00:03 < 3:18:50, 8.36 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-10\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-10/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-10/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-10] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-20\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-20/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-20/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-20] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-30\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-30/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-30/model.safetensors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 74\u001b[0m\n\u001b[1;32m     66\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     68\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     69\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[1;32m     70\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 7. Inicia o treinamento; você verá os logs de avanço no console.\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 8. Salva o modelo e o tokenizador adaptados\u001b[39;00m\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ModernBERT-portuguese-adapted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2598\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2598\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3078\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3075\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3078\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3079\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3213\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3214\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   3215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3334\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3329\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3330\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3331\u001b[0m     )\n\u001b[1;32m   3332\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3333\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3334\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3337\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3339\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=new_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# 3. Carrega o modelo original para MLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"answerdotai/ModernBERT-base\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 4. Redimensiona a camada de embeddings para o novo vocabulário\n",
    "new_vocab_size = len(new_tokenizer)\n",
    "model.resize_token_embeddings(new_vocab_size)\n",
    "\n",
    "# 5. Garanta que o modelo esteja na GPU (movendo novamente após o redimensionamento)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# 6. (Opcional) Ao tokenizar uma entrada para teste, mova os tensores para a GPU\n",
    "example_text = \"Este é um teste de adaptação de tokenizador.\"\n",
    "inputs = new_tokenizer(example_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(\"Saída do modelo com o novo tokenizador:\", outputs)\n",
    "\n",
    "\n",
    "# 5. Define os argumentos de treinamento com logging_steps para imprimir logs a cada 5 passos\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ModernBERT-portuguese-adapted\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,             # Utilize mais épocas para um treinamento real\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=0.00001,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,                # Log a cada 5 passos\n",
    "    logging_dir=\"./logs\",           # Diretório para os logs (opcional)\n",
    ")\n",
    "\n",
    "# 6. Configura o Trainer com o modelo, os argumentos, o dataset e o data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 7. Inicia o treinamento; você verá os logs de avanço no console.\n",
    "trainer.train()\n",
    "\n",
    "# 8. Salva o modelo e o tokenizador adaptados\n",
    "model.save_pretrained(\"./ModernBERT-portuguese-adapted\")\n",
    "new_tokenizer.save_pretrained(\"./ModernBERT-portuguese-adapted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e3d5a6-b928-438b-bb8d-53ba91809fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T16:16:28.293053Z",
     "iopub.status.busy": "2025-02-13T16:16:28.292846Z",
     "iopub.status.idle": "2025-02-13T16:16:46.921469Z",
     "shell.execute_reply": "2025-02-13T16:16:46.920733Z",
     "shell.execute_reply.started": "2025-02-13T16:16:28.293036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `ModernBertForMaskedLM.forward` and have been ignored: token_type_ids, text. If token_type_ids, text are not expected by `ModernBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 20\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 133,833,826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>15.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-10\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-10/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-10/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-10] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-20\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-20/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-20/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-20] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-30\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-30/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-30/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-30] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-40\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-40/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-40/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-40] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-50\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-50/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-50/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-60\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-60/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-60/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-60] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-70\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-70/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-70/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-70] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-80\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-80/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-80/model.safetensors\n",
      "Deleting older checkpoint [ModernBERT-portuguese-adapted/checkpoint-80] due to args.save_total_limit\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-90\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-90/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-90/model.safetensors\n",
      "Saving model checkpoint to ./ModernBERT-portuguese-adapted/checkpoint-100\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/checkpoint-100/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/checkpoint-100/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in ./ModernBERT-portuguese-adapted/config.json\n",
      "Model weights saved in ./ModernBERT-portuguese-adapted/model.safetensors\n",
      "tokenizer config file saved in ./ModernBERT-portuguese-adapted/tokenizer_config.json\n",
      "Special tokens file saved in ./ModernBERT-portuguese-adapted/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./ModernBERT-portuguese-adapted/tokenizer_config.json',\n",
       " './ModernBERT-portuguese-adapted/special_tokens_map.json',\n",
       " './ModernBERT-portuguese-adapted/vocab.txt',\n",
       " './ModernBERT-portuguese-adapted/added_tokens.json',\n",
       " './ModernBERT-portuguese-adapted/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c8f9b-fbe0-4bcf-8ed3-292660f40e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ced141-7fa8-411b-bbba-2fbe556385a1",
   "metadata": {},
   "source": [
    "# ------ Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2d11b-8935-4a1e-8583-fc3b4858a6ab",
   "metadata": {},
   "source": [
    "The documents are clustered into taxonomies and the corpus can be loaded in complete or taxonomy modes. To load a single taxonomy, it is possible to pass a code as a parameter to the loading script (see the example bellow). Codes are 3-letters string and possible values are:\n",
    "\n",
    "dat : datasets and other corpora;\n",
    "\n",
    "jud : judicial branch;\n",
    "\n",
    "leg : legislative branch;\n",
    "\n",
    "pub : public domain works;\n",
    "\n",
    "soc : social media;\n",
    "\n",
    "uni : university domains;\n",
    "\n",
    "wik : wikis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828b8a89-ed1e-44b6-a284-c5e05adfeee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T03:04:03.851552Z",
     "iopub.status.busy": "2025-02-13T03:04:03.851350Z",
     "iopub.status.idle": "2025-02-13T03:06:41.747160Z",
     "shell.execute_reply": "2025-02-13T03:06:41.746674Z",
     "shell.execute_reply.started": "2025-02-13T03:04:03.851530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7639bad02ad44924854557d734194484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/193 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b66c3fa97c649eebb47f7e32343a125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating corpus split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# https://huggingface.co/datasets/carolina-c4ai/corpus-carolina\n",
    "# dataset = load_dataset(\"carolina-c4ai/corpus-carolina\")\n",
    "dataset = load_dataset(\"carolina-c4ai/corpus-carolina\", taxonomy=\"wik\")\n",
    "\n",
    "\n",
    "\n",
    "all_texts = dataset[\"corpus\"][\"text\"]\n",
    "sample_texts = random.sample(all_texts, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698338af-355b-4e11-aaa3-a272bb4c803e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-13T03:03:32.302Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7f1b7-fa76-44f0-ab62-e69a8e7a6409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5197d2-e52a-4698-a8ed-ea7ccce0cfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be2a4c2-4c8f-4854-91a4-247c425f949a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:28:56.512557Z",
     "iopub.status.busy": "2025-02-13T02:28:56.512295Z",
     "iopub.status.idle": "2025-02-13T02:28:56.518253Z",
     "shell.execute_reply": "2025-02-13T02:28:56.517870Z",
     "shell.execute_reply.started": "2025-02-13T02:28:56.512541Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Desativa o paralelismo do tokenizers para evitar os avisos\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. Criação de um corpus fictício com algumas frases em português\n",
    "texts = [\n",
    "    \"O gato dorme no sofá.\",\n",
    "    \"A inteligência artificial está transformando o mundo.\",\n",
    "    \"Eu gosto de programar em Python.\",\n",
    "    \"Este é um exemplo de corpus em português.\"\n",
    "]\n",
    "\n",
    "# Converte a lista de textos em um Dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913dcf3a-2cf1-4813-9e21-79e6de76cb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:29:02.646526Z",
     "iopub.status.busy": "2025-02-13T02:29:02.646029Z",
     "iopub.status.idle": "2025-02-13T02:29:03.775035Z",
     "shell.execute_reply": "2025-02-13T02:29:03.774604Z",
     "shell.execute_reply.started": "2025-02-13T02:29:02.646508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19faa3847c3847069ce48677a2b5125f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 2. Carrega o tokenizador do modelo \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "\n",
    "# Função para tokenizar o dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "# Aplica a tokenização no dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. Configura um data collator para treinamento com masked language modeling (MLM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# 4. Carrega o modelo \"answerdotai/ModernBERT-base\" para MLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0717243d-e02c-4263-8836-0a7ce1a0f8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:29:24.513183Z",
     "iopub.status.busy": "2025-02-13T02:29:24.512973Z",
     "iopub.status.idle": "2025-02-13T02:29:24.779389Z",
     "shell.execute_reply": "2025-02-13T02:29:24.778872Z",
     "shell.execute_reply.started": "2025-02-13T02:29:24.513162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to cuda\n"
     ]
    }
   ],
   "source": [
    "# Se houver GPU disponível, move o modelo para GPU (necessário para Flash Attention 2.0)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"moving to cuda\")\n",
    "    model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a1ebc2-c19b-484d-8ac2-8282c40eb6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:29:31.613260Z",
     "iopub.status.busy": "2025-02-13T02:29:31.612951Z",
     "iopub.status.idle": "2025-02-13T02:29:31.630524Z",
     "shell.execute_reply": "2025-02-13T02:29:31.630062Z",
     "shell.execute_reply.started": "2025-02-13T02:29:31.613244Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Define os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ModernBERT-portuguese-adapted\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # Utilize mais épocas em um cenário real\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfecd79-fc23-4c17-b6dd-c08b61d8b6f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:29:36.691323Z",
     "iopub.status.busy": "2025-02-13T02:29:36.690989Z",
     "iopub.status.idle": "2025-02-13T02:29:36.801190Z",
     "shell.execute_reply": "2025-02-13T02:29:36.800659Z",
     "shell.execute_reply.started": "2025-02-13T02:29:36.691304Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Configura o Trainer com o modelo, os argumentos, o dataset e o data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "591e5fd8-a948-4d47-8050-4231932d7575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T02:30:14.089291Z",
     "iopub.status.busy": "2025-02-13T02:30:14.088583Z",
     "iopub.status.idle": "2025-02-13T02:30:41.488195Z",
     "shell.execute_reply": "2025-02-13T02:30:41.487628Z",
     "shell.execute_reply.started": "2025-02-13T02:30:14.089271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./ModernBERT-portuguese-adapted/tokenizer_config.json',\n",
       " './ModernBERT-portuguese-adapted/special_tokens_map.json',\n",
       " './ModernBERT-portuguese-adapted/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Inicia o treinamento (continuação do pré-treinamento)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# 8. Salva o modelo e o tokenizador adaptados\n",
    "model.save_pretrained(\"./ModernBERT-portuguese-adapted\")\n",
    "tokenizer.save_pretrained(\"./ModernBERT-portuguese-adapted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96edcb39-acbc-4040-803e-2b423600dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9fa11-0529-4ae6-b775-66ef0f6a7187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631c392-38f4-4d03-9191-c3b8f6cf01d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca01acfb-900e-4d39-bf1a-08aad873d251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:34:49.779578Z",
     "iopub.status.busy": "2024-09-24T03:34:49.778973Z",
     "iopub.status.idle": "2024-09-24T03:34:49.782338Z",
     "shell.execute_reply": "2024-09-24T03:34:49.781881Z",
     "shell.execute_reply.started": "2024-09-24T03:34:49.779562Z"
    }
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import yt_dlp\n",
    "import yt_dlp as youtube_dl\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "13442c8c-80f2-430c-a2ad-1de541e58157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:20:45.414356Z",
     "iopub.status.busy": "2024-09-24T04:20:45.414088Z",
     "iopub.status.idle": "2024-09-24T04:20:45.417585Z",
     "shell.execute_reply": "2024-09-24T04:20:45.417161Z",
     "shell.execute_reply.started": "2024-09-24T04:20:45.414338Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def get_youtube_video_metadata(video_url):\n",
    "    ydl_opts = {\n",
    "        'skip_download': True,  # Não baixa o vídeo\n",
    "        'extract_flat': True,   # Não extrai streams de mídia, só metadados\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(video_url, download=False)\n",
    "\n",
    "    return info_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4245c-1c68-47a4-95bc-3f60de3c660d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90770934-2fcc-4c93-b1bf-4ef3d326a02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765f7d31-ad85-4138-8760-c51d113ba3c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:37:42.536068Z",
     "iopub.status.busy": "2024-09-24T03:37:42.535683Z",
     "iopub.status.idle": "2024-09-24T03:37:42.539843Z",
     "shell.execute_reply": "2024-09-24T03:37:42.539312Z",
     "shell.execute_reply.started": "2024-09-24T03:37:42.536030Z"
    }
   },
   "outputs": [],
   "source": [
    "VIDEO_URL = \"https://www.youtube.com/watch?v=vX3A96_F3FU\"\n",
    "\n",
    "# VIDEO_URL = \"https://www.youtube.com/watch?v=evfLsSRtryk\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "60744bf4-1b0c-4a15-939c-d902ca2c650c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:21:38.425546Z",
     "iopub.status.busy": "2024-09-24T04:21:38.424453Z",
     "iopub.status.idle": "2024-09-24T04:21:38.443814Z",
     "shell.execute_reply": "2024-09-24T04:21:38.443323Z",
     "shell.execute_reply.started": "2024-09-24T04:21:38.425437Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class YoutubeTranscripter:\n",
    "    CACHE_FOLDER = '/root/.cache/diskcache'\n",
    "    def __init__(self, video_url, language=None):\n",
    "        self._cache = dc.Cache(self.CACHE_FOLDER)\n",
    "        self.video_url = video_url\n",
    "        self.metadata = self._get_metadata()\n",
    "        self.language = language if language else self.metadata[\"language\"]\n",
    "        self.title = self.metadata[\"title\"]\n",
    "        self.label = remove_special_characters(self.title.lower()).replace(\" \",\"_\")\n",
    "        self.chapters = self.metadata[\"chapters\"]\n",
    "\n",
    "    def _get_metadata(self):\n",
    "        index = (\"metadata\", self.video_url)\n",
    "        if index not in self._cache:\n",
    "            print(f\"Download do vídeo: {index}\")\n",
    "            self._cache[index] = json.dumps(\n",
    "                get_youtube_video_metadata(self.video_url),\n",
    "                ensure_ascii=False\n",
    "            )\n",
    "        return json.loads(self._cache[index])\n",
    "\n",
    "    def _download_audio(self):\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'outtmpl': f'{output_filename}.%(ext)s',\n",
    "        }\n",
    "        \n",
    "        with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([VIDEO_URL])\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._cache.clear()\n",
    "        print(\"Cache limpo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "943cd603-69bd-4ff5-9d22-a6a9f686005d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:21:41.373338Z",
     "iopub.status.busy": "2024-09-24T04:21:41.373120Z",
     "iopub.status.idle": "2024-09-24T04:21:41.379904Z",
     "shell.execute_reply": "2024-09-24T04:21:41.379397Z",
     "shell.execute_reply.started": "2024-09-24T04:21:41.373324Z"
    }
   },
   "outputs": [],
   "source": [
    "yt_transcriper = YoutubeTranscripter(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8b9f206c-8627-4498-8114-252b9db35d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:16:41.742896Z",
     "iopub.status.busy": "2024-09-24T04:16:41.742320Z",
     "iopub.status.idle": "2024-09-24T04:16:41.749285Z",
     "shell.execute_reply": "2024-09-24T04:16:41.748844Z",
     "shell.execute_reply.started": "2024-09-24T04:16:41.742807Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = yt_transcriper._get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3fe93305-e617-412a-a530-8b7adc10c157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:21:47.394108Z",
     "iopub.status.busy": "2024-09-24T04:21:47.393801Z",
     "iopub.status.idle": "2024-09-24T04:21:47.397089Z",
     "shell.execute_reply": "2024-09-24T04:21:47.396728Z",
     "shell.execute_reply.started": "2024-09-24T04:21:47.394091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph_rag_improving_rag_with_knowledge_graphs'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_transcriper.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "54cae91a-218a-4f45-9b11-e2b4800d5e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:12:21.661622Z",
     "iopub.status.busy": "2024-09-24T04:12:21.661389Z",
     "iopub.status.idle": "2024-09-24T04:12:21.663924Z",
     "shell.execute_reply": "2024-09-24T04:12:21.663527Z",
     "shell.execute_reply.started": "2024-09-24T04:12:21.661606Z"
    }
   },
   "outputs": [],
   "source": [
    "# yt_transcriper._cache.get('metadata_https://www.youtube.com/watch?v=vX3A96_F3FU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b53ada3e-49cb-464a-9a95-7673af5d9055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:16:37.176390Z",
     "iopub.status.busy": "2024-09-24T04:16:37.176125Z",
     "iopub.status.idle": "2024-09-24T04:16:37.179411Z",
     "shell.execute_reply": "2024-09-24T04:16:37.179034Z",
     "shell.execute_reply.started": "2024-09-24T04:16:37.176371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache limpo.\n"
     ]
    }
   ],
   "source": [
    "yt_transcriper.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e2e81f29-3e46-450c-bd00-3005dc7bf106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T04:20:29.390268Z",
     "iopub.status.busy": "2024-09-24T04:20:29.390005Z",
     "iopub.status.idle": "2024-09-24T04:20:29.393086Z",
     "shell.execute_reply": "2024-09-24T04:20:29.392679Z",
     "shell.execute_reply.started": "2024-09-24T04:20:29.390252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph rag improving rag with knowledge graphs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clean_text = remove_special_characters(yt_transcriper.title.lower())\n",
    "print(clean_text)  # Saída: \"Olá Bemvindo ao mundo de Python 2024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826c8ad-3bb7-4a68-a273-d61c61eead4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"data/{title}_raw\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'outtmpl': f'{output_filename}.%(ext)s',\n",
    "}\n",
    "\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([VIDEO_URL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53e95242-ca3f-424a-8e51-040e7a2dc742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:56:20.139564Z",
     "iopub.status.busy": "2024-09-24T03:56:20.139308Z",
     "iopub.status.idle": "2024-09-24T03:56:20.142182Z",
     "shell.execute_reply": "2024-09-24T03:56:20.141740Z",
     "shell.execute_reply.started": "2024-09-24T03:56:20.139543Z"
    }
   },
   "outputs": [],
   "source": [
    "yt_transcriper = YoutubeTranscripter(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44c6b1b8-d4db-4a51-a010-b5acd6e4afd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:56:27.085262Z",
     "iopub.status.busy": "2024-09-24T03:56:27.084995Z",
     "iopub.status.idle": "2024-09-24T03:56:27.088785Z",
     "shell.execute_reply": "2024-09-24T03:56:27.088351Z",
     "shell.execute_reply.started": "2024-09-24T03:56:27.085240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_time': 0.0,\n",
       "  'title': 'Introduction to GraphRAG and Its Cost Issue',\n",
       "  'end_time': 44.0},\n",
       " {'start_time': 44.0,\n",
       "  'title': 'Understanding Traditional RAG',\n",
       "  'end_time': 106.0},\n",
       " {'start_time': 106.0,\n",
       "  'title': 'Limitations of Traditional RAG',\n",
       "  'end_time': 142.0},\n",
       " {'start_time': 142.0, 'title': 'Introduction to GraphRAG', 'end_time': 159.0},\n",
       " {'start_time': 159.0,\n",
       "  'title': 'Technical Details of GraphRAG',\n",
       "  'end_time': 346.0},\n",
       " {'start_time': 346.0,\n",
       "  'title': 'Setting Up GraphRAG on Your Local Machine',\n",
       "  'end_time': 382.0},\n",
       " {'start_time': 382.0,\n",
       "  'title': 'Running the Indexing Process',\n",
       "  'end_time': 720.0},\n",
       " {'start_time': 720.0,\n",
       "  'title': 'Running Queries with GraphRAG',\n",
       "  'end_time': 866.0},\n",
       " {'start_time': 866.0,\n",
       "  'title': 'Cost Implications and Alternatives',\n",
       "  'end_time': 958}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_transcriper.chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4d99bc0-9915-4737-80e4-25083f4af447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:59:11.859878Z",
     "iopub.status.busy": "2024-09-24T03:59:11.859447Z",
     "iopub.status.idle": "2024-09-24T03:59:11.864254Z",
     "shell.execute_reply": "2024-09-24T03:59:11.863893Z",
     "shell.execute_reply.started": "2024-09-24T03:59:11.859860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import diskcache as dc\n",
    "\n",
    "cache = dc.Cache('./cache-directory')\n",
    "\n",
    "def expensive_function(n):\n",
    "    if (\"expensive_function\", n) in cache:\n",
    "        return cache[(\"expensive_function\", n)]\n",
    "    \n",
    "    result = n * n\n",
    "    cache[(\"expensive_function\", n)] = result\n",
    "    return result\n",
    "\n",
    "print(expensive_function(5))  # Calcula e armazena em cache\n",
    "print(expensive_function(5))  # Recupera do cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ddd21d7-20b3-4d6c-b366-442020243505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:58:34.927438Z",
     "iopub.status.busy": "2024-09-24T03:58:34.927086Z",
     "iopub.status.idle": "2024-09-24T03:58:35.079768Z",
     "shell.execute_reply": "2024-09-24T03:58:35.079208Z",
     "shell.execute_reply.started": "2024-09-24T03:58:34.927421Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('expensive_function', 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpensive_function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/diskcache/core.py:1234\u001b[0m, in \u001b[0;36mCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1232\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(key, default\u001b[38;5;241m=\u001b[39mENOVAL, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m ENOVAL:\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyError\u001b[0m: ('expensive_function', 4)"
     ]
    }
   ],
   "source": [
    "cache[(\"expensive_function\", 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f706eab-0b57-417a-8c21-f22ec8a12b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e806dc-b71c-434b-8e7c-a2770f453158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac0249-1211-4a8d-bc56-ac92244aff8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc55315-d4ec-4eb5-a43b-2364aa03ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "008bd0aa-57ef-4e8f-9f1d-664a576389ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:37:42.809043Z",
     "iopub.status.busy": "2024-09-24T03:37:42.808253Z",
     "iopub.status.idle": "2024-09-24T03:37:45.034883Z",
     "shell.execute_reply": "2024-09-24T03:37:45.034448Z",
     "shell.execute_reply.started": "2024-09-24T03:37:42.808972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vX3A96_F3FU\n",
      "[youtube] vX3A96_F3FU: Downloading webpage\n",
      "[youtube] vX3A96_F3FU: Downloading ios player API JSON\n",
      "[youtube] vX3A96_F3FU: Downloading web creator player API JSON\n",
      "[youtube] vX3A96_F3FU: Downloading m3u8 information\n"
     ]
    }
   ],
   "source": [
    "video_metadata = get_youtube_video_metadata(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e8e7bdb-5c51-4bca-9334-190a69027cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:37:45.035802Z",
     "iopub.status.busy": "2024-09-24T03:37:45.035666Z",
     "iopub.status.idle": "2024-09-24T03:37:45.038605Z",
     "shell.execute_reply": "2024-09-24T03:37:45.038203Z",
     "shell.execute_reply.started": "2024-09-24T03:37:45.035789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_metadata[\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7522ff33-f87a-4397-85e2-a81f60b17a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:37:45.039223Z",
     "iopub.status.busy": "2024-09-24T03:37:45.039073Z",
     "iopub.status.idle": "2024-09-24T03:37:45.043317Z",
     "shell.execute_reply": "2024-09-24T03:37:45.042762Z",
     "shell.execute_reply.started": "2024-09-24T03:37:45.039208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph RAG: Improving RAG with Knowledge Graphs'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_metadata[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "537fd84d-774c-4d42-9f39-729c3903794c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:37:45.044328Z",
     "iopub.status.busy": "2024-09-24T03:37:45.044154Z",
     "iopub.status.idle": "2024-09-24T03:37:45.047058Z",
     "shell.execute_reply": "2024-09-24T03:37:45.046109Z",
     "shell.execute_reply.started": "2024-09-24T03:37:45.044313Z"
    }
   },
   "outputs": [],
   "source": [
    "if video_metadata[\"chapters\"]:\n",
    "    _download_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7248355e-3673-4071-a75f-07203516d797",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T03:54:34.639575Z",
     "iopub.status.busy": "2024-09-24T03:54:34.639254Z",
     "iopub.status.idle": "2024-09-24T03:54:34.641786Z",
     "shell.execute_reply": "2024-09-24T03:54:34.641403Z",
     "shell.execute_reply.started": "2024-09-24T03:54:34.639558Z"
    }
   },
   "outputs": [],
   "source": [
    "# video_metadata[\"chapters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203699f-79ce-43de-bd6a-6a9e7c7247cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036e7ca-43e3-4bbe-9d4b-50caa50bcf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56610be5-a6d9-4c3e-8614-61f0c8c8a53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98071c-09ea-44a2-9073-ae86eb0effbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5b4635-5d29-49d5-b6b5-b2bb20b15cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:55:12.700645Z",
     "iopub.status.busy": "2024-09-16T04:55:12.697950Z",
     "iopub.status.idle": "2024-09-16T04:55:12.747129Z",
     "shell.execute_reply": "2024-09-16T04:55:12.746076Z",
     "shell.execute_reply.started": "2024-09-16T04:55:12.700519Z"
    }
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "\n",
    "# Inicializar o sistema de logs da Abseil\n",
    "logging.set_verbosity(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d297d047-6007-4243-a273-d585412f061e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:45:26.650962Z",
     "iopub.status.busy": "2024-09-16T04:45:26.650735Z",
     "iopub.status.idle": "2024-09-16T04:45:26.653170Z",
     "shell.execute_reply": "2024-09-16T04:45:26.652695Z",
     "shell.execute_reply.started": "2024-09-16T04:45:26.650946Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ba9455-5f5e-4eb1-98e4-c88217fcbede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:55:38.104901Z",
     "iopub.status.busy": "2024-09-16T04:55:38.104620Z",
     "iopub.status.idle": "2024-09-16T04:55:39.751130Z",
     "shell.execute_reply": "2024-09-16T04:55:39.750761Z",
     "shell.execute_reply.started": "2024-09-16T04:55:38.104877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726462539.725237     341 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462539.748697     341 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462539.748960     341 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "\n",
    "# Redirecionar STDERR temporariamente\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "# Inicializar o TensorFlow (ou qualquer código que gere os avisos)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Restaurar STDERR\n",
    "sys.stderr = sys.__stderr__\n",
    "\n",
    "# Continuar o código normal\n",
    "logging.set_verbosity(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9274fd57-25f0-4089-b26a-22750bb3ec97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:50:55.732707Z",
     "iopub.status.busy": "2024-09-16T04:50:55.732242Z",
     "iopub.status.idle": "2024-09-16T04:50:55.818525Z",
     "shell.execute_reply": "2024-09-16T04:50:55.818040Z",
     "shell.execute_reply.started": "2024-09-16T04:50:55.732685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3946435560444492848\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14230618112\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2834319952901452859\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726462255.734598      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462255.734887      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462255.735040      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462255.815505      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462255.815666      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726462255.815792      60 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c436ab67-f9ea-4aa0-b537-56c449cd7486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T04:53:51.219949Z",
     "iopub.status.busy": "2024-09-16T04:53:51.219701Z",
     "iopub.status.idle": "2024-09-16T04:53:51.225061Z",
     "shell.execute_reply": "2024-09-16T04:53:51.224696Z",
     "shell.execute_reply.started": "2024-09-16T04:53:51.219930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configurar TensorFlow para mostrar apenas erros\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63670064-96a6-4e29-bb7b-b4464f9a033b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
