{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1094c5f-0f29-48e7-8538-acb0aee05fa5",
   "metadata": {},
   "source": [
    "# Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ae98-bc58-4e10-87cd-1d05275789a0",
   "metadata": {},
   "source": [
    "### Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6e1488-81b0-4dd5-b261-35c4b9037060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:26.018680Z",
     "iopub.status.busy": "2025-02-16T05:53:26.018327Z",
     "iopub.status.idle": "2025-02-16T05:53:28.712849Z",
     "shell.execute_reply": "2025-02-16T05:53:28.712386Z",
     "shell.execute_reply.started": "2025-02-16T05:53:26.018652Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import flash_attn\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "from huggingface_hub import Repository, whoami\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dcc76d-0ff2-493c-bde6-3367de27da1d",
   "metadata": {},
   "source": [
    "### Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f513d9-6b3f-4a05-9adb-acbc97ec5e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.713511Z",
     "iopub.status.busy": "2025-02-16T05:53:28.713278Z",
     "iopub.status.idle": "2025-02-16T05:53:28.727830Z",
     "shell.execute_reply": "2025-02-16T05:53:28.727478Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.713499Z"
    }
   },
   "outputs": [],
   "source": [
    "AMOUNT_OF_NEWS = 3535\n",
    "AMOUNT_OF_SENTENCES = 3456\n",
    "\n",
    "MODEL_ID = \"neuralmind/bert-base-portuguese-cased\"\n",
    "DATASET_ID = \"emdemor/news-of-the-brazilian-newspaper\"\n",
    "USERNAME = \"emdemor\"\n",
    "TOKENIZER_PATH = \"domain_tokenizer\"\n",
    "TESTING = True\n",
    "FLASH_ATTENTION = False\n",
    "PUSH_INTERVAL = 10_000 if TESTING else 100_000\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
    "TRAINED_MODEL_PATH = f\"{MODEL_NAME}-ptbr-{'test' if TESTING else 'full'}\"\n",
    "\n",
    "# A probabilidade de MLM determina quantos tokens serão mascarados durante o treinamento\n",
    "# Usar diferentes probabilidades pode ajudar o modelo a aprender melhor\n",
    "# Aqui podemos aplicar o conceito de \"Aprendizagem Curricular\", onde:\n",
    "#   1. Começamos com tarefas mais simples\n",
    "#   2. Gradualmente aumentamos a dificuldade\n",
    "#   3. Permitimos que o modelo construa competências de forma incremental\n",
    "#\n",
    "# Como prever 14% dos tokens mascarados de uma sentença é mais fácil do que prever 30%,\n",
    "# podemos começar do menor para o maior\n",
    "#\n",
    "# Benefícios desta abordagem:\n",
    "#  1. O modelo primeiro aprende padrões básicos com menos tokens mascarados\n",
    "#  2. Gradualmente enfrenta desafios mais complexos à medida que ganha competência\n",
    "#  3. Potencialmente leva a um aprendizado mais estável e eficaz\n",
    "MLM_PROBABILITIES = [0.05, 0.10, 0.15, 0.20, 0.30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b15b1-0224-4f82-89cd-7c2baa562663",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebe4e52-6f69-419e-9bf4-c9a88270b5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.728483Z",
     "iopub.status.busy": "2025-02-16T05:53:28.728310Z",
     "iopub.status.idle": "2025-02-16T05:53:28.776743Z",
     "shell.execute_reply": "2025-02-16T05:53:28.776398Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.728471Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd30664-4760-4775-ae14-7392209af39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.777337Z",
     "iopub.status.busy": "2025-02-16T05:53:28.777207Z",
     "iopub.status.idle": "2025-02-16T05:53:28.780931Z",
     "shell.execute_reply": "2025-02-16T05:53:28.780301Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.777325Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_attention(model):\n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            print(\"Flash Attention não é compatível:\", str(e))\n",
    "            return False\n",
    "\n",
    "    if FLASH_ATTENTION and check_flash_attention_support():\n",
    "        print(\"Replacing standard attention with FlashAttention...\")\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.MultiheadAttention):\n",
    "                module.attention = FlashAttention()\n",
    "        print(\"FlashAttention integrated.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93d22d2-9892-4f6b-8544-58564787b4f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.782261Z",
     "iopub.status.busy": "2025-02-16T05:53:28.781746Z",
     "iopub.status.idle": "2025-02-16T05:53:28.786039Z",
     "shell.execute_reply": "2025-02-16T05:53:28.784685Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.782235Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_vocab_size(tokenizer, model):\n",
    "\n",
    "    # Verifique o maior ID no tokenizador\n",
    "    max_token_id = max(tokenizer.get_vocab().values())\n",
    "    print(\"Maior ID no tokenizador:\", max_token_id)\n",
    "\n",
    "    # Verifique o tamanho do vocabulário do modelo\n",
    "    print(\"Tamanho do vocabulário do modelo:\", model.config.vocab_size)\n",
    "\n",
    "    # Se o maior ID for maior ou igual ao tamanho do vocabulário, há um problema\n",
    "    assert max_token_id < model.config.vocab_size, \"IDs de tokens fora do intervalo!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb760b0-1889-4fd8-865c-9fbaaa3328b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8388afe3-3097-4a38-bf9d-a1dd1003765c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.788373Z",
     "iopub.status.busy": "2025-02-16T05:53:28.788038Z",
     "iopub.status.idle": "2025-02-16T05:53:28.859976Z",
     "shell.execute_reply": "2025-02-16T05:53:28.859519Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.788330Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tabulate\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    dataset_size: int\n",
    "    num_train_epochs: int\n",
    "    num_chunks: int\n",
    "    train_batch_size_per_device: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "\n",
    "    @field_validator(\"num_chunks\")\n",
    "    def validate_num_chunks(cls, v, info):\n",
    "        data = info.data\n",
    "        if (\n",
    "            \"dataset_size\" in data\n",
    "            and \"dataset_size\" in data\n",
    "            and \"eval_size_ratio\" in data\n",
    "        ):\n",
    "            dataset_size = data[\"dataset_size\"]\n",
    "            eval_size_per_chunk = int(data[\"dataset_size\"] * data[\"eval_size_ratio\"])\n",
    "            available_size = dataset_size - eval_size_per_chunk * v\n",
    "            if available_size < v:\n",
    "                raise ValueError(\n",
    "                    f\"available_size ({available_size}) deve ser maior ou igual a num_chunks ({v})\"\n",
    "                )\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        return self.train_batch_size_per_device * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        return math.ceil(self.dataset_size / self.effective_batch_size)\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        \"\"\"Tamanho do dataset de avaliação em cada chunk\"\"\"\n",
    "        return int(self.dataset_size * self.eval_size_ratio / self.num_chunks)\n",
    "\n",
    "    @property\n",
    "    def available_size(self):\n",
    "        return self.dataset_size - self.eval_size_per_chunk * self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def eval_size(self):\n",
    "        return self.dataset_size - self.available_size\n",
    "\n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        return self.dataset_size // self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def chunk_train_size(self):\n",
    "        return self.available_size // self.num_chunks\n",
    "\n",
    "    def __repr(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"dataset_size\", self.dataset_size],\n",
    "            [\"num_chunks\", self.num_chunks],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"chunk_train_size\", self.chunk_train_size],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"available_size\", self.available_size],\n",
    "            [\"eval_size\", self.eval_size],\n",
    "            [\"train_batch_size_per_device\", self.train_batch_size_per_device],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__repr()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c8632-f7b5-458b-8353-b89d2a9099f4",
   "metadata": {},
   "source": [
    "### Preparar a Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270be0bf-c764-4446-a1f1-efbd8805c3d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:28.860545Z",
     "iopub.status.busy": "2025-02-16T05:53:28.860418Z",
     "iopub.status.idle": "2025-02-16T05:53:33.753043Z",
     "shell.execute_reply": "2025-02-16T05:53:33.752413Z",
     "shell.execute_reply.started": "2025-02-16T05:53:28.860534Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Load and prepare the dataset for training.\"\"\"\n",
    "\n",
    "raw_dataset = load_dataset(DATASET_ID, split=\"train\")\n",
    "df = raw_dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "sample_df = df.sample(min(AMOUNT_OF_NEWS, len(df)))\n",
    "combined_texts = sample_df[\"text\"].to_list() + sample_df[\"title\"].to_list()\n",
    "sentences = [\n",
    "    phrase for text in combined_texts if text for phrase in split_into_sentences(text)\n",
    "]\n",
    "sentences_sample = pd.Series(sentences).sample(AMOUNT_OF_SENTENCES).to_list()\n",
    "dataset = Dataset.from_dict({\"text\": sentences_sample})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d29bd-c5b7-4e30-be5e-5622e5ddbcda",
   "metadata": {},
   "source": [
    "### Setup model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a94a4df-a868-4f29-aafd-63720a039524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:33.753759Z",
     "iopub.status.busy": "2025-02-16T05:53:33.753632Z",
     "iopub.status.idle": "2025-02-16T05:53:35.629286Z",
     "shell.execute_reply": "2025-02-16T05:53:35.628817Z",
     "shell.execute_reply.started": "2025-02-16T05:53:33.753746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maior ID no tokenizador: 32767\n",
      "Tamanho do vocabulário do modelo: 32768\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_PATH, clean_up_tokenization_spaces=False\n",
    ")\n",
    "config = AutoConfig.from_pretrained(MODEL_ID)\n",
    "config.torch_dtype = torch.float16\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_ID, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(DEVICE)\n",
    "model = set_attention(model)\n",
    "check_vocab_size(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d40020d-689d-4d78-9277-1c11bc88da03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.630026Z",
     "iopub.status.busy": "2025-02-16T05:53:35.629768Z",
     "iopub.status.idle": "2025-02-16T05:53:35.634138Z",
     "shell.execute_reply": "2025-02-16T05:53:35.633379Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.630013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------+\n",
      "| Attribute                   |   Value |\n",
      "+=============================+=========+\n",
      "| num_train_epochs            |     3   |\n",
      "+-----------------------------+---------+\n",
      "| dataset_size                |  3456   |\n",
      "+-----------------------------+---------+\n",
      "| num_chunks                  |     5   |\n",
      "+-----------------------------+---------+\n",
      "| chunk_size                  |   691   |\n",
      "+-----------------------------+---------+\n",
      "| chunk_train_size            |   622   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size_per_chunk         |    69   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size_ratio             |     0.1 |\n",
      "+-----------------------------+---------+\n",
      "| available_size              |  3111   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size                   |   345   |\n",
      "+-----------------------------+---------+\n",
      "| train_batch_size_per_device |     4   |\n",
      "+-----------------------------+---------+\n",
      "| gradient_accumulation_steps |     2   |\n",
      "+-----------------------------+---------+\n",
      "| total_save_limit            |     2   |\n",
      "+-----------------------------+---------+\n",
      "| effective_batch_size        |     8   |\n",
      "+-----------------------------+---------+\n",
      "| total_steps_per_epoch       |   432   |\n",
      "+-----------------------------+---------+\n",
      "| total_train_steps           |  1296   |\n",
      "+-----------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    num_train_epochs=3,\n",
    "    dataset_size=len(dataset),\n",
    "    num_chunks=len(MLM_PROBABILITIES),\n",
    "    train_batch_size_per_device=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_size_ratio=0.10,\n",
    "    total_save_limit=2,\n",
    "    estimated_dataset_size_in_rows=len(dataset),\n",
    ")\n",
    "\n",
    "\n",
    "print(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab4428-958c-4b1d-a747-17296e4c8aaf",
   "metadata": {},
   "source": [
    "### Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4312d769-bd1a-48c9-80c3-bec9a69ee3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.635097Z",
     "iopub.status.busy": "2025-02-16T05:53:35.634879Z",
     "iopub.status.idle": "2025-02-16T05:53:35.789324Z",
     "shell.execute_reply": "2025-02-16T05:53:35.788919Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.635083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e394a7e2c17247378765132c6b116614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples, target_column=\"text\"):\n",
    "    return tokenizer(\n",
    "        examples[target_column],\n",
    "        # No truncation and max_length to allow dynamic padding truncation=True, max_length=chunk_size, padding=\"longest\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b214a2-8748-413d-8b29-251a1c93ae1e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01325f5-c84c-461e-b5d1-5d97d2e7ff3c",
   "metadata": {},
   "source": [
    "## Definindo o DataCollator\n",
    "\n",
    "### O que é um DataCollator no Contexto de Processamento de Linguagem Natural\n",
    "\n",
    "Um DataCollator é uma componente fundamental no pipeline de treinamento de modelos de linguagem natural, especialmente quando trabalhamos com bibliotecas como a Hugging Face Transformers. É uma função ou classe responsável por agrupar vários exemplos individuais em um único lote (batch) que pode ser processado eficientemente pelo modelo durante o treinamento. O nome \"collator\" vem do verbo \"collate\", que significa reunir e organizar informações.\n",
    "\n",
    "#### Por que Precisamos de DataCollators?\n",
    "\n",
    "Quando treinamos modelos de linguagem, geralmente trabalhamos com textos de comprimentos variados. No entanto, os modelos de deep learning, especialmente aqueles implementados em PyTorch ou TensorFlow, esperam tensores de forma consistente (retangular). Isso cria um desafio:\n",
    "\n",
    "1. Como transformar exemplos de tamanhos diferentes em uma estrutura uniforme?\n",
    "2. Como fazer isso de maneira eficiente sem desperdiçar recursos computacionais?\n",
    "\n",
    "É aqui que entra o DataCollator.\n",
    "\n",
    "#### Funções Principais de um DataCollator\n",
    "\n",
    "##### 1. Padding (Preenchimento)\n",
    "\n",
    "A função mais importante é o preenchimento (padding). Como as sequências têm comprimentos diferentes, o DataCollator adiciona tokens especiais (geralmente 0s) para que todas as sequências em um lote tenham o mesmo comprimento.\n",
    "\n",
    "Por exemplo, se tivermos estas três sequências:\n",
    "```\n",
    "[101, 2054, 2003, 102]  # \"What is\"\n",
    "[101, 2054, 2003, 2023, 2003, 102]  # \"What is this\"\n",
    "[101, 2054, 2003, 2023, 102]  # \"What is this\"\n",
    "```\n",
    "\n",
    "O DataCollator as transformaria em:\n",
    "```\n",
    "[101, 2054, 2003, 102, 0, 0]\n",
    "[101, 2054, 2003, 2023, 2003, 102]\n",
    "[101, 2054, 2003, 2023, 102, 0]\n",
    "```\n",
    "\n",
    "##### 2. Mascaramento para MLM (no caso de DataCollatorForLanguageModeling)\n",
    "\n",
    "No código que você compartilhou, está sendo usado o `DataCollatorForLanguageModeling`, que é especializado para treinar modelos de linguagem mascarada (como BERT). Além do padding, ele:\n",
    "\n",
    "- Seleciona aleatoriamente uma porcentagem dos tokens (definida por `mlm_probability`)\n",
    "- Substitui esses tokens por um token especial [MASK]\n",
    "- Mantém o registro dos tokens originais para calcular a perda durante o treinamento\n",
    "\n",
    "Por exemplo, com `mlm_probability=0.15`:\n",
    "```\n",
    "Original: [101, 2054, 2003, 2023, 2003, 102]\n",
    "Mascarado: [101, 2054, [MASK], 2023, 2003, 102]  # \"2003\" foi mascarado\n",
    "```\n",
    "\n",
    "##### 3. Otimização de Memória\n",
    "\n",
    "Um bom DataCollator também otimiza o uso de memória. No código que você compartilhou, o `DynamicPaddingDataCollator` é um excelente exemplo disso:\n",
    "\n",
    "- Em vez de preencher todas as sequências até um comprimento máximo fixo (por exemplo, 512 tokens)\n",
    "- Ele calcula o comprimento máximo dentro do lote atual\n",
    "- Preenche apenas até esse comprimento\n",
    "\n",
    "Isso economiza muita memória quando a maioria das sequências em um lote é curta.\n",
    "\n",
    "#### O Caso Específico do DynamicPaddingDataCollator\n",
    "\n",
    "O `DynamicPaddingDataCollator` no seu código é uma extensão personalizada do `DataCollatorForLanguageModeling` padrão da Hugging Face. Ele:\n",
    "\n",
    "1. Encontra o comprimento máximo dentro do lote atual\n",
    "2. Para cada exemplo:\n",
    "   - Calcula quanto preenchimento é necessário\n",
    "   - Adiciona tokens de padding aos IDs de entrada\n",
    "   - Adiciona zeros à máscara de atenção correspondente\n",
    "   - Ou trunca se necessário (se a sequência for mais longa que o permitido)\n",
    "3. Aplica a lógica de mascaramento do MLM\n",
    "4. Garante que as formas e tipos dos tensores estão corretos usando a função `fix_batch_inputs`\n",
    "\n",
    "#### Analogia para Entender Melhor\n",
    "\n",
    "Pense no DataCollator como o gerente de uma mesa em um restaurante:\n",
    "\n",
    "1. Várias pessoas (exemplos) chegam em grupos diferentes\n",
    "2. O gerente precisa organizá-las em mesas (lotes) de tamanho fixo\n",
    "3. Algumas mesas podem ter espaços vazios (padding)\n",
    "4. Para um jogo especial durante o jantar (MLM), o gerente venda os olhos de algumas pessoas aleatoriamente\n",
    "\n",
    "#### Por que o Dynamic Padding é Importante?\n",
    "\n",
    "Considere dois cenários com 100 sequências, onde a maioria tem 20 tokens, mas algumas poucas têm 500:\n",
    "\n",
    "1. **Padding Fixo**: Todas as 100 sequências são preenchidas até 500 tokens = 50.000 tokens totais\n",
    "2. **Padding Dinâmico**: \n",
    "   - Lote 1: 16 sequências, max_len=20 → 320 tokens\n",
    "   - Lote 2: 16 sequências, max_len=25 → 400 tokens\n",
    "   - ...\n",
    "   - Último lote: 4 sequências, max_len=500 → 2.000 tokens\n",
    "   - Total muito menor que 50.000!\n",
    "\n",
    "Essa economia permite:\n",
    "- Treinar com lotes maiores\n",
    "- Usar menos memória GPU\n",
    "- Treinar modelos maiores\n",
    "- Acelerar o treinamento\n",
    "\n",
    "#### Conclusão\n",
    "\n",
    "O DataCollator é uma peça crucial que prepara os dados brutos para o consumo eficiente pelo modelo. No caso do MLM, ele também implementa a estratégia de mascaramento que é central para o aprendizado. A versão com padding dinâmico que você está usando representa uma otimização inteligente que pode melhorar significativamente a eficiência do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806e5df3-5c1b-49ac-869c-f5ff398e1caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.790059Z",
     "iopub.status.busy": "2025-02-16T05:53:35.789906Z",
     "iopub.status.idle": "2025-02-16T05:53:35.798685Z",
     "shell.execute_reply": "2025-02-16T05:53:35.798048Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.790046Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper Function to Fix Batch Inputs ---\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Esta função tem como objetivo garantir que os tensores de entrada tenham a forma e o tipo corretos:\n",
    "\n",
    "    - Ela verifica três chaves importantes: \"input_ids\", \"attention_mask\" e \"token_type_ids\"\n",
    "    - Remove dimensões extras (por exemplo, converte [1, batch, seq_len] para [batch, seq_len])\n",
    "    - Converte \"input_ids\" para o tipo torch.long, que é o tipo esperado para IDs de tokens\n",
    "    - Isso é importante porque inconsistências na forma dos tensores podem causar erros durante o treinamento\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# --- Forward Pass Function ---\n",
    "def forward_pass(model, inputs):\n",
    "    \"\"\"\n",
    "    Esta função realiza uma passagem para frente (forward pass) no modelo:\n",
    "\n",
    "    - Primeiro, aplica a função fix_batch_inputs para garantir que as entradas estão corretas\n",
    "    - Move os tensores para o dispositivo apropriado (CPU ou GPU)\n",
    "    - Usa torch.amp.autocast para habilitar precisão mista automática quando estiver usando GPU\n",
    "      - A precisão mista acelera o treinamento e reduz o uso de memória\n",
    "    - Executa o modelo com as entradas e solicita que retorne um dicionário completo\n",
    "    - Verifica se o modelo retornou uma perda (loss) e a retorna\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, eval_dataset, data_collator, batch_size):\n",
    "    \"\"\"\n",
    "    Esta função avalia o desempenho do modelo no conjunto de dados de avaliação:\n",
    "\n",
    "    - Coloca o modelo em modo de avaliação (model.eval())\n",
    "    - Itera sobre o conjunto de dados de avaliação em lotes\n",
    "    - Para cada lote:\n",
    "      - Desativa o cálculo de gradientes com torch.no_grad()\n",
    "      - Usa precisão mista se estiver em GPU\n",
    "      - Calcula a perda e a adiciona à lista de perdas\n",
    "      - Captura e imprime erros, mas continua a avaliação\n",
    "    - Retorna ao modo de treinamento (model.train())\n",
    "    - Calcula e retorna a perda média\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=batch_size)\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
    "        ):\n",
    "            inputs = data_collator(batch)\n",
    "            try:\n",
    "                loss = forward_pass(model, inputs)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Esta classe estende DataCollatorForLanguageModeling e implementa um colator de dados com preenchimento dinâmico:\n",
    "\n",
    "    - O preenchimento dinâmico significa que cada lote é preenchido apenas até o comprimento da sequência mais longa naquele lote específico\n",
    "    - Isso é mais eficiente que usar um comprimento fixo para todos os lotes\n",
    "    - Para cada exemplo no lote:\n",
    "      - Calcula quanto padding é necessário\n",
    "      - Adiciona tokens de padding aos IDs de entrada e zeros às máscaras de atenção\n",
    "      - Ou trunca se necessário\n",
    "    - Aplica a lógica de colação de dados do MLM (mascaramento aleatório de tokens)\n",
    "    - Garante formas e tipos corretos com fix_batch_inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(\n",
    "            batch\n",
    "        )  # Use torch_call instead of __call__ to call the parent's method\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b7820-a133-48ce-b59d-4a6265cf7029",
   "metadata": {},
   "source": [
    "### Entendendo Chunks, Batches e Lotes no Treinamento de Modelos\n",
    "\n",
    "#### Primeiro, vamos definir cada termo:\n",
    "\n",
    "##### Lote (Batch)\n",
    "\"Lote\" e \"batch\" são na verdade a mesma coisa - batch é simplesmente o termo em inglês para lote. No contexto de aprendizado de máquina:\n",
    "\n",
    "- Um lote/batch é um grupo de exemplos que são processados juntos pelo modelo\n",
    "- O tamanho do lote (batch size) é um hiperparâmetro importante que afeta a velocidade e a estabilidade do treinamento\n",
    "- No código, você vê referências como `batch_size=training_config.per_device_train_batch_size`\n",
    "\n",
    "##### Chunk\n",
    "Um chunk é uma divisão maior do dataset, que contém múltiplos lotes. No seu código:\n",
    "\n",
    "- O dataset inteiro é dividido em chunks\n",
    "- Cada chunk é associado a uma probabilidade diferente de MLM\n",
    "- Você vê isso em `chunk_size_dataset = available_size // num_chunks`\n",
    "\n",
    "#### Como eles se relacionam (do maior para o menor):\n",
    "\n",
    "1. **Dataset completo**: Todos os seus dados de treinamento\n",
    "2. **Chunks**: Grandes divisões do dataset, cada um usado com uma configuração específica (ex: diferente probabilidade de MLM)  \n",
    "3. **Lotes/Batches**: Pequenos grupos de exemplos processados juntos durante o treinamento\n",
    "\n",
    "\n",
    "#### Por que essa estrutura é usada?\n",
    "\n",
    "1. **Razão para usar chunks com diferentes mlm_probabilities**:\n",
    "   - Diferentes taxas de mascaramento ajudam o modelo a aprender de forma mais robusta\n",
    "   - Uma estratégia de \"currículo\" onde o modelo é exposto a desafios de dificuldade crescente\n",
    "\n",
    "2. **Razão para usar lotes/batches**:\n",
    "   - Limitações de memória: não é possível processar todos os dados de uma vez\n",
    "   - Estabilidade de treinamento: atualizar os pesos após cada lote é mais estável que após cada exemplo\n",
    "   - Eficiência computacional: processamento em paralelo de múltiplos exemplos\n",
    "\n",
    "#### Fluxo Completo no Código:\n",
    "\n",
    "1. O dataset é tokenizado (`tokenized_dataset`)\n",
    "2. É dividido em `num_chunks` chunks\n",
    "3. Para cada chunk:\n",
    "   - É criado um data collator com uma probabilidade MLM específica\n",
    "   - O chunk é dividido em lotes (batches)\n",
    "   - Cada lote é processado, a perda é calculada, e os pesos do modelo são atualizados\n",
    "\n",
    "Este método de \"treinamento em camadas\" (épocas → chunks → lotes) permite um treinamento mais eficiente e eficaz do modelo de linguagem mascarada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a008f1-07c5-45bb-8cad-c07764aa1fba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.799897Z",
     "iopub.status.busy": "2025-02-16T05:53:35.799618Z",
     "iopub.status.idle": "2025-02-16T05:53:35.881043Z",
     "shell.execute_reply": "2025-02-16T05:53:35.880505Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.799875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset = 3456\n",
      "Serão definidos 5 chunks de 691 dados\n",
      "Cada chunk terá 622 dados de treino e 69 dados de validação.\n",
      "O número de dados de treino será, portanto: 3111\n",
      "O número de dados de validação será: 345\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamanho do dataset = {training_config.dataset_size}\")\n",
    "print(\n",
    "    f\"Serão definidos {training_config.num_chunks} chunks de {training_config.chunk_size} dados\"\n",
    ")\n",
    "print(\n",
    "    f\"Cada chunk terá {training_config.chunk_train_size} dados de treino e {training_config.eval_size_per_chunk} dados de validação.\"\n",
    ")\n",
    "print(f\"O número de dados de treino será, portanto: {training_config.available_size}\")\n",
    "print(f\"O número de dados de validação será: {training_config.eval_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4227c5-d17b-430c-84b5-0deea03e2f93",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02cd5039-28f4-4e02-8c3e-eeceb342c2b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.881955Z",
     "iopub.status.busy": "2025-02-16T05:53:35.881730Z",
     "iopub.status.idle": "2025-02-16T05:53:35.890085Z",
     "shell.execute_reply": "2025-02-16T05:53:35.889197Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.881938Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_log(epoch, chunk_number, training_config):\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1}/{training_config.num_train_epochs} | \"\n",
    "        f\"MLM Probability: {MLM_PROBABILITIES[chunk_number]}\"\n",
    "    )\n",
    "\n",
    "def chunk_train_test_split(chunk_number, training_config):\n",
    "\n",
    "    eval_start_idx = chunk_number * training_config.chunk_size\n",
    "    eval_end_idx = eval_start_idx + training_config.eval_size_per_chunk - 1\n",
    "    train_start_idx = (\n",
    "        chunk_number * training_config.chunk_size\n",
    "        + training_config.eval_size_per_chunk\n",
    "    )\n",
    "    train_end_idx = train_start_idx + training_config.chunk_train_size - 1\n",
    "    \n",
    "    print(\n",
    "        f\"\\tSplitting | \"\n",
    "        f\"chunk: {eval_start_idx}-{train_end_idx} | \"\n",
    "        f\"eval: {eval_start_idx}-{eval_end_idx} | \"\n",
    "        f\"train: {train_start_idx}-{train_end_idx}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_dataset = (\n",
    "        tokenized_dataset.skip(train_start_idx)\n",
    "        .take(training_config.chunk_train_size)\n",
    "        .shuffle(seed=42)\n",
    "    )\n",
    "    \n",
    "    eval_dataset = (\n",
    "        tokenized_dataset.skip(eval_start_idx)\n",
    "        .take(training_config.eval_size_per_chunk)\n",
    "        .shuffle(seed=42)\n",
    "    )\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "def eval_loss(model, batch):\n",
    "    inputs = data_collator(batch)\n",
    "    loss = forward_pass(model, inputs)\n",
    "    return loss\n",
    "\n",
    "def update_model_parameters(scaler, optimizer, scheduler):\n",
    "    \"\"\"Atualiza os parâmetros do modelo, ajusta o learning rate e limpa os gradientes.\"\"\"\n",
    "    global global_step\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()  # Limpa a memória da GPU\n",
    "    global_step += 1\n",
    "\n",
    "\n",
    "def evaluate_step(model, eval_dataset, data_collator, training_config):\n",
    "    global global_step\n",
    "    eval_interval = training_config.total_steps_per_epoch // (training_config.num_train_epochs * 4)\n",
    "    if eval_interval > 0 and (global_step % eval_interval == 0):\n",
    "        eval_loss = evaluate(model, eval_dataset, data_collator, batch_size=training_config.train_batch_size_per_device)\n",
    "        print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "\n",
    "\n",
    "def push_to_hub(tokenizer, model): \n",
    "    global global_step\n",
    "    # Push to hub incl TESTING\n",
    "    if global_step % PUSH_INTERVAL == 0:\n",
    "        print(f\"Saving and pushing model at step {global_step}...\")\n",
    "        model.save_pretrained(TRAINED_MODEL_PATH)\n",
    "        tokenizer.save_pretrained(TRAINED_MODEL_PATH)\n",
    "        print(f\"Model saved and pushed at step {global_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7f6198-2d6e-4e41-9397-b759843b70d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.891765Z",
     "iopub.status.busy": "2025-02-16T05:53:35.891324Z",
     "iopub.status.idle": "2025-02-16T05:53:35.895625Z",
     "shell.execute_reply": "2025-02-16T05:53:35.894908Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.891744Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_WARMUP_STEPS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11cb4dc4-688b-4486-923e-b758e3b1f22d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:35.896784Z",
     "iopub.status.busy": "2025-02-16T05:53:35.896447Z",
     "iopub.status.idle": "2025-02-16T05:53:36.201308Z",
     "shell.execute_reply": "2025-02-16T05:53:36.200469Z",
     "shell.execute_reply.started": "2025-02-16T05:53:35.896767Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "# O torch.amp.GradScaler é uma ferramenta do PyTorch projetada para facilitar o treinamento de\n",
    "# modelos utilizando precisão mista (mixed precision training). Em resumo, ele serve para\n",
    "# escalonar (amplificar) os gradientes durante o backpropagation de forma a evitar problemas\n",
    "# numéricos, como o underflow, que podem ocorrer ao usar representações de 16 bits (FP16).\n",
    "# - Como Funciona\n",
    "#   1. Escalonamento dos Gradientes: Ao multiplicar o valor do loss por um fator de escala,\n",
    "#      os gradientes calculados ficam em uma faixa numérica mais segura, evitando que valores\n",
    "#      muito pequenos se percam durante os cálculos.\n",
    "#   2. Desescalonamento: Antes de atualizar os parâmetros do modelo, o GradScaler desfaz essa\n",
    "#      multiplicação, garantindo que as atualizações ocorram com os valores corretos.\n",
    "#   3. Ajuste Dinâmico da Escala: O GradScaler monitora a ocorrência de overflows (quando os\n",
    "#      valores ficam excessivamente grandes) e ajusta automaticamente o fator de escala,\n",
    "#      aumentando ou diminuindo conforme necessário para manter a estabilidade do treinamento.\n",
    "scaler = torch.amp.GradScaler(DEVICE, enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
    "    num_training_steps=training_config.total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a254fb-8c07-4eb7-90bc-43e2ff616e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T05:53:36.203400Z",
     "iopub.status.busy": "2025-02-16T05:53:36.203274Z",
     "iopub.status.idle": "2025-02-16T06:06:45.044372Z",
     "shell.execute_reply": "2025-02-16T06:06:45.043665Z",
     "shell.execute_reply.started": "2025-02-16T05:53:36.203389Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(training_config.num_train_epochs):\n",
    "    for chunk_number, mlm_probability in enumerate(MLM_PROBABILITIES):\n",
    "        start_log(epoch, chunk_number, training_config)\n",
    "        data_collator = DynamicPaddingDataCollator(tokenizer, mlm_probability=mlm_probability)\n",
    "        train_dataset, eval_dataset = chunk_train_test_split(chunk_number, training_config)\n",
    "        train_iterator = train_dataset.iter(batch_size=training_config.train_batch_size_per_device)\n",
    "        for step, batch in tqdm(enumerate(train_iterator), desc=f\"Training (MLM {mlm_probability})\"):\n",
    "            accumulation_step_complete = (step + 1) % training_config.gradient_accumulation_steps == 0\n",
    "            try:\n",
    "                loss = eval_loss(model, batch)\n",
    "                scaler.scale(loss / training_config.gradient_accumulation_steps).backward()\n",
    "                if accumulation_step_complete:\n",
    "                    update_model_parameters(scaler, optimizer, scheduler)\n",
    "                    evaluate_step(model, eval_dataset, data_collator, training_config)\n",
    "                    push_to_hub(tokenizer, model)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629cb2f-7e36-47d9-96b1-c6ffe8e7bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644e2f1-16ef-4000-b281-fbbe8f129d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2776b3-e851-4a7f-a8b5-39a707250fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44636d98-9598-4740-a55a-55999933b166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0d8f1-5fb8-4ff7-aa13-0bbf412fc6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb958f-957d-4f87-957b-dfc82156b901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d16b1e-dea9-4faf-a123-4b9e28729004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7377e1-7322-40f7-a2d7-ce3ffe318544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970345b8-1623-466c-b307-f1e9fb964d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
