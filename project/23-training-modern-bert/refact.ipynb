{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b2926-dff2-42d6-9fff-3734b967b53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:15:42.787454Z",
     "iopub.status.busy": "2025-02-15T05:15:42.787137Z",
     "iopub.status.idle": "2025-02-15T05:15:42.789969Z",
     "shell.execute_reply": "2025-02-15T05:15:42.789362Z",
     "shell.execute_reply.started": "2025-02-15T05:15:42.787433Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y\n",
    "# !pip install transformers==4.48.3 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7b1f87-a155-469c-be22-ccd0cacb7acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:54.484100Z",
     "iopub.status.busy": "2025-02-15T05:57:54.483736Z",
     "iopub.status.idle": "2025-02-15T05:57:57.536357Z",
     "shell.execute_reply": "2025-02-15T05:57:57.535709Z",
     "shell.execute_reply.started": "2025-02-15T05:57:54.484072Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "    os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import flash_attn\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "from huggingface_hub import Repository, whoami\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247c792a-5943-436c-97a5-9166712ee823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.538515Z",
     "iopub.status.busy": "2025-02-15T05:57:57.538379Z",
     "iopub.status.idle": "2025-02-15T05:57:57.579583Z",
     "shell.execute_reply": "2025-02-15T05:57:57.578799Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.538502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EXAMPLES_TO_TRAIN = 3000\n",
    "MODEL_CHECKPOINT = \"neuralmind/bert-base-portuguese-cased\"\n",
    "USERNAME = \"emdemor\"\n",
    "TOKENIZER_PATH = \"domain_tokenizer\"\n",
    "TESTING = True\n",
    "FLASH_ATTENTION = False\n",
    "PUSH_INTERVAL = 10_000 if TESTING else 100_000\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "mlm_probabilities = [0.3, 0.2, 0.18, 0.16, 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d945f27-88b5-4105-b736-3697b81d3948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.676971Z",
     "iopub.status.busy": "2025-02-15T05:57:57.676737Z",
     "iopub.status.idle": "2025-02-15T05:57:57.685374Z",
     "shell.execute_reply": "2025-02-15T05:57:57.684933Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.676954Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(num_examples: int = NUM_EXAMPLES_TO_TRAIN) -> Dataset:\n",
    "    \"\"\"Load and prepare the dataset for training.\"\"\"\n",
    "    dataset = load_dataset(\"emdemor/news-of-the-brazilian-newspaper\", split=\"train\")\n",
    "    df = dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "    temp = df.sample(min(num_examples, len(df)))\n",
    "    texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "    texts = [phrase for text in texts if text for phrase in split_into_sentences(text)]\n",
    "    return Dataset.from_dict({\"text\": list(set(texts))[:num_examples]})\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "def set_attention(model):\n",
    "\n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            print(\"Flash Attention não é compatível:\", str(e))\n",
    "            return False\n",
    "\n",
    "    if FLASH_ATTENTION and check_flash_attention_support():\n",
    "        print(\"Replacing standard attention with FlashAttention...\")\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.MultiheadAttention):\n",
    "                module.attention = FlashAttention()\n",
    "        print(\"FlashAttention integrated.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def check_vocab_size(tokenizer, model):\n",
    "\n",
    "    # Verifique o maior ID no tokenizador\n",
    "    max_token_id = max(tokenizer.get_vocab().values())\n",
    "    print(\"Maior ID no tokenizador:\", max_token_id)\n",
    "    \n",
    "    # Verifique o tamanho do vocabulário do modelo\n",
    "    print(\"Tamanho do vocabulário do modelo:\", model.config.vocab_size)\n",
    "    \n",
    "    # Se o maior ID for maior ou igual ao tamanho do vocabulário, há um problema\n",
    "    assert max_token_id < model.config.vocab_size, \"IDs de tokens fora do intervalo!\"\n",
    "\n",
    "def setup_model_and_tokenizer(\n",
    "    model_checkpoint: str, tokenizer_path: str, device: torch.device\n",
    "):\n",
    "    \"\"\"Setup model and tokenizer.\"\"\"\n",
    "    print(f\"Loading custom tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(f\"Loading model config from {model_checkpoint}...\")\n",
    "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    config.torch_dtype = torch.float16\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, config=config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model = set_attention(model)\n",
    "    check_vocab_size(tokenizer, model)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # No truncation and max_length to allow dynamic padding truncation=True, max_length=chunk_size, padding=\"longest\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba804f0-3a39-47e8-9294-a8cacc6acce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.872678Z",
     "iopub.status.busy": "2025-02-15T05:57:57.872442Z",
     "iopub.status.idle": "2025-02-15T05:57:57.878626Z",
     "shell.execute_reply": "2025-02-15T05:57:57.877783Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.872661Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelInfo:\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "\n",
    "\n",
    "def get_model_info():\n",
    "    model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "    model_info = ModelInfo(\n",
    "        model_name=model_name,\n",
    "        output_dir=f\"{model_name}-ptbr-{'test' if TESTING else 'full'}\",\n",
    "    )\n",
    "    if os.path.exists(model_info.output_dir):\n",
    "        shutil.rmtree(model_info.output_dir)\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8366aa68-2581-4162-8e4f-6958dcc86819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.038686Z",
     "iopub.status.busy": "2025-02-15T05:57:58.038389Z",
     "iopub.status.idle": "2025-02-15T05:57:58.048236Z",
     "shell.execute_reply": "2025-02-15T05:57:58.047403Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.038650Z"
    }
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    num_train_epochs: int\n",
    "    chunk_size: int | None\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "    estimated_dataset_size_in_rows: int\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        return self.per_device_train_batch_size * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        return math.ceil(\n",
    "            self.estimated_dataset_size_in_rows / self.effective_batch_size\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        return int(self.estimated_dataset_size_in_rows * self.eval_size_ratio)\n",
    "\n",
    "    def __repr__(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"per_device_train_batch_size\", self.per_device_train_batch_size],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"estimated_dataset_size_in_rows\", self.estimated_dataset_size_in_rows],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337e28b1-45aa-4a12-9396-9322485846a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.224776Z",
     "iopub.status.busy": "2025-02-15T05:57:58.224325Z",
     "iopub.status.idle": "2025-02-15T05:57:58.241475Z",
     "shell.execute_reply": "2025-02-15T05:57:58.239432Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.224742Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper Function to Fix Batch Inputs ---\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Ensures that input tensors have the correct shape and dtype.\n",
    "    - Removes any extra dimensions (e.g., [1, batch, seq_len] -> [batch, seq_len]).\n",
    "    - Casts input_ids to torch.long.\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# --- Forward Pass Function ---\n",
    "def forward_pass(model, inputs):\n",
    "    \"\"\"\n",
    "    Performs a forward pass with autocast for FP16.\n",
    "    Returns the loss.\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, eval_dataset, data_collator, batch_size):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the evaluation dataset.\n",
    "    Returns the average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=batch_size)\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
    "        ):\n",
    "            inputs = data_collator(batch)\n",
    "            try:\n",
    "                loss = forward_pass(model, inputs)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs for language modeling.\n",
    "    This ensures that all sequences within a batch have the same length,\n",
    "    but the overall length can vary between batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(\n",
    "            batch\n",
    "        )  # Use torch_call instead of __call__ to call the parent's method\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf97b0-80fe-4dc2-bc72-f3ab2983fbac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:45:29.726852Z",
     "iopub.status.busy": "2025-02-15T05:45:29.726649Z",
     "iopub.status.idle": "2025-02-15T05:45:34.121662Z",
     "shell.execute_reply": "2025-02-15T05:45:34.121236Z",
     "shell.execute_reply.started": "2025-02-15T05:45:29.726838Z"
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e423d4c-8c6b-4000-b09c-2fd08067b6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.553317Z",
     "iopub.status.busy": "2025-02-15T05:57:58.553061Z",
     "iopub.status.idle": "2025-02-15T05:58:05.106770Z",
     "shell.execute_reply": "2025-02-15T05:58:05.106318Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.553300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom tokenizer from domain_tokenizer...\n",
      "Loading model config from neuralmind/bert-base-portuguese-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maior ID no tokenizador: 32767\n",
      "Tamanho do vocabulário do modelo: 32768\n"
     ]
    }
   ],
   "source": [
    "dataset = load_and_prepare_data()\n",
    "\n",
    "model, tokenizer = setup_model_and_tokenizer(\n",
    "    model_checkpoint=MODEL_CHECKPOINT,\n",
    "    tokenizer_path=TOKENIZER_PATH,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de1f2eb-6d7a-4fdd-b4fa-ee20da7d16a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:58:05.107811Z",
     "iopub.status.busy": "2025-02-15T05:58:05.107537Z",
     "iopub.status.idle": "2025-02-15T05:58:05.112252Z",
     "shell.execute_reply": "2025-02-15T05:58:05.111694Z",
     "shell.execute_reply.started": "2025-02-15T05:58:05.107796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------+\n",
      "| Attribute                      |   Value |\n",
      "+================================+=========+\n",
      "| num_train_epochs               |    1    |\n",
      "+--------------------------------+---------+\n",
      "| chunk_size                     |         |\n",
      "+--------------------------------+---------+\n",
      "| per_device_train_batch_size    |    4    |\n",
      "+--------------------------------+---------+\n",
      "| gradient_accumulation_steps    |    2    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_ratio                |    0.05 |\n",
      "+--------------------------------+---------+\n",
      "| total_save_limit               |    2    |\n",
      "+--------------------------------+---------+\n",
      "| estimated_dataset_size_in_rows | 3000    |\n",
      "+--------------------------------+---------+\n",
      "| effective_batch_size           |    8    |\n",
      "+--------------------------------+---------+\n",
      "| total_steps_per_epoch          |  375    |\n",
      "+--------------------------------+---------+\n",
      "| total_train_steps              |  375    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_per_chunk            |  150    |\n",
      "+--------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    num_train_epochs=1,\n",
    "    chunk_size=None,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_size_ratio=0.05,\n",
    "    total_save_limit=2,\n",
    "    estimated_dataset_size_in_rows=len(dataset),\n",
    ")\n",
    "\n",
    "print(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febfdf19-4741-4a99-9a5a-a89bcb9cf7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:58:05.112969Z",
     "iopub.status.busy": "2025-02-15T05:58:05.112805Z",
     "iopub.status.idle": "2025-02-15T06:02:48.092303Z",
     "shell.execute_reply": "2025-02-15T06:02:48.091561Z",
     "shell.execute_reply.started": "2025-02-15T05:58:05.112954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40762aff4494cf6ad7807f8eb87a582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08292328fca245ad8f0fea0a51b2fffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.3): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21566c3188c94d0f9a0012f4e06195a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde6bc6523264d6d8f6d50f18908b028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 93: 7.894379716170461\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724e5984619347c18ea09c73a18214e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.18): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83d782d7e9549c19d6255c6620e0241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.16): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d90d98764b94057a72b59b583d9c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 186: 7.574212124473171\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72026b263ea04533b7fac794ff76c986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.14): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eb5baf698c4c56ac268fc18856b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 279: 8.023501885564704\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# --- Optimizer and Scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=training_config.total_train_steps\n",
    ")\n",
    "\n",
    "# --- AMP scaler for mixed precision ---\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenize_dataset(dataset)\n",
    "\n",
    "num_chunks = len(mlm_probabilities)\n",
    "available_size = len(tokenized_dataset) - training_config.eval_size_per_chunk * num_chunks\n",
    "if available_size < num_chunks:\n",
    "    num_chunks = max(1, available_size)\n",
    "    mlm_probabilities = mlm_probabilities[:num_chunks]\n",
    "\n",
    "chunk_size_dataset = available_size // num_chunks\n",
    "\n",
    "\n",
    "for epoch in range(training_config.num_train_epochs):\n",
    "    for i, mlm_probability in enumerate(mlm_probabilities):\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{training_config.num_train_epochs}, MLM Probability: {mlm_probability}\"\n",
    "        )\n",
    "\n",
    "        data_collator = DynamicPaddingDataCollator(\n",
    "            tokenizer=tokenizer, mlm_probability=mlm_probability\n",
    "        )\n",
    "\n",
    "        train_dataset = (\n",
    "            tokenized_dataset.skip(i * chunk_size_dataset + training_config.eval_size_per_chunk)\n",
    "            .take(chunk_size_dataset)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "        eval_dataset = tokenized_dataset.skip(i * chunk_size_dataset).take(\n",
    "            training_config.eval_size_per_chunk\n",
    "        )\n",
    "\n",
    "        train_iterator = train_dataset.iter(batch_size=training_config.per_device_train_batch_size)\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(train_iterator, desc=f\"Training (MLM {mlm_probability})\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss / training_config.gradient_accumulation_steps).backward()\n",
    "\n",
    "            if (step + 1) % training_config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()  # Clear cache\n",
    "                global_step += 1\n",
    "\n",
    "                # Evaluation\n",
    "                eval_interval = training_config.total_steps_per_epoch // (training_config.num_train_epochs * 4)\n",
    "                if eval_interval > 0 and (global_step % eval_interval == 0):\n",
    "                    eval_loss = evaluate(model, eval_dataset, data_collator, batch_size=training_config.per_device_train_batch_size)\n",
    "                    print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "\n",
    "                # Push to hub incl TESTING\n",
    "                if global_step % PUSH_INTERVAL == 0:\n",
    "                    print(f\"Saving and pushing model at step {global_step}...\")\n",
    "                    model.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    print(f\"Model saved and pushed at step {global_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9dcb84-2b34-4352-9ac8-c05f4726bbaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T06:02:48.093886Z",
     "iopub.status.busy": "2025-02-15T06:02:48.093630Z",
     "iopub.status.idle": "2025-02-15T06:02:48.721267Z",
     "shell.execute_reply": "2025-02-15T06:02:48.720710Z",
     "shell.execute_reply.started": "2025-02-15T06:02:48.093864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving and pushing final model...\n",
      "Final model saved and pushed.\n"
     ]
    }
   ],
   "source": [
    "model_info = get_model_info()\n",
    "\n",
    "\n",
    "print(\"\\nSaving and pushing final model...\")\n",
    "model.save_pretrained(model_info.output_dir)\n",
    "tokenizer.save_pretrained(model_info.output_dir)\n",
    "print(\"Final model saved and pushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ff6fd-904f-4b45-b34f-7808a443a75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db2bae9-daf2-4ba5-940a-f2aba2a62045",
   "metadata": {},
   "source": [
    "## Tempo\n",
    "\n",
    "| # Rows| Duração |\n",
    "|-------|---------|\n",
    "| 1000  | 1m 33.89s |\n",
    "| 2000  | 3m 5.12s  |\n",
    "| 3000  | 4m 42.98s  |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
