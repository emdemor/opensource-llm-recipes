{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b2926-dff2-42d6-9fff-3734b967b53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:15:42.787454Z",
     "iopub.status.busy": "2025-02-15T05:15:42.787137Z",
     "iopub.status.idle": "2025-02-15T05:15:42.789969Z",
     "shell.execute_reply": "2025-02-15T05:15:42.789362Z",
     "shell.execute_reply.started": "2025-02-15T05:15:42.787433Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers -y\n",
    "# !pip install transformers==4.48.3 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7b1f87-a155-469c-be22-ccd0cacb7acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:54.484100Z",
     "iopub.status.busy": "2025-02-15T05:57:54.483736Z",
     "iopub.status.idle": "2025-02-15T05:57:57.536357Z",
     "shell.execute_reply": "2025-02-15T05:57:57.535709Z",
     "shell.execute_reply.started": "2025-02-15T05:57:54.484072Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "    os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import flash_attn\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "from huggingface_hub import Repository, whoami\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247c792a-5943-436c-97a5-9166712ee823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.538515Z",
     "iopub.status.busy": "2025-02-15T05:57:57.538379Z",
     "iopub.status.idle": "2025-02-15T05:57:57.579583Z",
     "shell.execute_reply": "2025-02-15T05:57:57.578799Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.538502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EXAMPLES_TO_TRAIN = 3000\n",
    "MODEL_CHECKPOINT = \"neuralmind/bert-base-portuguese-cased\"\n",
    "USERNAME = \"emdemor\"\n",
    "TOKENIZER_PATH = \"domain_tokenizer\"\n",
    "TESTING = True\n",
    "FLASH_ATTENTION = False\n",
    "PUSH_INTERVAL = 10_000 if TESTING else 100_000\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# A probabilidade de MLM determina quantos tokens serão mascarados durante o treinamento\n",
    "# Usar diferentes probabilidades pode ajudar o modelo a aprender melhor\n",
    "mlm_probabilities = [0.3, 0.2, 0.18, 0.16, 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d945f27-88b5-4105-b736-3697b81d3948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.676971Z",
     "iopub.status.busy": "2025-02-15T05:57:57.676737Z",
     "iopub.status.idle": "2025-02-15T05:57:57.685374Z",
     "shell.execute_reply": "2025-02-15T05:57:57.684933Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.676954Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(num_examples: int = NUM_EXAMPLES_TO_TRAIN) -> Dataset:\n",
    "    \"\"\"Load and prepare the dataset for training.\"\"\"\n",
    "    dataset = load_dataset(\"emdemor/news-of-the-brazilian-newspaper\", split=\"train\")\n",
    "    df = dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "    temp = df.sample(min(num_examples, len(df)))\n",
    "    texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "    texts = [phrase for text in texts if text for phrase in split_into_sentences(text)]\n",
    "    return Dataset.from_dict({\"text\": list(set(texts))[:num_examples]})\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "def set_attention(model):\n",
    "\n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            print(\"Flash Attention não é compatível:\", str(e))\n",
    "            return False\n",
    "\n",
    "    if FLASH_ATTENTION and check_flash_attention_support():\n",
    "        print(\"Replacing standard attention with FlashAttention...\")\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.MultiheadAttention):\n",
    "                module.attention = FlashAttention()\n",
    "        print(\"FlashAttention integrated.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def check_vocab_size(tokenizer, model):\n",
    "\n",
    "    # Verifique o maior ID no tokenizador\n",
    "    max_token_id = max(tokenizer.get_vocab().values())\n",
    "    print(\"Maior ID no tokenizador:\", max_token_id)\n",
    "    \n",
    "    # Verifique o tamanho do vocabulário do modelo\n",
    "    print(\"Tamanho do vocabulário do modelo:\", model.config.vocab_size)\n",
    "    \n",
    "    # Se o maior ID for maior ou igual ao tamanho do vocabulário, há um problema\n",
    "    assert max_token_id < model.config.vocab_size, \"IDs de tokens fora do intervalo!\"\n",
    "\n",
    "def setup_model_and_tokenizer(\n",
    "    model_checkpoint: str, tokenizer_path: str, device: torch.device\n",
    "):\n",
    "    \"\"\"Setup model and tokenizer.\"\"\"\n",
    "    print(f\"Loading custom tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(f\"Loading model config from {model_checkpoint}...\")\n",
    "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    config.torch_dtype = torch.float16\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, config=config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.to(device)\n",
    "    model = set_attention(model)\n",
    "    check_vocab_size(tokenizer, model)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # No truncation and max_length to allow dynamic padding truncation=True, max_length=chunk_size, padding=\"longest\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba804f0-3a39-47e8-9294-a8cacc6acce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:57.872678Z",
     "iopub.status.busy": "2025-02-15T05:57:57.872442Z",
     "iopub.status.idle": "2025-02-15T05:57:57.878626Z",
     "shell.execute_reply": "2025-02-15T05:57:57.877783Z",
     "shell.execute_reply.started": "2025-02-15T05:57:57.872661Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelInfo:\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "\n",
    "\n",
    "def get_model_info():\n",
    "    model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "    model_info = ModelInfo(\n",
    "        model_name=model_name,\n",
    "        output_dir=f\"{model_name}-ptbr-{'test' if TESTING else 'full'}\",\n",
    "    )\n",
    "    if os.path.exists(model_info.output_dir):\n",
    "        shutil.rmtree(model_info.output_dir)\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8366aa68-2581-4162-8e4f-6958dcc86819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.038686Z",
     "iopub.status.busy": "2025-02-15T05:57:58.038389Z",
     "iopub.status.idle": "2025-02-15T05:57:58.048236Z",
     "shell.execute_reply": "2025-02-15T05:57:58.047403Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.038650Z"
    }
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    num_train_epochs: int\n",
    "    chunk_size: int | None\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "    estimated_dataset_size_in_rows: int\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        return self.per_device_train_batch_size * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        return math.ceil(\n",
    "            self.estimated_dataset_size_in_rows / self.effective_batch_size\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        return int(self.estimated_dataset_size_in_rows * self.eval_size_ratio)\n",
    "\n",
    "    def __repr__(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"per_device_train_batch_size\", self.per_device_train_batch_size],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"estimated_dataset_size_in_rows\", self.estimated_dataset_size_in_rows],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be99a7-78ed-47e8-b7b2-63a27a3f1bc5",
   "metadata": {},
   "source": [
    "## Definindo o DataCollator\n",
    "\n",
    "### O que é um DataCollator no Contexto de Processamento de Linguagem Natural\n",
    "\n",
    "Um DataCollator é uma componente fundamental no pipeline de treinamento de modelos de linguagem natural, especialmente quando trabalhamos com bibliotecas como a Hugging Face Transformers. Vou explicar em detalhes o que é, para que serve e como funciona.\n",
    "\n",
    "#### Definição Básica\n",
    "\n",
    "Um DataCollator é uma função ou classe responsável por agrupar vários exemplos individuais em um único lote (batch) que pode ser processado eficientemente pelo modelo durante o treinamento. O nome \"collator\" vem do verbo \"collate\", que significa reunir e organizar informações.\n",
    "\n",
    "#### Por que Precisamos de DataCollators?\n",
    "\n",
    "Quando treinamos modelos de linguagem, geralmente trabalhamos com textos de comprimentos variados. No entanto, os modelos de deep learning, especialmente aqueles implementados em PyTorch ou TensorFlow, esperam tensores de forma consistente (retangular). Isso cria um desafio:\n",
    "\n",
    "1. Como transformar exemplos de tamanhos diferentes em uma estrutura uniforme?\n",
    "2. Como fazer isso de maneira eficiente sem desperdiçar recursos computacionais?\n",
    "\n",
    "É aqui que entra o DataCollator.\n",
    "\n",
    "#### Funções Principais de um DataCollator\n",
    "\n",
    "##### 1. Padding (Preenchimento)\n",
    "\n",
    "A função mais importante é o preenchimento (padding). Como as sequências têm comprimentos diferentes, o DataCollator adiciona tokens especiais (geralmente 0s) para que todas as sequências em um lote tenham o mesmo comprimento.\n",
    "\n",
    "Por exemplo, se tivermos estas três sequências:\n",
    "```\n",
    "[101, 2054, 2003, 102]  # \"What is\"\n",
    "[101, 2054, 2003, 2023, 2003, 102]  # \"What is this\"\n",
    "[101, 2054, 2003, 2023, 102]  # \"What is this\"\n",
    "```\n",
    "\n",
    "O DataCollator as transformaria em:\n",
    "```\n",
    "[101, 2054, 2003, 102, 0, 0]\n",
    "[101, 2054, 2003, 2023, 2003, 102]\n",
    "[101, 2054, 2003, 2023, 102, 0]\n",
    "```\n",
    "\n",
    "##### 2. Mascaramento para MLM (no caso de DataCollatorForLanguageModeling)\n",
    "\n",
    "No código que você compartilhou, está sendo usado o `DataCollatorForLanguageModeling`, que é especializado para treinar modelos de linguagem mascarada (como BERT). Além do padding, ele:\n",
    "\n",
    "- Seleciona aleatoriamente uma porcentagem dos tokens (definida por `mlm_probability`)\n",
    "- Substitui esses tokens por um token especial [MASK]\n",
    "- Mantém o registro dos tokens originais para calcular a perda durante o treinamento\n",
    "\n",
    "Por exemplo, com `mlm_probability=0.15`:\n",
    "```\n",
    "Original: [101, 2054, 2003, 2023, 2003, 102]\n",
    "Mascarado: [101, 2054, [MASK], 2023, 2003, 102]  # \"2003\" foi mascarado\n",
    "```\n",
    "\n",
    "##### 3. Otimização de Memória\n",
    "\n",
    "Um bom DataCollator também otimiza o uso de memória. No código que você compartilhou, o `DynamicPaddingDataCollator` é um excelente exemplo disso:\n",
    "\n",
    "- Em vez de preencher todas as sequências até um comprimento máximo fixo (por exemplo, 512 tokens)\n",
    "- Ele calcula o comprimento máximo dentro do lote atual\n",
    "- Preenche apenas até esse comprimento\n",
    "\n",
    "Isso economiza muita memória quando a maioria das sequências em um lote é curta.\n",
    "\n",
    "#### O Caso Específico do DynamicPaddingDataCollator\n",
    "\n",
    "O `DynamicPaddingDataCollator` no seu código é uma extensão personalizada do `DataCollatorForLanguageModeling` padrão da Hugging Face. Ele:\n",
    "\n",
    "1. Encontra o comprimento máximo dentro do lote atual\n",
    "2. Para cada exemplo:\n",
    "   - Calcula quanto preenchimento é necessário\n",
    "   - Adiciona tokens de padding aos IDs de entrada\n",
    "   - Adiciona zeros à máscara de atenção correspondente\n",
    "   - Ou trunca se necessário (se a sequência for mais longa que o permitido)\n",
    "3. Aplica a lógica de mascaramento do MLM\n",
    "4. Garante que as formas e tipos dos tensores estão corretos usando a função `fix_batch_inputs`\n",
    "\n",
    "#### Analogia para Entender Melhor\n",
    "\n",
    "Pense no DataCollator como o gerente de uma mesa em um restaurante:\n",
    "\n",
    "1. Várias pessoas (exemplos) chegam em grupos diferentes\n",
    "2. O gerente precisa organizá-las em mesas (lotes) de tamanho fixo\n",
    "3. Algumas mesas podem ter espaços vazios (padding)\n",
    "4. Para um jogo especial durante o jantar (MLM), o gerente venda os olhos de algumas pessoas aleatoriamente\n",
    "\n",
    "#### Por que o Dynamic Padding é Importante?\n",
    "\n",
    "Considere dois cenários com 100 sequências, onde a maioria tem 20 tokens, mas algumas poucas têm 500:\n",
    "\n",
    "1. **Padding Fixo**: Todas as 100 sequências são preenchidas até 500 tokens = 50.000 tokens totais\n",
    "2. **Padding Dinâmico**: \n",
    "   - Lote 1: 16 sequências, max_len=20 → 320 tokens\n",
    "   - Lote 2: 16 sequências, max_len=25 → 400 tokens\n",
    "   - ...\n",
    "   - Último lote: 4 sequências, max_len=500 → 2.000 tokens\n",
    "   - Total muito menor que 50.000!\n",
    "\n",
    "Essa economia permite:\n",
    "- Treinar com lotes maiores\n",
    "- Usar menos memória GPU\n",
    "- Treinar modelos maiores\n",
    "- Acelerar o treinamento\n",
    "\n",
    "#### Conclusão\n",
    "\n",
    "O DataCollator é uma peça crucial que prepara os dados brutos para o consumo eficiente pelo modelo. No caso do MLM, ele também implementa a estratégia de mascaramento que é central para o aprendizado. A versão com padding dinâmico que você está usando representa uma otimização inteligente que pode melhorar significativamente a eficiência do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337e28b1-45aa-4a12-9396-9322485846a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.224776Z",
     "iopub.status.busy": "2025-02-15T05:57:58.224325Z",
     "iopub.status.idle": "2025-02-15T05:57:58.241475Z",
     "shell.execute_reply": "2025-02-15T05:57:58.239432Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.224742Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper Function to Fix Batch Inputs ---\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Esta função tem como objetivo garantir que os tensores de entrada tenham a forma e o tipo corretos:\n",
    "    \n",
    "    - Ela verifica três chaves importantes: \"input_ids\", \"attention_mask\" e \"token_type_ids\"\n",
    "    - Remove dimensões extras (por exemplo, converte [1, batch, seq_len] para [batch, seq_len])\n",
    "    - Converte \"input_ids\" para o tipo torch.long, que é o tipo esperado para IDs de tokens\n",
    "    - Isso é importante porque inconsistências na forma dos tensores podem causar erros durante o treinamento\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# --- Forward Pass Function ---\n",
    "def forward_pass(model, inputs):\n",
    "    \"\"\"\n",
    "    Esta função realiza uma passagem para frente (forward pass) no modelo:\n",
    "    \n",
    "    - Primeiro, aplica a função fix_batch_inputs para garantir que as entradas estão corretas\n",
    "    - Move os tensores para o dispositivo apropriado (CPU ou GPU)\n",
    "    - Usa torch.amp.autocast para habilitar precisão mista automática quando estiver usando GPU\n",
    "      - A precisão mista acelera o treinamento e reduz o uso de memória\n",
    "    - Executa o modelo com as entradas e solicita que retorne um dicionário completo\n",
    "    - Verifica se o modelo retornou uma perda (loss) e a retorna\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, eval_dataset, data_collator, batch_size):\n",
    "    \"\"\"\n",
    "    Esta função avalia o desempenho do modelo no conjunto de dados de avaliação:\n",
    "    \n",
    "    - Coloca o modelo em modo de avaliação (model.eval())\n",
    "    - Itera sobre o conjunto de dados de avaliação em lotes\n",
    "    - Para cada lote:\n",
    "      - Desativa o cálculo de gradientes com torch.no_grad()\n",
    "      - Usa precisão mista se estiver em GPU\n",
    "      - Calcula a perda e a adiciona à lista de perdas\n",
    "      - Captura e imprime erros, mas continua a avaliação\n",
    "    - Retorna ao modo de treinamento (model.train())\n",
    "    - Calcula e retorna a perda média\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=batch_size)\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
    "        ):\n",
    "            inputs = data_collator(batch)\n",
    "            try:\n",
    "                loss = forward_pass(model, inputs)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Esta classe estende DataCollatorForLanguageModeling e implementa um colator de dados com preenchimento dinâmico:\n",
    "    \n",
    "    - O preenchimento dinâmico significa que cada lote é preenchido apenas até o comprimento da sequência mais longa naquele lote específico\n",
    "    - Isso é mais eficiente que usar um comprimento fixo para todos os lotes\n",
    "    - Para cada exemplo no lote:\n",
    "      - Calcula quanto padding é necessário\n",
    "      - Adiciona tokens de padding aos IDs de entrada e zeros às máscaras de atenção\n",
    "      - Ou trunca se necessário\n",
    "    - Aplica a lógica de colação de dados do MLM (mascaramento aleatório de tokens)\n",
    "    - Garante formas e tipos corretos com fix_batch_inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(\n",
    "            batch\n",
    "        )  # Use torch_call instead of __call__ to call the parent's method\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf97b0-80fe-4dc2-bc72-f3ab2983fbac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:45:29.726852Z",
     "iopub.status.busy": "2025-02-15T05:45:29.726649Z",
     "iopub.status.idle": "2025-02-15T05:45:34.121662Z",
     "shell.execute_reply": "2025-02-15T05:45:34.121236Z",
     "shell.execute_reply.started": "2025-02-15T05:45:29.726838Z"
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e423d4c-8c6b-4000-b09c-2fd08067b6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:57:58.553317Z",
     "iopub.status.busy": "2025-02-15T05:57:58.553061Z",
     "iopub.status.idle": "2025-02-15T05:58:05.106770Z",
     "shell.execute_reply": "2025-02-15T05:58:05.106318Z",
     "shell.execute_reply.started": "2025-02-15T05:57:58.553300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom tokenizer from domain_tokenizer...\n",
      "Loading model config from neuralmind/bert-base-portuguese-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maior ID no tokenizador: 32767\n",
      "Tamanho do vocabulário do modelo: 32768\n"
     ]
    }
   ],
   "source": [
    "dataset = load_and_prepare_data()\n",
    "\n",
    "model, tokenizer = setup_model_and_tokenizer(\n",
    "    model_checkpoint=MODEL_CHECKPOINT,\n",
    "    tokenizer_path=TOKENIZER_PATH,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de1f2eb-6d7a-4fdd-b4fa-ee20da7d16a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:58:05.107811Z",
     "iopub.status.busy": "2025-02-15T05:58:05.107537Z",
     "iopub.status.idle": "2025-02-15T05:58:05.112252Z",
     "shell.execute_reply": "2025-02-15T05:58:05.111694Z",
     "shell.execute_reply.started": "2025-02-15T05:58:05.107796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------+\n",
      "| Attribute                      |   Value |\n",
      "+================================+=========+\n",
      "| num_train_epochs               |    1    |\n",
      "+--------------------------------+---------+\n",
      "| chunk_size                     |         |\n",
      "+--------------------------------+---------+\n",
      "| per_device_train_batch_size    |    4    |\n",
      "+--------------------------------+---------+\n",
      "| gradient_accumulation_steps    |    2    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_ratio                |    0.05 |\n",
      "+--------------------------------+---------+\n",
      "| total_save_limit               |    2    |\n",
      "+--------------------------------+---------+\n",
      "| estimated_dataset_size_in_rows | 3000    |\n",
      "+--------------------------------+---------+\n",
      "| effective_batch_size           |    8    |\n",
      "+--------------------------------+---------+\n",
      "| total_steps_per_epoch          |  375    |\n",
      "+--------------------------------+---------+\n",
      "| total_train_steps              |  375    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_per_chunk            |  150    |\n",
      "+--------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    num_train_epochs=1,\n",
    "    chunk_size=None,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_size_ratio=0.05,\n",
    "    total_save_limit=2,\n",
    "    estimated_dataset_size_in_rows=len(dataset),\n",
    ")\n",
    "\n",
    "print(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febfdf19-4741-4a99-9a5a-a89bcb9cf7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T05:58:05.112969Z",
     "iopub.status.busy": "2025-02-15T05:58:05.112805Z",
     "iopub.status.idle": "2025-02-15T06:02:48.092303Z",
     "shell.execute_reply": "2025-02-15T06:02:48.091561Z",
     "shell.execute_reply.started": "2025-02-15T05:58:05.112954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40762aff4494cf6ad7807f8eb87a582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08292328fca245ad8f0fea0a51b2fffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.3): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21566c3188c94d0f9a0012f4e06195a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde6bc6523264d6d8f6d50f18908b028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 93: 7.894379716170461\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724e5984619347c18ea09c73a18214e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.18): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83d782d7e9549c19d6255c6620e0241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.16): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d90d98764b94057a72b59b583d9c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 186: 7.574212124473171\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72026b263ea04533b7fac794ff76c986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.14): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eb5baf698c4c56ac268fc18856b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 279: 8.023501885564704\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# --- Optimizer and Scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=training_config.total_train_steps\n",
    ")\n",
    "\n",
    "# --- AMP scaler for mixed precision ---\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenize_dataset(dataset)\n",
    "\n",
    "num_chunks = len(mlm_probabilities)\n",
    "available_size = len(tokenized_dataset) - training_config.eval_size_per_chunk * num_chunks\n",
    "if available_size < num_chunks:\n",
    "    num_chunks = max(1, available_size)\n",
    "    mlm_probabilities = mlm_probabilities[:num_chunks]\n",
    "\n",
    "chunk_size_dataset = available_size // num_chunks\n",
    "\n",
    "\n",
    "for epoch in range(training_config.num_train_epochs):\n",
    "    for i, mlm_probability in enumerate(mlm_probabilities):\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{training_config.num_train_epochs}, MLM Probability: {mlm_probability}\"\n",
    "        )\n",
    "\n",
    "        data_collator = DynamicPaddingDataCollator(\n",
    "            tokenizer=tokenizer, mlm_probability=mlm_probability\n",
    "        )\n",
    "\n",
    "        train_dataset = (\n",
    "            tokenized_dataset.skip(i * chunk_size_dataset + training_config.eval_size_per_chunk)\n",
    "            .take(chunk_size_dataset)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "        eval_dataset = tokenized_dataset.skip(i * chunk_size_dataset).take(\n",
    "            training_config.eval_size_per_chunk\n",
    "        )\n",
    "\n",
    "        train_iterator = train_dataset.iter(batch_size=training_config.per_device_train_batch_size)\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(train_iterator, desc=f\"Training (MLM {mlm_probability})\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss / training_config.gradient_accumulation_steps).backward()\n",
    "\n",
    "            if (step + 1) % training_config.gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()  # Clear cache\n",
    "                global_step += 1\n",
    "\n",
    "                # Evaluation\n",
    "                eval_interval = training_config.total_steps_per_epoch // (training_config.num_train_epochs * 4)\n",
    "                if eval_interval > 0 and (global_step % eval_interval == 0):\n",
    "                    eval_loss = evaluate(model, eval_dataset, data_collator, batch_size=training_config.per_device_train_batch_size)\n",
    "                    print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "\n",
    "                # Push to hub incl TESTING\n",
    "                if global_step % PUSH_INTERVAL == 0:\n",
    "                    print(f\"Saving and pushing model at step {global_step}...\")\n",
    "                    model.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    print(f\"Model saved and pushed at step {global_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9dcb84-2b34-4352-9ac8-c05f4726bbaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T06:02:48.093886Z",
     "iopub.status.busy": "2025-02-15T06:02:48.093630Z",
     "iopub.status.idle": "2025-02-15T06:02:48.721267Z",
     "shell.execute_reply": "2025-02-15T06:02:48.720710Z",
     "shell.execute_reply.started": "2025-02-15T06:02:48.093864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving and pushing final model...\n",
      "Final model saved and pushed.\n"
     ]
    }
   ],
   "source": [
    "model_info = get_model_info()\n",
    "\n",
    "\n",
    "print(\"\\nSaving and pushing final model...\")\n",
    "model.save_pretrained(model_info.output_dir)\n",
    "tokenizer.save_pretrained(model_info.output_dir)\n",
    "print(\"Final model saved and pushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ff6fd-904f-4b45-b34f-7808a443a75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db2bae9-daf2-4ba5-940a-f2aba2a62045",
   "metadata": {},
   "source": [
    "## Tempo\n",
    "\n",
    "| # Rows| Duração |\n",
    "|-------|---------|\n",
    "| 1000  | 1m 33.89s |\n",
    "| 2000  | 3m 5.12s  |\n",
    "| 3000  | 4m 42.98s  |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
