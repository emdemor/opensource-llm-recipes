{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8b2926-dff2-42d6-9fff-3734b967b53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:41:37.815310Z",
     "iopub.status.busy": "2025-02-15T04:41:37.814988Z",
     "iopub.status.idle": "2025-02-15T04:41:44.209779Z",
     "shell.execute_reply": "2025-02-15T04:41:44.208617Z",
     "shell.execute_reply.started": "2025-02-15T04:41:37.815286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.44.2\n",
      "Uninstalling transformers-4.44.2:\n",
      "  Successfully uninstalled transformers-4.44.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers==4.48.3 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7b1f87-a155-469c-be22-ccd0cacb7acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:23.830995Z",
     "iopub.status.busy": "2025-02-15T04:44:23.830606Z",
     "iopub.status.idle": "2025-02-15T04:44:26.668165Z",
     "shell.execute_reply": "2025-02-15T04:44:26.667574Z",
     "shell.execute_reply.started": "2025-02-15T04:44:23.830975Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "    os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import flash_attn\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from flash_attn import flash_attn_qkvpacked_func\n",
    "from huggingface_hub import Repository, whoami\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247c792a-5943-436c-97a5-9166712ee823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.668854Z",
     "iopub.status.busy": "2025-02-15T04:44:26.668639Z",
     "iopub.status.idle": "2025-02-15T04:44:26.683951Z",
     "shell.execute_reply": "2025-02-15T04:44:26.683557Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.668841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_EXAMPLES_TO_TRAIN = 3000\n",
    "MODEL_CHECKPOINT = \"answerdotai/ModernBERT-base\"\n",
    "USERNAME = \"emdemor\"\n",
    "TOKENIZER_PATH = \"domain_tokenizer\"\n",
    "TESTING = True\n",
    "FLASH_ATTENTION = False\n",
    "PUSH_INTERVAL = 10_000 if TESTING else 100_000\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "mlm_probabilities = [0.3, 0.2, 0.18, 0.16, 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d945f27-88b5-4105-b736-3697b81d3948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.684715Z",
     "iopub.status.busy": "2025-02-15T04:44:26.684469Z",
     "iopub.status.idle": "2025-02-15T04:44:26.737214Z",
     "shell.execute_reply": "2025-02-15T04:44:26.736810Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.684703Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(num_examples: int = NUM_EXAMPLES_TO_TRAIN) -> Dataset:\n",
    "    \"\"\"Load and prepare the dataset for training.\"\"\"\n",
    "    dataset = load_dataset(\"emdemor/news-of-the-brazilian-newspaper\", split=\"train\")\n",
    "    df = dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "    temp = df.sample(min(num_examples, len(df)))\n",
    "    texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "    texts = [phrase for text in texts if text for phrase in split_into_sentences(text)]\n",
    "    return Dataset.from_dict({\"text\": list(set(texts))[:num_examples]})\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "def set_attention(model):\n",
    "\n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            print(\"Flash Attention não é compatível:\", str(e))\n",
    "            return False\n",
    "\n",
    "    if FLASH_ATTENTION and check_flash_attention_support():\n",
    "        print(\"Replacing standard attention with FlashAttention...\")\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.MultiheadAttention):\n",
    "                module.attention = FlashAttention()\n",
    "        print(\"FlashAttention integrated.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def setup_model_and_tokenizer(\n",
    "    model_checkpoint: str, tokenizer_path: str, device: torch.device\n",
    "):\n",
    "    \"\"\"Setup model and tokenizer.\"\"\"\n",
    "    print(f\"Loading custom tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(f\"Loading model config from {model_checkpoint}...\")\n",
    "    config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    config.torch_dtype = torch.float16\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_checkpoint, config=config).to(\n",
    "        device\n",
    "    )\n",
    "    model = set_attention(model)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # No truncation and max_length to allow dynamic padding truncation=True, max_length=chunk_size, padding=\"longest\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba804f0-3a39-47e8-9294-a8cacc6acce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.737846Z",
     "iopub.status.busy": "2025-02-15T04:44:26.737676Z",
     "iopub.status.idle": "2025-02-15T04:44:26.741469Z",
     "shell.execute_reply": "2025-02-15T04:44:26.740640Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.737836Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelInfo:\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "\n",
    "\n",
    "def get_model_info():\n",
    "    model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
    "    model_info = ModelInfo(\n",
    "        model_name=model_name,\n",
    "        output_dir=f\"{model_name}-ptbr-{'test' if TESTING else 'full'}\",\n",
    "    )\n",
    "    if os.path.exists(model_info.output_dir):\n",
    "        shutil.rmtree(model_info.output_dir)\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8366aa68-2581-4162-8e4f-6958dcc86819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.743161Z",
     "iopub.status.busy": "2025-02-15T04:44:26.742772Z",
     "iopub.status.idle": "2025-02-15T04:44:26.750944Z",
     "shell.execute_reply": "2025-02-15T04:44:26.750063Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.743132Z"
    }
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    num_train_epochs: int\n",
    "    chunk_size: int | None\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "    estimated_dataset_size_in_rows: int\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        return self.per_device_train_batch_size * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        return math.ceil(\n",
    "            self.estimated_dataset_size_in_rows / self.effective_batch_size\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        return int(self.estimated_dataset_size_in_rows * self.eval_size_ratio)\n",
    "\n",
    "    def __repr__(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"per_device_train_batch_size\", self.per_device_train_batch_size],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"estimated_dataset_size_in_rows\", self.estimated_dataset_size_in_rows],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337e28b1-45aa-4a12-9396-9322485846a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.753110Z",
     "iopub.status.busy": "2025-02-15T04:44:26.752842Z",
     "iopub.status.idle": "2025-02-15T04:44:26.762441Z",
     "shell.execute_reply": "2025-02-15T04:44:26.761283Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.753091Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper Function to Fix Batch Inputs ---\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Ensures that input tensors have the correct shape and dtype.\n",
    "    - Removes any extra dimensions (e.g., [1, batch, seq_len] -> [batch, seq_len]).\n",
    "    - Casts input_ids to torch.long.\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# --- Forward Pass Function ---\n",
    "def forward_pass(model, inputs):\n",
    "    \"\"\"\n",
    "    Performs a forward pass with autocast for FP16.\n",
    "    Returns the loss.\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, eval_dataset, data_collator):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the evaluation dataset.\n",
    "    Returns the average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=per_device_train_batch_size)\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(DEVICE.type == \"cuda\")\n",
    "        ):\n",
    "            inputs = data_collator(batch)\n",
    "            try:\n",
    "                loss = forward_pass(model, inputs)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs for language modeling.\n",
    "    This ensures that all sequences within a batch have the same length,\n",
    "    but the overall length can vary between batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(\n",
    "            batch\n",
    "        )  # Use torch_call instead of __call__ to call the parent's method\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b8b8e6-e21a-4255-b008-61ae2bf5b423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:26.763924Z",
     "iopub.status.busy": "2025-02-15T04:44:26.763344Z",
     "iopub.status.idle": "2025-02-15T04:44:31.339306Z",
     "shell.execute_reply": "2025-02-15T04:44:31.338780Z",
     "shell.execute_reply.started": "2025-02-15T04:44:26.763879Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a06866-4d5b-4977-8ad8-78ff45652b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:31.340069Z",
     "iopub.status.busy": "2025-02-15T04:44:31.339933Z",
     "iopub.status.idle": "2025-02-15T04:44:31.342651Z",
     "shell.execute_reply": "2025-02-15T04:44:31.342298Z",
     "shell.execute_reply.started": "2025-02-15T04:44:31.340056Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e423d4c-8c6b-4000-b09c-2fd08067b6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:31.343378Z",
     "iopub.status.busy": "2025-02-15T04:44:31.343222Z",
     "iopub.status.idle": "2025-02-15T04:44:33.091709Z",
     "shell.execute_reply": "2025-02-15T04:44:33.091232Z",
     "shell.execute_reply.started": "2025-02-15T04:44:31.343365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom tokenizer from domain_tokenizer...\n",
      "Loading model config from answerdotai/ModernBERT-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = setup_model_and_tokenizer(\n",
    "    model_checkpoint=MODEL_CHECKPOINT,\n",
    "    tokenizer_path=TOKENIZER_PATH,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b876c90-2b4d-4127-92a3-db9e0ec9b947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:33.095527Z",
     "iopub.status.busy": "2025-02-15T04:44:33.094901Z",
     "iopub.status.idle": "2025-02-15T04:44:33.216223Z",
     "shell.execute_reply": "2025-02-15T04:44:33.215855Z",
     "shell.execute_reply.started": "2025-02-15T04:44:33.095492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bcd06a331247baa567e413155eda8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34521123-b728-4364-9e4d-e6c2dae69487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:33.216807Z",
     "iopub.status.busy": "2025-02-15T04:44:33.216674Z",
     "iopub.status.idle": "2025-02-15T04:44:33.219315Z",
     "shell.execute_reply": "2025-02-15T04:44:33.218705Z",
     "shell.execute_reply.started": "2025-02-15T04:44:33.216792Z"
    }
   },
   "outputs": [],
   "source": [
    "model_info = get_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49246f60-4dcd-4248-8727-4423562f1683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:33.220212Z",
     "iopub.status.busy": "2025-02-15T04:44:33.219961Z",
     "iopub.status.idle": "2025-02-15T04:44:33.223877Z",
     "shell.execute_reply": "2025-02-15T04:44:33.223161Z",
     "shell.execute_reply.started": "2025-02-15T04:44:33.220190Z"
    }
   },
   "outputs": [],
   "source": [
    "mlm_probabilities = [0.3, 0.2, 0.18, 0.16, 0.14]\n",
    "\n",
    "chunk_size_dataset = len(dataset) // len(mlm_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de1f2eb-6d7a-4fdd-b4fa-ee20da7d16a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:44:33.225226Z",
     "iopub.status.busy": "2025-02-15T04:44:33.224813Z",
     "iopub.status.idle": "2025-02-15T04:44:33.234807Z",
     "shell.execute_reply": "2025-02-15T04:44:33.233204Z",
     "shell.execute_reply.started": "2025-02-15T04:44:33.225197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------+\n",
      "| Attribute                      |   Value |\n",
      "+================================+=========+\n",
      "| num_train_epochs               |    1    |\n",
      "+--------------------------------+---------+\n",
      "| chunk_size                     |         |\n",
      "+--------------------------------+---------+\n",
      "| per_device_train_batch_size    |    4    |\n",
      "+--------------------------------+---------+\n",
      "| gradient_accumulation_steps    |    2    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_ratio                |    0.05 |\n",
      "+--------------------------------+---------+\n",
      "| total_save_limit               |    2    |\n",
      "+--------------------------------+---------+\n",
      "| estimated_dataset_size_in_rows | 3000    |\n",
      "+--------------------------------+---------+\n",
      "| effective_batch_size           |    8    |\n",
      "+--------------------------------+---------+\n",
      "| total_steps_per_epoch          |  375    |\n",
      "+--------------------------------+---------+\n",
      "| total_train_steps              |  375    |\n",
      "+--------------------------------+---------+\n",
      "| eval_size_per_chunk            |  150    |\n",
      "+--------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "training_config = TrainingConfig(\n",
    "    num_train_epochs=1,\n",
    "    chunk_size=None,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_size_ratio=0.05,\n",
    "    total_save_limit=2,\n",
    "    estimated_dataset_size_in_rows=len(dataset),\n",
    ")\n",
    "\n",
    "print(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "febfdf19-4741-4a99-9a5a-a89bcb9cf7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T04:45:25.408561Z",
     "iopub.status.busy": "2025-02-15T04:45:25.408325Z",
     "iopub.status.idle": "2025-02-15T04:45:29.167037Z",
     "shell.execute_reply": "2025-02-15T04:45:29.166224Z",
     "shell.execute_reply.started": "2025-02-15T04:45:25.408544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2199778d51a544bd94e310a6b0e381d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.3): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b503e17eea74a9ab760f148cfac1b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238d280938dd4a0dafb07e3c5c5325b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.18): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dea39772f447deb89fd6391bebbf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.16): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "Training batch failed: FlashAttention only supports Ampere GPUs or newer.. Skipping.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.14\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Index 599 out of range for dataset of size 450.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_config\u001b[38;5;241m.\u001b[39mnum_train_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MLM Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlm_probability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DynamicPaddingDataCollator(\n\u001b[1;32m     22\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm_probability\u001b[38;5;241m=\u001b[39mmlm_probability\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     26\u001b[0m     \u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk_size_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_size_per_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset\u001b[38;5;241m.\u001b[39mskip(i \u001b[38;5;241m*\u001b[39m chunk_size_dataset)\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m     31\u001b[0m     training_config\u001b[38;5;241m.\u001b[39meval_size_per_chunk\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m train_iterator \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39miter(batch_size\u001b[38;5;241m=\u001b[39mtraining_config\u001b[38;5;241m.\u001b[39mper_device_train_batch_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:4183\u001b[0m, in \u001b[0;36mDataset.take\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   4162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;124;03m    Create a new [`Dataset`] with only the first `n` elements.\u001b[39;00m\n\u001b[1;32m   4165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4181\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3949\u001b[0m, in \u001b[0;36mDataset.select\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_range_contiguous(indices) \u001b[38;5;129;01mand\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3948\u001b[0m         start, length \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m-\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m-> 3949\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_contiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3951\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:482\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:4010\u001b[0m, in \u001b[0;36mDataset._select_contiguous\u001b[0;34m(self, start, length, new_fingerprint)\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   4009\u001b[0m _check_valid_indices_value(start, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m-> 4010\u001b[0m \u001b[43m_check_valid_indices_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[1;32m   4013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mslice(start, length),\n\u001b[1;32m   4014\u001b[0m         info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m   4015\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit,\n\u001b[1;32m   4016\u001b[0m         fingerprint\u001b[38;5;241m=\u001b[39mnew_fingerprint,\n\u001b[1;32m   4017\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:659\u001b[0m, in \u001b[0;36m_check_valid_indices_value\u001b[0;34m(index, size)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_indices_value\u001b[39m(index, size):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Index 599 out of range for dataset of size 450."
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# --- Optimizer and Scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=training_config.total_train_steps\n",
    ")\n",
    "\n",
    "# --- AMP scaler for mixed precision ---\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(training_config.num_train_epochs):\n",
    "    for i, mlm_probability in enumerate(mlm_probabilities):\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{training_config.num_train_epochs}, MLM Probability: {mlm_probability}\"\n",
    "        )\n",
    "\n",
    "        data_collator = DynamicPaddingDataCollator(\n",
    "            tokenizer=tokenizer, mlm_probability=mlm_probability\n",
    "        )\n",
    "\n",
    "        train_dataset = (\n",
    "            tokenized_dataset.skip(i * chunk_size_dataset + training_config.eval_size_per_chunk)\n",
    "            .take(chunk_size_dataset)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "        eval_dataset = tokenized_dataset.skip(i * chunk_size_dataset).take(\n",
    "            training_config.eval_size_per_chunk\n",
    "        )\n",
    "\n",
    "        train_iterator = train_dataset.iter(batch_size=training_config.per_device_train_batch_size)\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(train_iterator, desc=f\"Training (MLM {mlm_probability})\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss / gradient_accumulation_steps).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()  # Clear cache\n",
    "                global_step += 1\n",
    "\n",
    "                # Evaluation\n",
    "                eval_interval = total_steps_per_epoch // (num_train_epochs * 4)\n",
    "                if eval_interval > 0 and (global_step % eval_interval == 0):\n",
    "                    eval_loss = evaluate(model, eval_dataset, data_collator)\n",
    "                    print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "\n",
    "                # Push to hub incl TESTING\n",
    "                if global_step % push_interval == 0:\n",
    "                    print(f\"Saving and pushing model at step {global_step}...\")\n",
    "                    model.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    print(f\"Model saved and pushed at step {global_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9dcb84-2b34-4352-9ac8-c05f4726bbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb70bbb-06e0-41f8-aa2d-4523251f10fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793000b-9d20-44ca-9aa1-c345aed262c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
