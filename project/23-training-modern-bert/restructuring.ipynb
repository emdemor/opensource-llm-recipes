{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6550706-4255-4f01-8c45-1ddb4779d0f6",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651ba40f-2423-413d-ac7e-2145b51f1a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:38.041172Z",
     "iopub.status.busy": "2025-02-19T03:37:38.040949Z",
     "iopub.status.idle": "2025-02-19T03:37:40.659092Z",
     "shell.execute_reply": "2025-02-19T03:37:40.658549Z",
     "shell.execute_reply.started": "2025-02-19T03:37:38.041149Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from pydantic import BaseModel, field_validator\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dca242-c4a8-4692-a7ae-9111fb4e6f20",
   "metadata": {},
   "source": [
    "# 2. Constantes e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7802d5e0-de1f-413b-bcdc-c0bd96414490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:41.624514Z",
     "iopub.status.busy": "2025-02-19T03:37:41.624314Z",
     "iopub.status.idle": "2025-02-19T03:37:41.628224Z",
     "shell.execute_reply": "2025-02-19T03:37:41.627535Z",
     "shell.execute_reply.started": "2025-02-19T03:37:41.624499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"training.log\"), logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ce248b-3d81-48f0-8920-a0d844c691bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:42.465455Z",
     "iopub.status.busy": "2025-02-19T03:37:42.465233Z",
     "iopub.status.idle": "2025-02-19T03:37:42.469576Z",
     "shell.execute_reply": "2025-02-19T03:37:42.468880Z",
     "shell.execute_reply.started": "2025-02-19T03:37:42.465438Z"
    }
   },
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     \"\"\"Parse command line arguments.\"\"\"\n",
    "#     parser = argparse.ArgumentParser(description=\"Treinar modelo MLM em português\")\n",
    "\n",
    "#     # Argumentos principais\n",
    "#     parser.add_argument(\"--model_id\", default=\"answerdotai/ModernBERT-base\",\n",
    "#                         help=\"ID do modelo base no Hugging Face Hub\")\n",
    "#     parser.add_argument(\"--dataset_id\", default=\"emdemor/news-of-the-brazilian-newspaper\",\n",
    "#                         help=\"ID do dataset no Hugging Face Hub\")\n",
    "#     parser.add_argument(\"--tokenizer_path\", default=\"domain_tokenizer\",\n",
    "#                         help=\"Caminho para o tokenizador\")\n",
    "#     parser.add_argument(\"--output_dir\", default=None,\n",
    "#                         help=\"Diretório para salvar o modelo treinado\")\n",
    "\n",
    "#     # Configurações de treinamento\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=3,\n",
    "#                         help=\"Número de épocas de treinamento\")\n",
    "#     parser.add_argument(\"--train_batch_size\", type=int, default=4,\n",
    "#                         help=\"Tamanho do batch por dispositivo\")\n",
    "#     parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2,\n",
    "#                         help=\"Passos de acumulação de gradiente\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=5e-3,\n",
    "#                         help=\"Taxa de aprendizado inicial\")\n",
    "#     parser.add_argument(\"--weight_decay\", type=float, default=0.01,\n",
    "#                         help=\"Peso de decaimento\")\n",
    "#     parser.add_argument(\"--warmup_steps\", type=int, default=0,\n",
    "#                         help=\"Passos de aquecimento para o scheduler\")\n",
    "\n",
    "#     # Configurações de dados\n",
    "#     parser.add_argument(\"--max_news\", type=int, default=3535,\n",
    "#                         help=\"Número máximo de notícias a usar\")\n",
    "#     parser.add_argument(\"--max_sentences\", type=int, default=3456,\n",
    "#                         help=\"Número máximo de sentenças\")\n",
    "#     parser.add_argument(\"--eval_ratio\", type=float, default=0.1,\n",
    "#                         help=\"Fração dos dados usada para validação\")\n",
    "\n",
    "#     # Otimizações\n",
    "#     parser.add_argument(\"--use_flash_attention\", action=\"store_true\",\n",
    "#                         help=\"Usar Flash Attention se disponível\")\n",
    "#     parser.add_argument(\"--fp16\", action=\"store_true\",\n",
    "#                         help=\"Usar treinamento em precisão mista\")\n",
    "#     parser.add_argument(\"--push_interval\", type=int, default=10000,\n",
    "#                         help=\"Intervalo de steps para push para o Hub\")\n",
    "#     parser.add_argument(\"--num_workers\", type=int, default=4,\n",
    "#                         help=\"Número de workers para processamento de dados\")\n",
    "\n",
    "#     # Modo de teste\n",
    "#     parser.add_argument(\"--testing\", action=\"store_true\",\n",
    "#                         help=\"Executar em modo de teste\")\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Configurar diretório de saída se não especificado\n",
    "#     if args.output_dir is None:\n",
    "#         model_name = args.model_id.split(\"/\")[-1]\n",
    "#         args.output_dir = f\"{model_name}-ptbr-{'test' if args.testing else 'full'}\"\n",
    "\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a568c-bcc4-495a-9ddf-4b5934126751",
   "metadata": {},
   "source": [
    "# 3. Classes utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e904e71f-adf9-498c-9648-7fa83f8804f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:43.423909Z",
     "iopub.status.busy": "2025-02-19T03:37:43.423600Z",
     "iopub.status.idle": "2025-02-19T03:37:43.467985Z",
     "shell.execute_reply": "2025-02-19T03:37:43.466963Z",
     "shell.execute_reply.started": "2025-02-19T03:37:43.423883Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainingParams(BaseModel):\n",
    "    \"\"\"Configuração completa para treinamento.\"\"\"\n",
    "\n",
    "    dataset_size: int\n",
    "    num_train_epochs: int\n",
    "    num_chunks: int\n",
    "    train_batch_size_per_device: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "\n",
    "    @field_validator(\"num_chunks\")\n",
    "    def validate_num_chunks(cls, v, info):\n",
    "        data = info.data\n",
    "        if (\n",
    "            \"dataset_size\" in data\n",
    "            and \"dataset_size\" in data\n",
    "            and \"eval_size_ratio\" in data\n",
    "        ):\n",
    "            dataset_size = data[\"dataset_size\"]\n",
    "            eval_size_per_chunk = int(data[\"dataset_size\"] * data[\"eval_size_ratio\"])\n",
    "            available_size = dataset_size - eval_size_per_chunk * v\n",
    "            if available_size < v:\n",
    "                raise ValueError(\n",
    "                    f\"available_size ({available_size}) deve ser maior ou igual a num_chunks ({v})\"\n",
    "                )\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        \"\"\"Tamanho efetivo do batch considerando a acumulação de gradiente.\"\"\"\n",
    "        return self.train_batch_size_per_device * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        \"\"\"Total de passos por época.\"\"\"\n",
    "        return math.ceil(self.dataset_size / self.effective_batch_size)\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        \"\"\"Total de passos de treinamento.\"\"\"\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        \"\"\"Tamanho do dataset de avaliação em cada chunk.\"\"\"\n",
    "        return int(self.dataset_size * self.eval_size_ratio / self.num_chunks)\n",
    "\n",
    "    @property\n",
    "    def available_size(self):\n",
    "        \"\"\"Tamanho disponível para treinamento.\"\"\"\n",
    "        return self.dataset_size - self.eval_size_per_chunk * self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def eval_size(self):\n",
    "        \"\"\"Tamanho total para avaliação.\"\"\"\n",
    "        return self.dataset_size - self.available_size\n",
    "\n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        \"\"\"Tamanho de cada chunk de dados.\"\"\"\n",
    "        return self.dataset_size // self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def chunk_train_size(self):\n",
    "        \"\"\"Tamanho de treinamento em cada chunk.\"\"\"\n",
    "        return self.available_size // self.num_chunks\n",
    "\n",
    "    def __repr(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"dataset_size\", self.dataset_size],\n",
    "            [\"num_chunks\", self.num_chunks],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"chunk_train_size\", self.chunk_train_size],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"available_size\", self.available_size],\n",
    "            [\"eval_size\", self.eval_size],\n",
    "            [\"train_batch_size_per_device\", self.train_batch_size_per_device],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__repr()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af160973-dd73-40a5-bb89-1bdf5306778a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:44.094008Z",
     "iopub.status.busy": "2025-02-19T03:37:44.093811Z",
     "iopub.status.idle": "2025-02-19T03:37:44.108693Z",
     "shell.execute_reply": "2025-02-19T03:37:44.108297Z",
     "shell.execute_reply.started": "2025-02-19T03:37:44.093993Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide o texto em sentenças.\n",
    "\n",
    "    Args:\n",
    "        text: Texto a ser dividido\n",
    "\n",
    "    Returns:\n",
    "        Lista de sentenças\n",
    "    \"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "def set_attention(model, use_flash_attention=False):\n",
    "    \"\"\"\n",
    "    Configura atenção do modelo, possibilitando uso de Flash Attention.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo a ser configurado\n",
    "        use_flash_attention: Se deve usar Flash Attention\n",
    "\n",
    "    Returns:\n",
    "        Modelo configurado\n",
    "    \"\"\"\n",
    "    if not use_flash_attention:\n",
    "        return model\n",
    "\n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            from flash_attn import flash_attn_qkvpacked_func\n",
    "\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except (ImportError, RuntimeError) as e:\n",
    "            logger.warning(f\"Flash Attention não é compatível: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    if check_flash_attention_support():\n",
    "        logger.info(\"Replacing standard attention with FlashAttention...\")\n",
    "        try:\n",
    "            from flash_attn import FlashAttention\n",
    "\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.MultiheadAttention):\n",
    "                    module.attention = FlashAttention()\n",
    "            logger.info(\"FlashAttention integrated successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to integrate FlashAttention: {str(e)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def check_vocab_size(tokenizer, model):\n",
    "    \"\"\"\n",
    "    Verifica se o tamanho do vocabulário do tokenizador é compatível com o modelo.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizador a ser verificado\n",
    "        model: Modelo a ser verificado\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Se os IDs do token estiverem fora do intervalo do modelo\n",
    "    \"\"\"\n",
    "    max_token_id = max(tokenizer.get_vocab().values())\n",
    "    logger.info(f\"Maior ID no tokenizador: {max_token_id}\")\n",
    "    logger.info(f\"Tamanho do vocabulário do modelo: {model.config.vocab_size}\")\n",
    "    assert max_token_id < model.config.vocab_size, \"IDs de tokens fora do intervalo!\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer, target_column=\"text\"):\n",
    "    \"\"\"\n",
    "    Função para tokenizar exemplos do dataset.\n",
    "\n",
    "    Args:\n",
    "        examples: Batch de exemplos a serem tokenizados\n",
    "        tokenizer: Tokenizador a ser utilizado\n",
    "        target_column: Nome da coluna contendo o texto\n",
    "\n",
    "    Returns:\n",
    "        Exemplos tokenizados\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[target_column],\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, num_proc=4):\n",
    "    \"\"\"\n",
    "    Tokeniza o dataset completo usando processamento paralelo.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset a ser tokenizado\n",
    "        tokenizer: Tokenizador a ser utilizado\n",
    "        num_proc: Número de processos para paralelização\n",
    "\n",
    "    Returns:\n",
    "        Dataset tokenizado\n",
    "    \"\"\"\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: tokenize_function(examples, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Colator de dados com preenchimento dinâmico para MLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(batch)\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Garante que os tensores de entrada tenham a forma e o tipo corretos.\n",
    "\n",
    "    Args:\n",
    "        inputs: Dicionário com tensores de entrada\n",
    "\n",
    "    Returns:\n",
    "        Dicionário com tensores corrigidos\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Se algum tensor tiver forma inesperada\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def forward_pass(model, inputs, device):\n",
    "    \"\"\"\n",
    "    Realiza uma passagem para frente no modelo.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo para realizar a passagem\n",
    "        inputs: Entradas do modelo\n",
    "        device: Dispositivo onde o modelo está\n",
    "\n",
    "    Returns:\n",
    "        Perda calculada pelo modelo\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Se o modelo não retornar uma perda\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "def evaluate(model, eval_dataset, data_collator, batch_size, device):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho do modelo no conjunto de validação.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo a ser avaliado\n",
    "        eval_dataset: Dataset de avaliação\n",
    "        data_collator: Colator de dados para preparar batches\n",
    "        batch_size: Tamanho do batch para avaliação\n",
    "        device: Dispositivo onde o modelo está\n",
    "\n",
    "    Returns:\n",
    "        Perda média de avaliação\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=batch_size)\n",
    "\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(device.type == \"cuda\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs, device)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    model, optimizer, scheduler, tokenizer, global_step, epoch, output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Salva um checkpoint do treinamento.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo a ser salvo\n",
    "        optimizer: Otimizador a ser salvo\n",
    "        scheduler: Scheduler a ser salvo\n",
    "        tokenizer: Tokenizador a ser salvo\n",
    "        global_step: Passo global atual\n",
    "        epoch: Época atual\n",
    "        output_dir: Diretório base para salvar\n",
    "    \"\"\"\n",
    "    checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Salvar estado do modelo e otimizador\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"global_step\": global_step,\n",
    "        },\n",
    "        os.path.join(checkpoint_dir, \"training_state.pt\"),\n",
    "    )\n",
    "\n",
    "    # Salvar modelo e tokenizador no formato HF\n",
    "    model.save_pretrained(checkpoint_dir)\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "    logger.info(f\"Saved checkpoint at step {global_step} to {checkpoint_dir}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Carrega um checkpoint de treinamento.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo a ser carregado\n",
    "        optimizer: Otimizador a ser carregado\n",
    "        scheduler: Scheduler a ser carregado\n",
    "        checkpoint_path: Caminho para o checkpoint\n",
    "\n",
    "    Returns:\n",
    "        Tupla com (modelo, otimizador, scheduler, global_step, epoch)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_path, \"training_state.pt\"))\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    return model, optimizer, scheduler, checkpoint[\"global_step\"], checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cdd39-c6e4-4d8e-b37c-eeb93986f31a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d818353-e5cf-4568-945a-86343678a9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T02:52:56.169751Z",
     "iopub.status.busy": "2025-02-19T02:52:56.169572Z",
     "iopub.status.idle": "2025-02-19T02:52:56.181958Z",
     "shell.execute_reply": "2025-02-19T02:52:56.181375Z",
     "shell.execute_reply.started": "2025-02-19T02:52:56.169739Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    base_id: str = Field(..., description=\"ID do modelo base no Hugging Face Hub\")\n",
    "    tokenizer_path: str = Field(..., description=\"Caminho para o tokenizador\")\n",
    "    output_dir: str = Field(..., description=\"Diretório para salvar o modelo treinado\")\n",
    "\n",
    "\n",
    "class DatasetConfig(BaseModel):\n",
    "    id: str = Field(..., description=\"ID do dataset no HF Hub\")\n",
    "    max_news: int = Field(..., description=\"Número máximo de notícias a usar\")\n",
    "    max_sentences: int = Field(..., description=\"Número máximo de sentenças\")\n",
    "    eval_ratio: float = Field(..., description=\"Fração dos dados usada para validação\")\n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    num_train_epochs: int = Field(..., description=\"Número de épocas de treinamento\")\n",
    "    train_batch_size: int = Field(..., description=\"Tamanho do batch por dispositivo\")\n",
    "    gradient_accumulation_steps: int = Field(\n",
    "        ..., description=\"Passos de acumulação de gradiente\"\n",
    "    )\n",
    "    learning_rate: float = Field(..., description=\"Taxa de aprendizado inicial\")\n",
    "    weight_decay: float = Field(..., description=\"Peso de decaimento\")\n",
    "    warmup_steps: int = Field(..., description=\"Passos de aquecimento para o scheduler\")\n",
    "    mlm_probabilities: List[float] = Field(\n",
    "        ..., description=\"Probabilidades de mascaramento para diferentes chunks\"\n",
    "    )\n",
    "    total_save_limit: int = Field(\n",
    "        ..., description=\"Número máximo de checkpoints a manter\"\n",
    "    )\n",
    "\n",
    "\n",
    "class OptimizationConfig(BaseModel):\n",
    "    use_flash_attention: bool = Field(\n",
    "        ..., description=\"Usar Flash Attention se disponível\"\n",
    "    )\n",
    "    fp16: bool = Field(..., description=\"Usar treinamento em precisão mista\")\n",
    "    push_interval: int = Field(\n",
    "        ..., description=\"Intervalo de steps para push para o Hub\"\n",
    "    )\n",
    "    num_workers: int = Field(\n",
    "        ..., description=\"Número de workers para processamento de dados\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ExecutionConfig(BaseModel):\n",
    "    testing: bool = Field(..., description=\"Executar em modo de teste\")\n",
    "    seed: int = Field(..., description=\"Semente para reprodutibilidade\")\n",
    "    device: str = Field(..., description=\"Dispositivo de execução: auto, cuda ou cpu\")\n",
    "\n",
    "\n",
    "class MLMTrainingConfig(BaseModel):\n",
    "    model: ModelConfig\n",
    "    dataset: DatasetConfig\n",
    "    training: TrainingConfig\n",
    "    optimization: OptimizationConfig\n",
    "    execution: ExecutionConfig\n",
    "\n",
    "\n",
    "def parse_yaml(file_path: str) -> MLMTrainingConfig:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return MLMTrainingConfig(**data)\n",
    "\n",
    "\n",
    "config = parse_yaml(\"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d0db4-74e9-43ed-a84e-77277e63401e",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe69390-fad8-44cf-8b4a-ab6ed08f0d0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:50.607648Z",
     "iopub.status.busy": "2025-02-19T03:37:50.607389Z",
     "iopub.status.idle": "2025-02-19T03:37:50.623261Z",
     "shell.execute_reply": "2025-02-19T03:37:50.622884Z",
     "shell.execute_reply.started": "2025-02-19T03:37:50.607630Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:37:50,621 - __main__ - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d79bd5-77d7-4935-88a4-0bf822489058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fc0d1-e2b5-4e79-84df-3d50a996f74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e43f3e-cd4e-4745-950f-f58580533423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:52.211008Z",
     "iopub.status.busy": "2025-02-19T03:37:52.210777Z",
     "iopub.status.idle": "2025-02-19T03:37:52.536858Z",
     "shell.execute_reply": "2025-02-19T03:37:52.536226Z",
     "shell.execute_reply.started": "2025-02-19T03:37:52.210991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:37:52,223 - __main__ - INFO - Loading dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m raw_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mid, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m raw_dataset\u001b[38;5;241m.\u001b[39mto_pandas()\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(config\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmax_news, \u001b[38;5;28mlen\u001b[39m(df)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "raw_dataset = load_dataset(config.dataset.id, split=\"train\")\n",
    "df = raw_dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "sample_df = df.sample(min(config.dataset.max_news, len(df)))\n",
    "\n",
    "logger.info(\"Preparing sentences...\")\n",
    "combined_texts = sample_df[\"text\"].to_list() + sample_df[\"title\"].to_list()\n",
    "sentences = [\n",
    "    phrase for text in combined_texts if text for phrase in split_into_sentences(text)\n",
    "]\n",
    "sentences_sample = pd.Series(sentences).sample(config.dataset.max_sentences).to_list()\n",
    "dataset = Dataset.from_dict({\"text\": sentences_sample})\n",
    "\n",
    "# Setup model and tokenizer\n",
    "logger.info(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model.tokenizer_path, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(config.model.base_id)\n",
    "\n",
    "if config.optimization.fp16:\n",
    "    model_config.torch_dtype = torch.float16\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(config.model.base_id, config=model_config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Setup attention mechanism\n",
    "model = set_attention(model, config.optimization.use_flash_attention)\n",
    "\n",
    "# Check vocabulary compatibility\n",
    "check_vocab_size(tokenizer, model)\n",
    "\n",
    "# Create training config\n",
    "training_params = TrainingParams(\n",
    "    num_train_epochs=config.training.num_train_epochs,\n",
    "    dataset_size=len(dataset),\n",
    "    num_chunks=len(config.training.mlm_probabilities),\n",
    "    train_batch_size_per_device=config.training.train_batch_size,\n",
    "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "    eval_size_ratio=config.dataset.eval_ratio,\n",
    "    total_save_limit=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"Training configuration:\\n{training_params}\")\n",
    "\n",
    "# Tokenize dataset\n",
    "logger.info(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(\n",
    "    dataset, tokenizer, num_proc=config.optimization.num_workers\n",
    ")\n",
    "\n",
    "# Setup optimizer, scheduler and scaler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    weight_decay=config.training.weight_decay,\n",
    ")\n",
    "scaler = torch.amp.GradScaler(\n",
    "    enabled=(device.type == \"cuda\" and config.optimization.fp16)\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.training.warmup_steps,\n",
    "    num_training_steps=training_params.total_train_steps,\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "latest_checkpoint = None\n",
    "\n",
    "if os.path.exists(config.model.output_dir):\n",
    "    checkpoints = [\n",
    "        d for d in os.listdir(config.model.output_dir) if d.startswith(\"checkpoint-\")\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "        checkpoint_path = os.path.join(config.model.output_dir, latest_checkpoint)\n",
    "        model, optimizer, scheduler, global_step, start_epoch = load_checkpoint(\n",
    "            model, optimizer, scheduler, checkpoint_path\n",
    "        )\n",
    "else:\n",
    "    os.makedirs(config.model.output_dir, exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, training_params.num_train_epochs):\n",
    "        for chunk_number, mlm_probability in enumerate(\n",
    "            config.training.mlm_probabilities\n",
    "        ):\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch + 1}/{training_params.num_train_epochs} | \"\n",
    "                f\"MLM Probability: {mlm_probability}\"\n",
    "            )\n",
    "\n",
    "            # Setup data collator with current MLM probability\n",
    "            data_collator = DynamicPaddingDataCollator(\n",
    "                tokenizer, mlm_probability=mlm_probability\n",
    "            )\n",
    "\n",
    "            # Split dataset for this chunk\n",
    "            eval_start_idx = chunk_number * training_params.chunk_size\n",
    "            eval_end_idx = eval_start_idx + training_params.eval_size_per_chunk - 1\n",
    "            train_start_idx = (\n",
    "                chunk_number * training_params.chunk_size\n",
    "                + training_params.eval_size_per_chunk\n",
    "            )\n",
    "            train_end_idx = train_start_idx + training_params.chunk_train_size - 1\n",
    "\n",
    "            logger.info(\n",
    "                f\"Splitting | \"\n",
    "                f\"chunk: {eval_start_idx}-{train_end_idx} | \"\n",
    "                f\"eval: {eval_start_idx}-{eval_end_idx} | \"\n",
    "                f\"train: {train_start_idx}-{train_end_idx}\"\n",
    "            )\n",
    "\n",
    "            train_dataset = (\n",
    "                tokenized_dataset.skip(train_start_idx)\n",
    "                .take(training_params.chunk_train_size)\n",
    "                .shuffle(seed=42)\n",
    "            )\n",
    "\n",
    "            eval_dataset = (\n",
    "                tokenized_dataset.skip(eval_start_idx)\n",
    "                .take(training_params.eval_size_per_chunk)\n",
    "                .shuffle(seed=42)\n",
    "            )\n",
    "\n",
    "            # Train on this chunk\n",
    "            train_iterator = train_dataset.iter(\n",
    "                batch_size=training_params.train_batch_size_per_device\n",
    "            )\n",
    "\n",
    "            for step, batch in tqdm(\n",
    "                enumerate(train_iterator), desc=f\"Training (MLM {mlm_probability})\"\n",
    "            ):\n",
    "                # Check if accumulation step is complete\n",
    "                accumulation_step_complete = (\n",
    "                    step + 1\n",
    "                ) % training_params.gradient_accumulation_steps == 0\n",
    "\n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    inputs = data_collator(batch)\n",
    "                    loss = forward_pass(model, inputs, device)\n",
    "\n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(\n",
    "                        loss / training_params.gradient_accumulation_steps\n",
    "                    ).backward()\n",
    "\n",
    "                    if accumulation_step_complete:\n",
    "                        # Update model parameters\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Update global step\n",
    "                        global_step += 1\n",
    "\n",
    "                        # Evaluate periodically\n",
    "                        eval_interval = max(\n",
    "                            1, training_params.total_steps_per_epoch // 4\n",
    "                        )\n",
    "                        if global_step % eval_interval == 0:\n",
    "                            eval_loss = evaluate(\n",
    "                                model,\n",
    "                                eval_dataset,\n",
    "                                data_collator,\n",
    "                                batch_size=training_params.train_batch_size_per_device,\n",
    "                                device=device,\n",
    "                            )\n",
    "                            logger.info(\n",
    "                                f\"Evaluation loss at step {global_step}: {eval_loss}\"\n",
    "                            )\n",
    "\n",
    "                        # Save checkpoint periodically\n",
    "                        save_interval = config.optimization.push_interval\n",
    "                        if global_step % save_interval == 0:\n",
    "                            save_checkpoint(\n",
    "                                model,\n",
    "                                optimizer,\n",
    "                                scheduler,\n",
    "                                tokenizer,\n",
    "                                global_step,\n",
    "                                epoch,\n",
    "                                config.model.output_dir,\n",
    "                            )\n",
    "\n",
    "                        # Clear CUDA cache periodically\n",
    "                        if device.type == \"cuda\" and global_step % 100 == 0:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Training batch failed: {e}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "            # Evaluate at the end of each chunk\n",
    "            logger.info(f\"Evaluating at the end of chunk {chunk_number}...\")\n",
    "            eval_loss = evaluate(\n",
    "                model,\n",
    "                eval_dataset,\n",
    "                data_collator,\n",
    "                batch_size=training_params.train_batch_size_per_device,\n",
    "                device=device,\n",
    "            )\n",
    "            logger.info(f\"Chunk {chunk_number} evaluation loss: {eval_loss}\")\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            tokenizer,\n",
    "            global_step,\n",
    "            epoch,\n",
    "            config.model.output_dir,\n",
    "        )\n",
    "\n",
    "    # Save final model\n",
    "    logger.info(\"Training complete. Saving final model...\")\n",
    "    model.save_pretrained(\n",
    "        config.model.output_dir,\n",
    "    )\n",
    "    tokenizer.save_pretrained(\n",
    "        config.model.output_dir,\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Training interrupted by user. Saving checkpoint...\")\n",
    "    save_checkpoint(\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        tokenizer,\n",
    "        global_step,\n",
    "        epoch,\n",
    "        config.model.output_dir,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135813ec-67e5-4f56-b55c-d2d8f6607e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:37:53.124148Z",
     "iopub.status.busy": "2025-02-19T03:37:53.123924Z",
     "iopub.status.idle": "2025-02-19T03:37:53.139266Z",
     "shell.execute_reply": "2025-02-19T03:37:53.138558Z",
     "shell.execute_reply.started": "2025-02-19T03:37:53.124132Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mmlm_probabilities\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "config.training.mlm_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60894db8-9342-47eb-b97c-b5ca659339bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9b0041-2789-4911-880d-ffe5c284855b",
   "metadata": {},
   "source": [
    "# REFACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84b9dee-70be-4aea-a09a-5b177da7fdad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:38:06.395227Z",
     "iopub.status.busy": "2025-02-19T03:38:06.394954Z",
     "iopub.status.idle": "2025-02-19T03:38:06.411252Z",
     "shell.execute_reply": "2025-02-19T03:38:06.410816Z",
     "shell.execute_reply.started": "2025-02-19T03:38:06.395206Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    base_id: str = Field(..., description=\"ID do modelo base no Hugging Face Hub\")\n",
    "    tokenizer_path: str = Field(..., description=\"Caminho para o tokenizador\")\n",
    "    output_dir: str = Field(..., description=\"Diretório para salvar o modelo treinado\")\n",
    "\n",
    "\n",
    "class DatasetConfig(BaseModel):\n",
    "    id: str = Field(..., description=\"ID do dataset no HF Hub\")\n",
    "    max_news: int = Field(..., description=\"Número máximo de notícias a usar\")\n",
    "    max_sentences: int = Field(..., description=\"Número máximo de sentenças\")\n",
    "    eval_ratio: float = Field(..., description=\"Fração dos dados usada para validação\")\n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    num_train_epochs: int = Field(..., description=\"Número de épocas de treinamento\")\n",
    "    train_batch_size: int = Field(..., description=\"Tamanho do batch por dispositivo\")\n",
    "    gradient_accumulation_steps: int = Field(\n",
    "        ..., description=\"Passos de acumulação de gradiente\"\n",
    "    )\n",
    "    learning_rate: float = Field(..., description=\"Taxa de aprendizado inicial\")\n",
    "    weight_decay: float = Field(..., description=\"Peso de decaimento\")\n",
    "    warmup_steps: int = Field(..., description=\"Passos de aquecimento para o scheduler\")\n",
    "    mlm_probabilities: List[float] = Field(\n",
    "        ..., description=\"Probabilidades de mascaramento para diferentes chunks\"\n",
    "    )\n",
    "    total_save_limit: int = Field(\n",
    "        ..., description=\"Número máximo de checkpoints a manter\"\n",
    "    )\n",
    "    fp16: bool = Field(..., description=\"Usar treinamento em precisão mista\")\n",
    "    use_flash_attention: bool = Field(\n",
    "        ..., description=\"Usar Flash Attention se disponível\"\n",
    "    )\n",
    "    push_interval: int = Field(\n",
    "        ..., description=\"Intervalo de steps para push para o Hub\"\n",
    "    )\n",
    "    num_workers: int = Field(\n",
    "        ..., description=\"Número de workers para processamento de dados\"\n",
    "    )\n",
    "    clear_memory_steps: int = Field(\n",
    "        ..., description=\"Número de passos antes de limpar a memória\"\n",
    "    )\n",
    "    memory_log_steps: int = Field(\n",
    "        ..., description=\"Número de passos para retornar um log de uso da memória\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ExecutionConfig(BaseModel):\n",
    "    testing: bool = Field(..., description=\"Executar em modo de teste\")\n",
    "    seed: int = Field(..., description=\"Semente para reprodutibilidade\")\n",
    "    device: str = Field(..., description=\"Dispositivo de execução: auto, cuda ou cpu\")\n",
    "\n",
    "\n",
    "class MLMTrainingConfig(BaseModel):\n",
    "    model: ModelConfig\n",
    "    dataset: DatasetConfig\n",
    "    training: TrainingConfig\n",
    "    execution: ExecutionConfig\n",
    "\n",
    "\n",
    "def parse_yaml(file_path: str) -> MLMTrainingConfig:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return MLMTrainingConfig(**data)\n",
    "\n",
    "\n",
    "config = parse_yaml(\"config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec899ba-bcec-48d5-aa6d-fc8bcb5d9f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:48:13.114864Z",
     "iopub.status.busy": "2025-02-19T03:48:13.114642Z",
     "iopub.status.idle": "2025-02-19T03:48:13.136736Z",
     "shell.execute_reply": "2025-02-19T03:48:13.136178Z",
     "shell.execute_reply.started": "2025-02-19T03:48:13.114848Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import Dataset, load_dataset\n",
    "from pydantic import BaseModel, Field\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "def log_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        device_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = device_properties.total_memory / 1024**2\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**2\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / 1024**2\n",
    "        allocated_percent = (allocated_memory / total_memory) * 100\n",
    "        reserved_percent = (reserved_memory / total_memory) * 100\n",
    "        table = tabulate([\n",
    "            [\"Total de Memória (MB)\", f\"{int(round(total_memory))}\", f\"100%\"],\n",
    "            [\"Memória Alocada (MB)\", f\"{int(round(allocated_memory))}\", f\"{int(round(allocated_percent))}%\"],\n",
    "            [\"Memória Reservada (MB)\", f\"{int(round(reserved_memory))}\", f\"{int(round(reserved_percent))}%\"]\n",
    "        ], headers=[\"Descrição\", \"Valor\", \"Percentual\"], tablefmt=\"grid\")\n",
    "        logger.info(f\"\\nUso de VRAM pela GPU:\\n{table}\")\n",
    "    else:\n",
    "        logger.info(\"CUDA não está disponível.\")\n",
    "\n",
    "\n",
    "import gc\n",
    "def clear_vram():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()\n",
    "    if hasattr(torch.cuda, 'ipc_collect'):\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "    logger.info(\"VRAM cleared definitively.\")\n",
    "\n",
    "# def clear_vram():\n",
    "#     \"\"\"Limpa a memória da GPU antes de iniciar o treinamento.\"\"\"\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#         logger.info(\"VRAM cleared before training.\")\n",
    "\n",
    "class TrainingParams:\n",
    "    \"\"\"Calcula parâmetros derivados para o treinamento\"\"\"\n",
    "\n",
    "    def __init__(self, config: MLMTrainingConfig):\n",
    "        self.num_train_epochs = config.training.num_train_epochs\n",
    "        self.train_batch_size_per_device = config.training.train_batch_size\n",
    "        self.gradient_accumulation_steps = config.training.gradient_accumulation_steps\n",
    "        self.eval_size_ratio = config.dataset.eval_ratio\n",
    "        self.total_save_limit = config.training.total_save_limit\n",
    "        self.clear_memory_steps = config.training.clear_memory_steps\n",
    "        self.memory_log_steps = config.training.memory_log_steps\n",
    "\n",
    "        # Cálculos derivados\n",
    "        self.dataset_size = None  # Será definido posteriormente\n",
    "        self.num_chunks = len(config.training.mlm_probabilities)\n",
    "\n",
    "    def calculate_derived_params(self, dataset_size: int):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.chunk_size = self.dataset_size // self.num_chunks\n",
    "        self.eval_size_per_chunk = int(self.chunk_size * self.eval_size_ratio)\n",
    "        self.chunk_train_size = self.chunk_size - self.eval_size_per_chunk\n",
    "        self.total_train_steps = (\n",
    "            self.chunk_train_size // self.train_batch_size_per_device\n",
    "        ) * self.num_train_epochs\n",
    "        self.effective_batch_size = (\n",
    "            self.train_batch_size_per_device * self.gradient_accumulation_steps\n",
    "        )\n",
    "        self.total_steps_per_epoch = math.ceil(\n",
    "            self.dataset_size / self.effective_batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Prepara e processa o dataset para treinamento\"\"\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_and_prepare_data(self) -> Dataset:\n",
    "        logger.info(\"Loading dataset...\")\n",
    "        raw_dataset = load_dataset(self.config.id, split=\"train\")\n",
    "        df = raw_dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "        sample_df = df.sample(min(self.config.max_news, len(df)))\n",
    "\n",
    "        logger.info(\"Preparing sentences...\")\n",
    "        combined_texts = sample_df[\"text\"].to_list() + sample_df[\"title\"].to_list()\n",
    "        sentences = [\n",
    "            phrase\n",
    "            for text in combined_texts\n",
    "            if text\n",
    "            for phrase in split_into_sentences(text)\n",
    "        ]\n",
    "        sentences_sample = (\n",
    "            pd.Series(sentences).sample(self.config.max_sentences).to_list()\n",
    "        )\n",
    "        return Dataset.from_dict({\"text\": sentences_sample})\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "\n",
    "# region Gerenciamento de Modelo\n",
    "class ModelManager:\n",
    "    \"\"\"Responsável por carregar e configurar o modelo\"\"\"\n",
    "\n",
    "    def __init__(self, model_config: ModelConfig, training_config: TrainingConfig):\n",
    "        self.model_config = model_config\n",
    "        self.training_config = training_config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def initialize_model(self) -> Tuple[AutoModelForMaskedLM, AutoTokenizer]:\n",
    "        logger.info(\"Loading tokenizer and model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_config.tokenizer_path, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        model_config = AutoConfig.from_pretrained(self.model_config.base_id)\n",
    "        if self.training_config.fp16:\n",
    "            model_config.torch_dtype = torch.float16\n",
    "\n",
    "        model = AutoModelForMaskedLM.from_pretrained(\n",
    "            self.model_config.base_id, config=model_config\n",
    "        )\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.to(self.device)\n",
    "\n",
    "        # Configuração adicional\n",
    "        model = set_attention(model, self.training_config.use_flash_attention)\n",
    "        check_vocab_size(tokenizer, model)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "\n",
    "# region Treinamento\n",
    "class TrainingComponents:\n",
    "    \"\"\"Configura componentes para o treinamento\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForMaskedLM,\n",
    "        training_config: TrainingConfig,\n",
    "        total_steps: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.training_config = training_config\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def setup_components(self) -> Tuple[AdamW, GradScaler, Any]:\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.training_config.learning_rate,\n",
    "            weight_decay=self.training_config.weight_decay,\n",
    "        )\n",
    "\n",
    "        scaler = GradScaler(\n",
    "            enabled=(self.model.device.type == \"cuda\" and self.training_config.fp16)\n",
    "        )\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.training_config.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "\n",
    "        return optimizer, scaler, scheduler\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Gerencia checkpoints de treinamento\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str,\n",
    "        model: AutoModelForMaskedLM,\n",
    "        optimizer: AdamW,\n",
    "        scheduler: Any,\n",
    "    ):\n",
    "        self.output_dir = output_dir\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def save_checkpoint(self, global_step: int, epoch: int, tokenizer: AutoTokenizer):\n",
    "        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-{global_step}\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"global_step\": global_step,\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "            },\n",
    "            os.path.join(checkpoint_dir, \"training_state.pt\"),\n",
    "        )\n",
    "\n",
    "        self.model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)\n",
    "        logger.info(f\"Checkpoint saved at {checkpoint_dir}\")\n",
    "\n",
    "    def load_latest_checkpoint(self) -> Tuple[int, int]:\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return 0, 0\n",
    "\n",
    "        checkpoints = [\n",
    "            d for d in os.listdir(self.output_dir) if d.startswith(\"checkpoint-\")\n",
    "        ]\n",
    "        if not checkpoints:\n",
    "            return 0, 0\n",
    "\n",
    "        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "        checkpoint_path = os.path.join(self.output_dir, latest_checkpoint)\n",
    "\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_path, \"training_state.pt\"))\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "        return checkpoint[\"global_step\"], checkpoint[\"epoch\"]\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Orquestra o processo de treinamento\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MLMTrainingConfig,\n",
    "        model: AutoModelForMaskedLM,\n",
    "        tokenizer: AutoTokenizer,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "        self.global_step = 0\n",
    "        self.start_epoch = 0\n",
    "\n",
    "        clear_vram()\n",
    "\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        training_params = TrainingParams(self.config)\n",
    "        training_params.calculate_derived_params(len(dataset))\n",
    "\n",
    "        optimizer, scaler, scheduler = TrainingComponents(\n",
    "            self.model, self.config.training, training_params.total_train_steps\n",
    "        ).setup_components()\n",
    "\n",
    "        checkpoint_manager = CheckpointManager(\n",
    "            self.config.model.output_dir, self.model, optimizer, scheduler\n",
    "        )\n",
    "\n",
    "        # Tenta carregar checkpoint existente\n",
    "        self.global_step, self.start_epoch = checkpoint_manager.load_latest_checkpoint()\n",
    "\n",
    "        try:\n",
    "            for epoch in range(self.start_epoch, training_params.num_train_epochs):\n",
    "                self._train_epoch(\n",
    "                    epoch,\n",
    "                    training_params,\n",
    "                    optimizer,\n",
    "                    scaler,\n",
    "                    scheduler,\n",
    "                    checkpoint_manager,\n",
    "                    dataset,\n",
    "                )\n",
    "\n",
    "            self._save_final_model()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Training interrupted. Saving final checkpoint...\")\n",
    "            checkpoint_manager.save_checkpoint(self.global_step, epoch, self.tokenizer)\n",
    "\n",
    "    def _train_epoch(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        params: TrainingParams,\n",
    "        optimizer: AdamW,\n",
    "        scaler: GradScaler,\n",
    "        scheduler: Any,\n",
    "        checkpoint_manager: CheckpointManager,\n",
    "        dataset: Dataset,\n",
    "    ):\n",
    "        tokenized_dataset = tokenize_dataset(\n",
    "            dataset, self.tokenizer, num_proc=self.config.training.num_workers\n",
    "        )\n",
    "\n",
    "        for chunk_number, mlm_probability in enumerate(\n",
    "            self.config.training.mlm_probabilities\n",
    "        ):\n",
    "            data_collator = DynamicPaddingDataCollator(\n",
    "                self.tokenizer, mlm_probability=mlm_probability\n",
    "            )\n",
    "\n",
    "            train_dataset, eval_dataset = self._prepare_datachunk(\n",
    "                chunk_number, params, tokenized_dataset\n",
    "            )\n",
    "\n",
    "            self._train_chunk(\n",
    "                epoch,\n",
    "                chunk_number,\n",
    "                mlm_probability,\n",
    "                params,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                data_collator,\n",
    "                optimizer,\n",
    "                scaler,\n",
    "                scheduler,\n",
    "                checkpoint_manager,\n",
    "            )\n",
    "\n",
    "    def _prepare_datachunk(\n",
    "        self, chunk_number: int, params: TrainingParams, tokenized_dataset\n",
    "    ) -> Tuple[Dataset, Dataset]:\n",
    "        eval_start_idx = chunk_number * params.chunk_size\n",
    "        eval_end_idx = eval_start_idx + params.eval_size_per_chunk - 1\n",
    "        train_start_idx = chunk_number * params.chunk_size + params.eval_size_per_chunk\n",
    "        train_end_idx = train_start_idx + params.chunk_train_size - 1\n",
    "\n",
    "        train_dataset = (\n",
    "            tokenized_dataset.skip(train_start_idx)\n",
    "            .take(params.chunk_train_size)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "\n",
    "        eval_dataset = (\n",
    "            tokenized_dataset.skip(eval_start_idx)\n",
    "            .take(params.eval_size_per_chunk)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def _train_chunk(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        chunk_number: int,\n",
    "        mlm_probability: float,\n",
    "        params: TrainingParams,\n",
    "        train_dataset: Dataset,\n",
    "        eval_dataset: Dataset,\n",
    "        data_collator: DynamicPaddingDataCollator,\n",
    "        optimizer: AdamW,\n",
    "        scaler: GradScaler,\n",
    "        scheduler: Any,\n",
    "        checkpoint_manager: CheckpointManager,\n",
    "    ):\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch + 1}/{params.num_train_epochs} | MLM Probability: {mlm_probability}\"\n",
    "        )\n",
    "\n",
    "        train_iterator = train_dataset.iter(\n",
    "            batch_size=params.train_batch_size_per_device\n",
    "        )\n",
    "\n",
    "        for step, batch in tqdm(\n",
    "            enumerate(train_iterator), desc=f\"Training (MLM {mlm_probability})\"\n",
    "        ):\n",
    "            accumulation_step_complete = (\n",
    "                step + 1\n",
    "            ) % params.gradient_accumulation_steps == 0\n",
    "\n",
    "            try:\n",
    "                loss = self._training_step(batch, data_collator, scaler)\n",
    "\n",
    "                if accumulation_step_complete:\n",
    "                    self._update_model(optimizer, scaler, scheduler)\n",
    "                    self.global_step += 1\n",
    "\n",
    "                    self._periodic_operations(\n",
    "                        params,\n",
    "                        eval_dataset,\n",
    "                        data_collator,\n",
    "                        optimizer,\n",
    "                        scaler,\n",
    "                        scheduler,\n",
    "                        checkpoint_manager,\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "        self._evaluate_chunk(chunk_number, eval_dataset, data_collator, params)\n",
    "\n",
    "    def _training_step(\n",
    "        self, batch: dict, data_collator: DynamicPaddingDataCollator, scaler: GradScaler\n",
    "    ) -> float:\n",
    "        inputs = data_collator(batch)\n",
    "        loss = forward_pass(self.model, inputs, self.device)\n",
    "        scaler.scale(loss / self.config.training.gradient_accumulation_steps).backward()\n",
    "        return loss.item()\n",
    "\n",
    "    def _update_model(self, optimizer: AdamW, scaler: GradScaler, scheduler: Any):\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def _periodic_operations(\n",
    "        self,\n",
    "        params: TrainingParams,\n",
    "        eval_dataset: Dataset,\n",
    "        data_collator: DynamicPaddingDataCollator,\n",
    "        optimizer: AdamW,\n",
    "        scaler: GradScaler,\n",
    "        scheduler: Any,\n",
    "        checkpoint_manager: CheckpointManager,\n",
    "    ):\n",
    "        eval_interval = max(1, params.total_steps_per_epoch // 4)\n",
    "        if self.global_step % eval_interval == 0:\n",
    "            eval_loss = evaluate(\n",
    "                self.model,\n",
    "                eval_dataset,\n",
    "                data_collator,\n",
    "                batch_size=params.train_batch_size_per_device,\n",
    "                device=self.device,\n",
    "            )\n",
    "            logger.info(f\"Evaluation loss at step {self.global_step}: {eval_loss}\")\n",
    "\n",
    "        if self.global_step % self.config.training.push_interval == 0:\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                self.global_step, self.start_epoch, self.tokenizer\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            self.global_step % params.memory_log_steps == 0\n",
    "        ):  # Ajuste a frequência conforme necessário\n",
    "            log_gpu_memory_usage()\n",
    "\n",
    "        if (\n",
    "            self.device.type == \"cuda\"\n",
    "            and self.global_step % params.clear_memory_steps == 0\n",
    "        ):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def _evaluate_chunk(\n",
    "        self,\n",
    "        chunk_number: int,\n",
    "        eval_dataset: Dataset,\n",
    "        data_collator: DynamicPaddingDataCollator,\n",
    "        params: TrainingParams,\n",
    "    ):\n",
    "        logger.info(f\"Evaluating at the end of chunk {chunk_number}...\")\n",
    "        eval_loss = evaluate(\n",
    "            self.model,\n",
    "            eval_dataset,\n",
    "            data_collator,\n",
    "            batch_size=params.train_batch_size_per_device,\n",
    "            device=self.device,\n",
    "        )\n",
    "        logger.info(f\"Chunk evaluation loss: {eval_loss}\")\n",
    "\n",
    "    def _save_final_model(self):\n",
    "        logger.info(\"Training complete. Saving final model...\")\n",
    "        self.model.save_pretrained(self.config.model.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.config.model.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7a6038b-45d1-47db-9616-131ca6b9e1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:48:14.658463Z",
     "iopub.status.busy": "2025-02-19T03:48:14.658275Z",
     "iopub.status.idle": "2025-02-19T03:48:19.961380Z",
     "shell.execute_reply": "2025-02-19T03:48:19.960885Z",
     "shell.execute_reply.started": "2025-02-19T03:48:14.658450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:14,663 - __main__ - INFO - Loading dataset...\n",
      "2025-02-19 03:48:18,744 - __main__ - INFO - Preparing sentences...\n",
      "2025-02-19 03:48:18,835 - __main__ - INFO - Loading tokenizer and model...\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-02-19 03:48:19,958 - __main__ - INFO - Maior ID no tokenizador: 32767\n",
      "2025-02-19 03:48:19,959 - __main__ - INFO - Tamanho do vocabulário do modelo: 32768\n"
     ]
    }
   ],
   "source": [
    "config_path = \"config.yml\"\n",
    "config = parse_yaml(config_path)\n",
    "\n",
    "# Pré-processamento de dados\n",
    "data_preprocessor = DataPreprocessor(config.dataset)\n",
    "dataset = data_preprocessor.load_and_prepare_data()\n",
    "\n",
    "# Inicialização do modelo\n",
    "model_manager = ModelManager(config.model, config.training)\n",
    "model, tokenizer = model_manager.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f66b4afb-ed5a-4a89-8872-1224628e336f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:48:24.296210Z",
     "iopub.status.busy": "2025-02-19T03:48:24.295893Z",
     "iopub.status.idle": "2025-02-19T03:48:24.299778Z",
     "shell.execute_reply": "2025-02-19T03:48:24.299395Z",
     "shell.execute_reply.started": "2025-02-19T03:48:24.296186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:24,297 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1200 | 31%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    1270 | 33%          |\n",
      "+------------------------+---------+--------------+\n"
     ]
    }
   ],
   "source": [
    "log_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d07c6134-a3ab-40ca-a046-90ea6c04ed33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:48:30.254229Z",
     "iopub.status.busy": "2025-02-19T03:48:30.253860Z",
     "iopub.status.idle": "2025-02-19T03:48:30.695170Z",
     "shell.execute_reply": "2025-02-19T03:48:30.694801Z",
     "shell.execute_reply.started": "2025-02-19T03:48:30.254202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:30,692 - __main__ - INFO - VRAM cleared definitively.\n"
     ]
    }
   ],
   "source": [
    "clear_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70d9f14a-e9c4-45ae-84ac-f5c6be0e5a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:48:34.890731Z",
     "iopub.status.busy": "2025-02-19T03:48:34.890546Z",
     "iopub.status.idle": "2025-02-19T03:48:34.894559Z",
     "shell.execute_reply": "2025-02-19T03:48:34.894129Z",
     "shell.execute_reply.started": "2025-02-19T03:48:34.890717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:34,891 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1200 | 31%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    1270 | 33%          |\n",
      "+------------------------+---------+--------------+\n"
     ]
    }
   ],
   "source": [
    "log_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed0cb9-f07e-46da-8dd5-41a6def6e7c5",
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-19T03:49:56.551Z",
     "iopub.execute_input": "2025-02-19T03:48:37.817673Z",
     "iopub.status.busy": "2025-02-19T03:48:37.817083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:38,387 - __main__ - INFO - VRAM cleared definitively.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480ab1b5fd9493f882f7e30202984b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 03:48:39,509 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.05\n",
      "2025-02-19 03:48:40,288 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2252 | 58%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.05): 3it [00:01,  2.43it/s]\n",
      "\u001b[Aluating: 0it [00:00, ?it/s]\n",
      "\u001b[Aluating: 1it [00:00,  6.68it/s]\n",
      "\u001b[Aluating: 2it [00:00,  6.38it/s]\n",
      "\u001b[Aluating: 3it [00:00,  2.75it/s]\n",
      "\u001b[Aluating: 4it [00:01,  3.80it/s]\n",
      "Evaluating: 5it [00:01,  4.20it/s]\n",
      "2025-02-19 03:48:42,464 - __main__ - INFO - Evaluation loss at step 31: 10.999372100830078\n",
      "2025-02-19 03:48:50,058 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2454 | 63%          |\n",
      "+------------------------+---------+--------------+\n",
      "2025-02-19 03:48:59,764 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2454 | 63%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.05): 45it [00:21,  2.08it/s]\n",
      "2025-02-19 03:49:01,171 - __main__ - INFO - Evaluating at the end of chunk 0...\n",
      "Evaluating: 5it [00:01,  4.34it/s]\n",
      "2025-02-19 03:49:02,326 - __main__ - INFO - Chunk evaluation loss: 8.647303295135497\n",
      "2025-02-19 03:49:02,335 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.1\n",
      "2025-02-19 03:49:10,757 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2432 | 62%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.1): 21it [00:09,  2.36it/s]\n",
      "\u001b[Aluating: 0it [00:00, ?it/s]\n",
      "\u001b[Aluating: 1it [00:00,  6.69it/s]\n",
      "\u001b[Aluating: 2it [00:00,  6.39it/s]\n",
      "\u001b[Aluating: 3it [00:00,  2.99it/s]\n",
      "\u001b[Aluating: 4it [00:01,  3.77it/s]\n",
      "Evaluating: 5it [00:01,  3.92it/s]\n",
      "2025-02-19 03:49:13,517 - __main__ - INFO - Evaluation loss at step 62: 9.68447561264038\n",
      "2025-02-19 03:49:21,523 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2432 | 62%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.1): 45it [00:22,  2.04it/s]\n",
      "2025-02-19 03:49:24,378 - __main__ - INFO - Evaluating at the end of chunk 1...\n",
      "Evaluating: 5it [00:01,  4.07it/s]\n",
      "2025-02-19 03:49:25,609 - __main__ - INFO - Chunk evaluation loss: 9.48110294342041\n",
      "2025-02-19 03:49:25,618 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.15\n",
      "2025-02-19 03:49:31,696 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2432 | 62%          |\n",
      "+------------------------+---------+--------------+\n",
      "2025-02-19 03:49:40,470 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2432 | 62%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.15): 39it [00:17,  2.32it/s]\n",
      "\u001b[Aluating: 0it [00:00, ?it/s]\n",
      "\u001b[Aluating: 1it [00:00,  6.72it/s]\n",
      "\u001b[Aluating: 2it [00:00,  8.00it/s]\n",
      "\u001b[Aluating: 3it [00:00,  6.36it/s]\n",
      "Evaluating: 5it [00:00,  7.46it/s]\n",
      "2025-02-19 03:49:43,743 - __main__ - INFO - Evaluation loss at step 93: 8.691747570037842\n",
      "Training (MLM 0.15): 45it [00:20,  2.23it/s]\n",
      "2025-02-19 03:49:45,766 - __main__ - INFO - Evaluating at the end of chunk 2...\n",
      "Evaluating: 5it [00:00,  8.00it/s]\n",
      "2025-02-19 03:49:46,394 - __main__ - INFO - Chunk evaluation loss: 9.4239107131958\n",
      "2025-02-19 03:49:46,402 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.2\n",
      "2025-02-19 03:49:50,996 - __main__ - INFO - \n",
      "Uso de VRAM pela GPU:\n",
      "+------------------------+---------+--------------+\n",
      "| Descrição              |   Valor | Percentual   |\n",
      "+========================+=========+==============+\n",
      "| Total de Memória (MB)  |    3904 | 100%         |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Alocada (MB)   |    1296 | 33%          |\n",
      "+------------------------+---------+--------------+\n",
      "| Memória Reservada (MB) |    2448 | 63%          |\n",
      "+------------------------+---------+--------------+\n",
      "Training (MLM 0.2): 18it [00:08,  2.12it/s]\n",
      "2025-02-19 03:49:54,907 - __main__ - INFO - Training interrupted. Saving final checkpoint...\n"
     ]
    }
   ],
   "source": [
    "# Treinamento\n",
    "trainer = Trainer(config, model, tokenizer)\n",
    "trainer.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5e555fb-aad9-4018-b554-5e6e922d1b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:09:30.496244Z",
     "iopub.status.busy": "2025-02-19T03:09:30.495717Z",
     "iopub.status.idle": "2025-02-19T03:09:30.500087Z",
     "shell.execute_reply": "2025-02-19T03:09:30.499642Z",
     "shell.execute_reply.started": "2025-02-19T03:09:30.496217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TrainingConfig"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "879523ca-db9a-4f33-86a4-030388f8318c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:07:31.498527Z",
     "iopub.status.busy": "2025-02-19T03:07:31.497965Z",
     "iopub.status.idle": "2025-02-19T03:07:31.502807Z",
     "shell.execute_reply": "2025-02-19T03:07:31.502121Z",
     "shell.execute_reply.started": "2025-02-19T03:07:31.498481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLMTrainingConfig(model=ModelConfig(base_id='neuralmind/bert-base-portuguese-cased', tokenizer_path='domain_tokenizer', output_dir='bert-base-portuguese-cased-ptbr-test'), dataset=DatasetConfig(id='emdemor/news-of-the-brazilian-newspaper', max_news=1234, max_sentences=1001, eval_ratio=0.1), training=TrainingConfig(num_train_epochs=3, train_batch_size=4, gradient_accumulation_steps=2, learning_rate=0.005, weight_decay=0.01, warmup_steps=0, mlm_probabilities=[0.05, 0.1, 0.15, 0.2, 0.3], total_save_limit=2), optimization=OptimizationConfig(use_flash_attention=False, fp16=True, push_interval=10000, num_workers=4), execution=ExecutionConfig(testing=False, seed=42, device='auto'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
