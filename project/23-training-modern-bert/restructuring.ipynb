{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6550706-4255-4f01-8c45-1ddb4779d0f6",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ba40f-2423-413d-ac7e-2145b51f1a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:00:34.911181Z",
     "iopub.status.busy": "2025-02-16T17:00:34.910393Z",
     "iopub.status.idle": "2025-02-16T17:00:38.447885Z",
     "shell.execute_reply": "2025-02-16T17:00:38.447349Z",
     "shell.execute_reply.started": "2025-02-16T17:00:34.911109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset\n",
    "from pydantic import BaseModel, field_validator\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dca242-c4a8-4692-a7ae-9111fb4e6f20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Constantes e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7802d5e0-de1f-413b-bcdc-c0bd96414490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:00:38.448963Z",
     "iopub.status.busy": "2025-02-16T17:00:38.448658Z",
     "iopub.status.idle": "2025-02-16T17:00:38.452152Z",
     "shell.execute_reply": "2025-02-16T17:00:38.451594Z",
     "shell.execute_reply.started": "2025-02-16T17:00:38.448947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ce248b-3d81-48f0-8920-a0d844c691bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:03:44.035857Z",
     "iopub.status.busy": "2025-02-16T17:03:44.033925Z",
     "iopub.status.idle": "2025-02-16T17:03:44.052075Z",
     "shell.execute_reply": "2025-02-16T17:03:44.051664Z",
     "shell.execute_reply.started": "2025-02-16T17:03:44.035760Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Treinar modelo MLM em português\")\n",
    "    \n",
    "    # Argumentos principais\n",
    "    parser.add_argument(\"--model_id\", default=\"answerdotai/ModernBERT-base\",\n",
    "                        help=\"ID do modelo base no Hugging Face Hub\")\n",
    "    parser.add_argument(\"--dataset_id\", default=\"emdemor/news-of-the-brazilian-newspaper\",\n",
    "                        help=\"ID do dataset no Hugging Face Hub\")\n",
    "    parser.add_argument(\"--tokenizer_path\", default=\"domain_tokenizer\",\n",
    "                        help=\"Caminho para o tokenizador\")\n",
    "    parser.add_argument(\"--output_dir\", default=None,\n",
    "                        help=\"Diretório para salvar o modelo treinado\")\n",
    "    \n",
    "    # Configurações de treinamento\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=3,\n",
    "                        help=\"Número de épocas de treinamento\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=4,\n",
    "                        help=\"Tamanho do batch por dispositivo\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2,\n",
    "                        help=\"Passos de acumulação de gradiente\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-3,\n",
    "                        help=\"Taxa de aprendizado inicial\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.01,\n",
    "                        help=\"Peso de decaimento\")\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=0,\n",
    "                        help=\"Passos de aquecimento para o scheduler\")\n",
    "    \n",
    "    # Configurações de dados\n",
    "    parser.add_argument(\"--max_news\", type=int, default=3535,\n",
    "                        help=\"Número máximo de notícias a usar\")\n",
    "    parser.add_argument(\"--max_sentences\", type=int, default=3456,\n",
    "                        help=\"Número máximo de sentenças\")\n",
    "    parser.add_argument(\"--eval_ratio\", type=float, default=0.1,\n",
    "                        help=\"Fração dos dados usada para validação\")\n",
    "    \n",
    "    # Otimizações\n",
    "    parser.add_argument(\"--use_flash_attention\", action=\"store_true\",\n",
    "                        help=\"Usar Flash Attention se disponível\")\n",
    "    parser.add_argument(\"--fp16\", action=\"store_true\",\n",
    "                        help=\"Usar treinamento em precisão mista\")\n",
    "    parser.add_argument(\"--push_interval\", type=int, default=10000,\n",
    "                        help=\"Intervalo de steps para push para o Hub\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4,\n",
    "                        help=\"Número de workers para processamento de dados\")\n",
    "    \n",
    "    # Modo de teste\n",
    "    parser.add_argument(\"--testing\", action=\"store_true\",\n",
    "                        help=\"Executar em modo de teste\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Configurar diretório de saída se não especificado\n",
    "    if args.output_dir is None:\n",
    "        model_name = args.model_id.split(\"/\")[-1]\n",
    "        args.output_dir = f\"{model_name}-ptbr-{'test' if args.testing else 'full'}\"\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a568c-bcc4-495a-9ddf-4b5934126751",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Classes utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e904e71f-adf9-498c-9648-7fa83f8804f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:03:29.579644Z",
     "iopub.status.busy": "2025-02-16T17:03:29.579337Z",
     "iopub.status.idle": "2025-02-16T17:03:29.630079Z",
     "shell.execute_reply": "2025-02-16T17:03:29.629566Z",
     "shell.execute_reply.started": "2025-02-16T17:03:29.579626Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(BaseModel):\n",
    "    \"\"\"Configuração completa para treinamento.\"\"\"\n",
    "    dataset_size: int\n",
    "    num_train_epochs: int\n",
    "    num_chunks: int\n",
    "    train_batch_size_per_device: int\n",
    "    gradient_accumulation_steps: int\n",
    "    eval_size_ratio: float\n",
    "    total_save_limit: int\n",
    "\n",
    "    @field_validator(\"num_chunks\")\n",
    "    def validate_num_chunks(cls, v, info):\n",
    "        data = info.data\n",
    "        if (\n",
    "            \"dataset_size\" in data\n",
    "            and \"dataset_size\" in data\n",
    "            and \"eval_size_ratio\" in data\n",
    "        ):\n",
    "            dataset_size = data[\"dataset_size\"]\n",
    "            eval_size_per_chunk = int(data[\"dataset_size\"] * data[\"eval_size_ratio\"])\n",
    "            available_size = dataset_size - eval_size_per_chunk * v\n",
    "            if available_size < v:\n",
    "                raise ValueError(\n",
    "                    f\"available_size ({available_size}) deve ser maior ou igual a num_chunks ({v})\"\n",
    "                )\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def effective_batch_size(self):\n",
    "        \"\"\"Tamanho efetivo do batch considerando a acumulação de gradiente.\"\"\"\n",
    "        return self.train_batch_size_per_device * self.gradient_accumulation_steps\n",
    "\n",
    "    @property\n",
    "    def total_steps_per_epoch(self):\n",
    "        \"\"\"Total de passos por época.\"\"\"\n",
    "        return math.ceil(self.dataset_size / self.effective_batch_size)\n",
    "\n",
    "    @property\n",
    "    def total_train_steps(self):\n",
    "        \"\"\"Total de passos de treinamento.\"\"\"\n",
    "        return self.total_steps_per_epoch * self.num_train_epochs\n",
    "\n",
    "    @property\n",
    "    def eval_size_per_chunk(self):\n",
    "        \"\"\"Tamanho do dataset de avaliação em cada chunk.\"\"\"\n",
    "        return int(self.dataset_size * self.eval_size_ratio / self.num_chunks)\n",
    "\n",
    "    @property\n",
    "    def available_size(self):\n",
    "        \"\"\"Tamanho disponível para treinamento.\"\"\"\n",
    "        return self.dataset_size - self.eval_size_per_chunk * self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def eval_size(self):\n",
    "        \"\"\"Tamanho total para avaliação.\"\"\"\n",
    "        return self.dataset_size - self.available_size\n",
    "\n",
    "    @property\n",
    "    def chunk_size(self):\n",
    "        \"\"\"Tamanho de cada chunk de dados.\"\"\"\n",
    "        return self.dataset_size // self.num_chunks\n",
    "\n",
    "    @property\n",
    "    def chunk_train_size(self):\n",
    "        \"\"\"Tamanho de treinamento em cada chunk.\"\"\"\n",
    "        return self.available_size // self.num_chunks\n",
    "\n",
    "    def __repr(self):\n",
    "        data = [\n",
    "            [\"num_train_epochs\", self.num_train_epochs],\n",
    "            [\"dataset_size\", self.dataset_size],\n",
    "            [\"num_chunks\", self.num_chunks],\n",
    "            [\"chunk_size\", self.chunk_size],\n",
    "            [\"chunk_train_size\", self.chunk_train_size],\n",
    "            [\"eval_size_per_chunk\", self.eval_size_per_chunk],\n",
    "            [\"eval_size_ratio\", self.eval_size_ratio],\n",
    "            [\"available_size\", self.available_size],\n",
    "            [\"eval_size\", self.eval_size],\n",
    "            [\"train_batch_size_per_device\", self.train_batch_size_per_device],\n",
    "            [\"gradient_accumulation_steps\", self.gradient_accumulation_steps],\n",
    "            [\"total_save_limit\", self.total_save_limit],\n",
    "            [\"effective_batch_size\", self.effective_batch_size],\n",
    "            [\"total_steps_per_epoch\", self.total_steps_per_epoch],\n",
    "            [\"total_train_steps\", self.total_train_steps],\n",
    "        ]\n",
    "\n",
    "        return tabulate.tabulate(data, headers=[\"Attribute\", \"Value\"], tablefmt=\"grid\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__repr()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af160973-dd73-40a5-bb89-1bdf5306778a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:03:30.158811Z",
     "iopub.status.busy": "2025-02-16T17:03:30.157963Z",
     "iopub.status.idle": "2025-02-16T17:03:30.195506Z",
     "shell.execute_reply": "2025-02-16T17:03:30.194728Z",
     "shell.execute_reply.started": "2025-02-16T17:03:30.158736Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide o texto em sentenças.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a ser dividido\n",
    "        \n",
    "    Returns:\n",
    "        Lista de sentenças\n",
    "    \"\"\"\n",
    "    return [\n",
    "        sentence.strip()\n",
    "        for sentence in re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "\n",
    "def set_attention(model, use_flash_attention=False):\n",
    "    \"\"\"\n",
    "    Configura atenção do modelo, possibilitando uso de Flash Attention.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser configurado\n",
    "        use_flash_attention: Se deve usar Flash Attention\n",
    "        \n",
    "    Returns:\n",
    "        Modelo configurado\n",
    "    \"\"\"\n",
    "    if not use_flash_attention:\n",
    "        return model\n",
    "    \n",
    "    def check_flash_attention_support():\n",
    "        if not torch.cuda.is_available():\n",
    "            return False\n",
    "        try:\n",
    "            from flash_attn import flash_attn_qkvpacked_func\n",
    "            qkv = torch.randn(1, 1, 3, 16, 64, dtype=torch.float16, device=\"cuda\")\n",
    "            flash_attn_qkvpacked_func(qkv, causal=False)\n",
    "            return True\n",
    "        except (ImportError, RuntimeError) as e:\n",
    "            logger.warning(f\"Flash Attention não é compatível: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    if check_flash_attention_support():\n",
    "        logger.info(\"Replacing standard attention with FlashAttention...\")\n",
    "        try:\n",
    "            from flash_attn import FlashAttention\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, nn.MultiheadAttention):\n",
    "                    module.attention = FlashAttention()\n",
    "            logger.info(\"FlashAttention integrated successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to integrate FlashAttention: {str(e)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_vocab_size(tokenizer, model):\n",
    "    \"\"\"\n",
    "    Verifica se o tamanho do vocabulário do tokenizador é compatível com o modelo.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: Tokenizador a ser verificado\n",
    "        model: Modelo a ser verificado\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: Se os IDs do token estiverem fora do intervalo do modelo\n",
    "    \"\"\"\n",
    "    max_token_id = max(tokenizer.get_vocab().values())\n",
    "    logger.info(f\"Maior ID no tokenizador: {max_token_id}\")\n",
    "    logger.info(f\"Tamanho do vocabulário do modelo: {model.config.vocab_size}\")\n",
    "    assert max_token_id < model.config.vocab_size, \"IDs de tokens fora do intervalo!\"\n",
    "\n",
    "def tokenize_function(examples, tokenizer, target_column=\"text\"):\n",
    "    \"\"\"\n",
    "    Função para tokenizar exemplos do dataset.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch de exemplos a serem tokenizados\n",
    "        tokenizer: Tokenizador a ser utilizado\n",
    "        target_column: Nome da coluna contendo o texto\n",
    "        \n",
    "    Returns:\n",
    "        Exemplos tokenizados\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples[target_column],\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, num_proc=4):\n",
    "    \"\"\"\n",
    "    Tokeniza o dataset completo usando processamento paralelo.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset a ser tokenizado\n",
    "        tokenizer: Tokenizador a ser utilizado\n",
    "        num_proc: Número de processos para paralelização\n",
    "        \n",
    "    Returns:\n",
    "        Dataset tokenizado\n",
    "    \"\"\"\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: tokenize_function(examples, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        num_proc=num_proc,\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Colator de dados com preenchimento dinâmico para MLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples[\"input_ids\"])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(batch)\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Garante que os tensores de entrada tenham a forma e o tipo corretos.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dicionário com tensores de entrada\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com tensores corrigidos\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: Se algum tensor tiver forma inesperada\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "def forward_pass(model, inputs, device):\n",
    "    \"\"\"\n",
    "    Realiza uma passagem para frente no modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo para realizar a passagem\n",
    "        inputs: Entradas do modelo\n",
    "        device: Dispositivo onde o modelo está\n",
    "        \n",
    "    Returns:\n",
    "        Perda calculada pelo modelo\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: Se o modelo não retornar uma perda\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "def evaluate(model, eval_dataset, data_collator, batch_size, device):\n",
    "    \"\"\"\n",
    "    Avalia o desempenho do modelo no conjunto de validação.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser avaliado\n",
    "        eval_dataset: Dataset de avaliação\n",
    "        data_collator: Colator de dados para preparar batches\n",
    "        batch_size: Tamanho do batch para avaliação\n",
    "        device: Dispositivo onde o modelo está\n",
    "        \n",
    "    Returns:\n",
    "        Perda média de avaliação\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=batch_size)\n",
    "    \n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            \"cuda\", enabled=(device.type == \"cuda\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs, device)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    \n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, tokenizer, global_step, epoch, output_dir):\n",
    "    \"\"\"\n",
    "    Salva um checkpoint do treinamento.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser salvo\n",
    "        optimizer: Otimizador a ser salvo\n",
    "        scheduler: Scheduler a ser salvo\n",
    "        tokenizer: Tokenizador a ser salvo\n",
    "        global_step: Passo global atual\n",
    "        epoch: Época atual\n",
    "        output_dir: Diretório base para salvar\n",
    "    \"\"\"\n",
    "    checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Salvar estado do modelo e otimizador\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'global_step': global_step,\n",
    "    }, os.path.join(checkpoint_dir, \"training_state.pt\"))\n",
    "    \n",
    "    # Salvar modelo e tokenizador no formato HF\n",
    "    model.save_pretrained(checkpoint_dir)\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    logger.info(f\"Saved checkpoint at step {global_step} to {checkpoint_dir}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Carrega um checkpoint de treinamento.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser carregado\n",
    "        optimizer: Otimizador a ser carregado\n",
    "        scheduler: Scheduler a ser carregado\n",
    "        checkpoint_path: Caminho para o checkpoint\n",
    "        \n",
    "    Returns:\n",
    "        Tupla com (modelo, otimizador, scheduler, global_step, epoch)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_path, \"training_state.pt\"))\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    return model, optimizer, scheduler, checkpoint['global_step'], checkpoint['epoch']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cdd39-c6e4-4d8e-b37c-eeb93986f31a",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d818353-e5cf-4568-945a-86343678a9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:10:13.899025Z",
     "iopub.status.busy": "2025-02-16T17:10:13.898591Z",
     "iopub.status.idle": "2025-02-16T17:10:13.904430Z",
     "shell.execute_reply": "2025-02-16T17:10:13.903926Z",
     "shell.execute_reply.started": "2025-02-16T17:10:13.899009Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "# Passo 1: Carregar as configurações do arquivo YAML\n",
    "with open('config.yml', 'r') as file:\n",
    "    args = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e43f3e-cd4e-4745-950f-f58580533423",
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-16T17:27:13.081Z",
     "iopub.execute_input": "2025-02-16T17:26:29.355793Z",
     "iopub.status.busy": "2025-02-16T17:26:29.355606Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:29,367 - __main__ - INFO - Using device: cuda\n",
      "2025-02-16 17:26:29,367 - __main__ - INFO - Loading dataset...\n",
      "2025-02-16 17:26:33,399 - __main__ - INFO - Preparing sentences...\n",
      "2025-02-16 17:26:33,572 - __main__ - INFO - Loading tokenizer and model...\n",
      "2025-02-16 17:26:34,467 - __main__ - INFO - Maior ID no tokenizador: 32767\n",
      "2025-02-16 17:26:34,467 - __main__ - INFO - Tamanho do vocabulário do modelo: 32768\n",
      "2025-02-16 17:26:34,468 - __main__ - INFO - Training configuration:\n",
      "+-----------------------------+---------+\n",
      "| Attribute                   |   Value |\n",
      "+=============================+=========+\n",
      "| num_train_epochs            |     3   |\n",
      "+-----------------------------+---------+\n",
      "| dataset_size                |  3456   |\n",
      "+-----------------------------+---------+\n",
      "| num_chunks                  |     5   |\n",
      "+-----------------------------+---------+\n",
      "| chunk_size                  |   691   |\n",
      "+-----------------------------+---------+\n",
      "| chunk_train_size            |   622   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size_per_chunk         |    69   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size_ratio             |     0.1 |\n",
      "+-----------------------------+---------+\n",
      "| available_size              |  3111   |\n",
      "+-----------------------------+---------+\n",
      "| eval_size                   |   345   |\n",
      "+-----------------------------+---------+\n",
      "| train_batch_size_per_device |     4   |\n",
      "+-----------------------------+---------+\n",
      "| gradient_accumulation_steps |     2   |\n",
      "+-----------------------------+---------+\n",
      "| total_save_limit            |     2   |\n",
      "+-----------------------------+---------+\n",
      "| effective_batch_size        |     8   |\n",
      "+-----------------------------+---------+\n",
      "| total_steps_per_epoch       |   432   |\n",
      "+-----------------------------+---------+\n",
      "| total_train_steps           |  1296   |\n",
      "+-----------------------------+---------+\n",
      "2025-02-16 17:26:34,469 - __main__ - INFO - Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93793d9046674f4bbda91b562d891933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:35,017 - __main__ - INFO - Starting training...\n",
      "2025-02-16 17:26:35,018 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.05\n",
      "2025-02-16 17:26:35,023 - __main__ - INFO - Splitting | chunk: 0-690 | eval: 0-68 | train: 69-690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b6d5f2e174d2b82b6827a66e2dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.05): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:45,108 - __main__ - INFO - Evaluating at the end of chunk 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebed7441b934387ab4a7444b913d22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:45,475 - __main__ - INFO - Chunk 0 evaluation loss: nan\n",
      "2025-02-16 17:26:45,476 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.1\n",
      "2025-02-16 17:26:45,481 - __main__ - INFO - Splitting | chunk: 691-1381 | eval: 691-759 | train: 760-1381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11fb435a2764947883ab082986c7900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.1): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe973c28970b4f0d90e762367d43d41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:49,417 - __main__ - INFO - Evaluation loss at step 108: 8.594348033269247\n",
      "2025-02-16 17:26:55,390 - __main__ - INFO - Evaluating at the end of chunk 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45e52efab484ad98951b651821cbcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:26:55,763 - __main__ - INFO - Chunk 1 evaluation loss: 8.538917594485813\n",
      "2025-02-16 17:26:55,763 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.15\n",
      "2025-02-16 17:26:55,764 - __main__ - INFO - Splitting | chunk: 1382-2072 | eval: 1382-1450 | train: 1451-2072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176284d51d044887a8737f648453e731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.15): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0006a66310e4d518abd5db6047884a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:27:03,221 - __main__ - INFO - Evaluation loss at step 216: 8.113239261839125\n",
      "2025-02-16 17:27:05,304 - __main__ - INFO - Evaluating at the end of chunk 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bc5624279942e797233a8bec66c39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:27:05,668 - __main__ - INFO - Chunk 2 evaluation loss: 8.423120816548666\n",
      "2025-02-16 17:27:05,669 - __main__ - INFO - Epoch 1/3 | MLM Probability: 0.2\n",
      "2025-02-16 17:27:05,669 - __main__ - INFO - Splitting | chunk: 2073-2763 | eval: 2073-2141 | train: 2142-2763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84577ba14c6240668cc2348dd9df9e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:27:11,748 - __main__ - INFO - Training interrupted by user. Saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Setup MLM probabilities\n",
    "mlm_probabilities = [0.05, 0.10, 0.15, 0.20, 0.30]\n",
    "\n",
    "# Load dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "raw_dataset = load_dataset(args[\"dataset\"][\"id\"], split=\"train\")\n",
    "df = raw_dataset.to_pandas().sample(frac=1).reset_index(drop=True)\n",
    "sample_df = df.sample(min(args[\"dataset\"][\"max_news\"], len(df)))\n",
    "\n",
    "logger.info(\"Preparing sentences...\")\n",
    "combined_texts = sample_df[\"text\"].to_list() + sample_df[\"title\"].to_list()\n",
    "sentences = [\n",
    "    phrase for text in combined_texts if text for phrase in split_into_sentences(text)\n",
    "]\n",
    "sentences_sample = pd.Series(sentences).sample(args[\"dataset\"][\"max_sentences\"]).to_list()\n",
    "dataset = Dataset.from_dict({\"text\": sentences_sample})\n",
    "\n",
    "# Setup model and tokenizer\n",
    "logger.info(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args[\"model\"][\"tokenizer_path\"], clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(args[\"model\"][\"base_id\"])\n",
    "\n",
    "if args[\"optimization\"][\"fp16\"]:\n",
    "    config.torch_dtype = torch.float16\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(args[\"model\"][\"base_id\"], config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# Setup attention mechanism\n",
    "model = set_attention(model, args[\"optimization\"][\"use_flash_attention\"])\n",
    "\n",
    "# Check vocabulary compatibility\n",
    "check_vocab_size(tokenizer, model)\n",
    "\n",
    "# Create training config\n",
    "training_config = TrainingConfig(\n",
    "    num_train_epochs=args[\"training\"][\"num_train_epochs\"],\n",
    "    dataset_size=len(dataset),\n",
    "    num_chunks=len(mlm_probabilities),\n",
    "    train_batch_size_per_device=args[\"training\"][\"train_batch_size\"],\n",
    "    gradient_accumulation_steps=args[\"training\"][\"gradient_accumulation_steps\"],\n",
    "    eval_size_ratio=args[\"dataset\"][\"eval_ratio\"],\n",
    "    total_save_limit=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"Training configuration:\\n{training_config}\")\n",
    "\n",
    "# Tokenize dataset\n",
    "logger.info(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset, tokenizer, num_proc=args[\"optimization\"][\"num_workers\"])\n",
    "\n",
    "# Setup optimizer, scheduler and scaler\n",
    "optimizer = AdamW(model.parameters(), lr=args[\"training\"][\"learning_rate\"], weight_decay=args[\"training\"][\"weight_decay\"])\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == \"cuda\" and args[\"optimization\"][\"fp16\"]))\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=args[\"training\"][\"warmup_steps\"],\n",
    "    num_training_steps=training_config.total_train_steps,\n",
    ")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "global_step = 0\n",
    "start_epoch = 0\n",
    "latest_checkpoint = None\n",
    "\n",
    "if os.path.exists(args[\"model\"][\"output_dir\"]):\n",
    "    checkpoints = [d for d in os.listdir(args[\"model\"][\"output_dir\"]) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))[-1]\n",
    "        checkpoint_path = os.path.join(args[\"model\"][\"output_dir\"], latest_checkpoint)\n",
    "        model, optimizer, scheduler, global_step, start_epoch = load_checkpoint(\n",
    "            model, optimizer, scheduler, checkpoint_path\n",
    "        )\n",
    "else:\n",
    "    os.makedirs(args[\"model\"][\"output_dir\"], exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "logger.info(\"Starting training...\")\n",
    "model.train()\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, training_config.num_train_epochs):\n",
    "        for chunk_number, mlm_probability in enumerate(mlm_probabilities):\n",
    "            logger.info(\n",
    "                f\"Epoch {epoch + 1}/{training_config.num_train_epochs} | \"\n",
    "                f\"MLM Probability: {mlm_probability}\"\n",
    "            )\n",
    "            \n",
    "            # Setup data collator with current MLM probability\n",
    "            data_collator = DynamicPaddingDataCollator(\n",
    "                tokenizer, mlm_probability=mlm_probability\n",
    "            )\n",
    "            \n",
    "            # Split dataset for this chunk\n",
    "            eval_start_idx = chunk_number * training_config.chunk_size\n",
    "            eval_end_idx = eval_start_idx + training_config.eval_size_per_chunk - 1\n",
    "            train_start_idx = (\n",
    "                chunk_number * training_config.chunk_size + training_config.eval_size_per_chunk\n",
    "            )\n",
    "            train_end_idx = train_start_idx + training_config.chunk_train_size - 1\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Splitting | \"\n",
    "                f\"chunk: {eval_start_idx}-{train_end_idx} | \"\n",
    "                f\"eval: {eval_start_idx}-{eval_end_idx} | \"\n",
    "                f\"train: {train_start_idx}-{train_end_idx}\"\n",
    "            )\n",
    "            \n",
    "            train_dataset = (\n",
    "                tokenized_dataset.skip(train_start_idx)\n",
    "                .take(training_config.chunk_train_size)\n",
    "                .shuffle(seed=42)\n",
    "            )\n",
    "            \n",
    "            eval_dataset = (\n",
    "                tokenized_dataset.skip(eval_start_idx)\n",
    "                .take(training_config.eval_size_per_chunk)\n",
    "                .shuffle(seed=42)\n",
    "            )\n",
    "            \n",
    "            # Train on this chunk\n",
    "            train_iterator = train_dataset.iter(\n",
    "                batch_size=training_config.train_batch_size_per_device\n",
    "            )\n",
    "            \n",
    "            for step, batch in tqdm(\n",
    "                enumerate(train_iterator), desc=f\"Training (MLM {mlm_probability})\"\n",
    "            ):\n",
    "                # Check if accumulation step is complete\n",
    "                accumulation_step_complete = (\n",
    "                    step + 1\n",
    "                ) % training_config.gradient_accumulation_steps == 0\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    inputs = data_collator(batch)\n",
    "                    loss = forward_pass(model, inputs, device)\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    scaler.scale(\n",
    "                        loss / training_config.gradient_accumulation_steps\n",
    "                    ).backward()\n",
    "                    \n",
    "                    if accumulation_step_complete:\n",
    "                        # Update model parameters\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        # Update global step\n",
    "                        global_step += 1\n",
    "                        \n",
    "                        # Evaluate periodically\n",
    "                        eval_interval = max(1, training_config.total_steps_per_epoch // 4)\n",
    "                        if global_step % eval_interval == 0:\n",
    "                            eval_loss = evaluate(\n",
    "                                model,\n",
    "                                eval_dataset,\n",
    "                                data_collator,\n",
    "                                batch_size=training_config.train_batch_size_per_device,\n",
    "                                device=device\n",
    "                            )\n",
    "                            logger.info(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "                        \n",
    "                        # Save checkpoint periodically\n",
    "                        save_interval = args[\"optimization\"][\"push_interval\"]\n",
    "                        if global_step % save_interval == 0:\n",
    "                            save_checkpoint(\n",
    "                                model, optimizer, scheduler, tokenizer,\n",
    "                                global_step, epoch, args[\"model\"][\"output_dir\"],\n",
    "                            )\n",
    "                        \n",
    "                        # Clear CUDA cache periodically\n",
    "                        if device.type == \"cuda\" and global_step % 100 == 0:\n",
    "                            torch.cuda.empty_cache()\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Training batch failed: {e}. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            # Evaluate at the end of each chunk\n",
    "            logger.info(f\"Evaluating at the end of chunk {chunk_number}...\")\n",
    "            eval_loss = evaluate(\n",
    "                model,\n",
    "                eval_dataset,\n",
    "                data_collator,\n",
    "                batch_size=training_config.train_batch_size_per_device,\n",
    "                device=device\n",
    "            )\n",
    "            logger.info(f\"Chunk {chunk_number} evaluation loss: {eval_loss}\")\n",
    "        \n",
    "        # Save checkpoint at the end of each epoch\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, tokenizer,\n",
    "            global_step, epoch, args[\"model\"][\"output_dir\"],\n",
    "        )\n",
    "    \n",
    "    # Save final model\n",
    "    logger.info(\"Training complete. Saving final model...\")\n",
    "    model.save_pretrained(args[\"model\"][\"output_dir\"],)\n",
    "    tokenizer.save_pretrained(args[\"model\"][\"output_dir\"],)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(\"Training interrupted by user. Saving checkpoint...\")\n",
    "    save_checkpoint(\n",
    "        model, optimizer, scheduler, tokenizer,\n",
    "        global_step, epoch, args[\"model\"][\"output_dir\"],\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Training process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135813ec-67e5-4f56-b55c-d2d8f6607e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
