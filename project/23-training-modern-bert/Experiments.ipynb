{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3170c79f-54ea-47e0-b761-540874d32824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:11:38.725044Z",
     "iopub.status.busy": "2025-02-14T13:11:38.724496Z",
     "iopub.status.idle": "2025-02-14T13:11:38.734649Z",
     "shell.execute_reply": "2025-02-14T13:11:38.734249Z",
     "shell.execute_reply.started": "2025-02-14T13:11:38.725004Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fed1c-702e-40b9-a2cf-745e64663a8b",
   "metadata": {},
   "source": [
    "# Fine Tune ModernBERT\n",
    "\n",
    "## Referências\n",
    "1. Warner, Benjamin, et al. [\"Finally, a Replacement for BERT.\" Hugging Face, 19 Dec. 2024, huggingface.co/blog/modernbert](https://huggingface.co/blog/modernbert).\n",
    "2. Stijn Smits. [\"Fine-tuning ModernBERT on a Dutch Dataset with Custom Tokenizer Training\" GitHub, 14 Fev. 2025, https://github.com/s-smits/modernbert-finetune](https://github.com/s-smits/modernbert-finetune).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae764a-691f-48ab-8551-22c2e0d3f237",
   "metadata": {},
   "source": [
    "## Training a WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902fcad-1b3e-410b-a7f5-580d95083560",
   "metadata": {},
   "source": [
    "Para treinar um novo tokenizador, é preciso seguir os seguintes passos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd3680-a24c-42a0-834d-f11f24901125",
   "metadata": {},
   "source": [
    "A. Configure os parâmetros `DATASET_NAME`, `TOKENIZER_SAVE_PATH`, `VOCAB_SIZE` e `NUM_EXAMPLES_TO_TRAIN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d74889c-1888-4176-b5e0-65f35c531beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:11:38.735706Z",
     "iopub.status.busy": "2025-02-14T13:11:38.735411Z",
     "iopub.status.idle": "2025-02-14T13:11:38.743431Z",
     "shell.execute_reply": "2025-02-14T13:11:38.743078Z",
     "shell.execute_reply.started": "2025-02-14T13:11:38.735687Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = None\n",
    "TOKENIZER_SAVE_PATH = \"domain_tokenizer\"\n",
    "VOCAB_SIZE = 32768\n",
    "NUM_EXAMPLES_TO_TRAIN = 3_634_908\n",
    "MODEL_TYPE = \"bpe\"\n",
    "BATCH_SIZE = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbe011-4aa8-4a93-9c11-acaa88d1b8e3",
   "metadata": {},
   "source": [
    "**Importando o dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5df115-9f96-4a64-ac1a-eac484806187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:10:04.898107Z",
     "iopub.status.busy": "2025-02-14T13:10:04.897810Z",
     "iopub.status.idle": "2025-02-14T13:10:15.906839Z",
     "shell.execute_reply": "2025-02-14T13:10:15.906246Z",
     "shell.execute_reply.started": "2025-02-14T13:10:04.898085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3634908"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "#  baixar arquivo de https://github.com/emdemor/News-of-the-Brazilian-Newspaper/blob/main/data/brazilian-news.parquet\n",
    "df = pd.read_parquet(\"../data/brazilian-news.parquet\")\n",
    "\n",
    "temp = df.sample(min(NUM_EXAMPLES_TO_TRAIN, len(df)))\n",
    "texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "\n",
    "\n",
    "\n",
    "def dividir_em_frases(texto):\n",
    "    frases = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "    return [frase.strip() for frase in frases if frase.strip()]\n",
    "\n",
    "texts = []\n",
    "for string in temp[\"text\"].to_list() + temp[\"title\"].to_list():\n",
    "    if string:\n",
    "        frases = dividir_em_frases(string)\n",
    "        texts.extend(frases)\n",
    "\n",
    "texts = list(set(texts))\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4f7ae5-cae7-48e8-bce1-f6e97fd16f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T12:33:09.583490Z",
     "iopub.status.busy": "2025-02-14T12:33:09.583218Z",
     "iopub.status.idle": "2025-02-14T12:33:35.999947Z",
     "shell.execute_reply": "2025-02-14T12:33:35.999337Z",
     "shell.execute_reply.started": "2025-02-14T12:33:09.583474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinando tokenizer: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3634908/3634908 [00:17<00:00, 210689.46exemplos/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to domain_tokenizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "    os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.bert.tokenization_bert import BertTokenizer, BasicTokenizer\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "dataset_iterator = iter(dataset)\n",
    "\n",
    "# Cria o tokenizer com o modelo WordPiece\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.normalizer = normalizers.Sequence([])\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    min_frequency=2,\n",
    ")\n",
    "\n",
    "\n",
    "def batch_iterator(batch_size=BATCH_SIZE):\n",
    "    total_batches = (NUM_EXAMPLES_TO_TRAIN + batch_size - 1) // batch_size\n",
    "    from tqdm import (\n",
    "        tqdm,  # Certifique-se de importar o tqdm se ainda não estiver importado\n",
    "    )\n",
    "\n",
    "    with tqdm(\n",
    "        total=NUM_EXAMPLES_TO_TRAIN, desc=\"Treinando tokenizer\", unit=\"exemplos\"\n",
    "    ) as pbar:\n",
    "        for i in range(0, NUM_EXAMPLES_TO_TRAIN, batch_size):\n",
    "            batch_texts = dataset[i : i + batch_size][\"text\"]\n",
    "            pbar.update(len(batch_texts))\n",
    "            yield batch_texts\n",
    "\n",
    "\n",
    "# Treina o tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    batch_iterator(), trainer=trainer, length=NUM_EXAMPLES_TO_TRAIN\n",
    ")\n",
    "\n",
    "# Cria o diretório se não existir e salva o tokenizer\n",
    "os.makedirs(TOKENIZER_SAVE_PATH, exist_ok=True)\n",
    "tokenizer_file = os.path.join(TOKENIZER_SAVE_PATH, \"tokenizer.json\")\n",
    "tokenizer.save(tokenizer_file)\n",
    "print(f\"Tokenizer trained and saved to {TOKENIZER_SAVE_PATH}\")\n",
    "\n",
    "# Cria automaticamente o arquivo config.json se não existir, informando o model_type\n",
    "config_path = os.path.join(TOKENIZER_SAVE_PATH, \"config.json\")\n",
    "if not os.path.exists(config_path):\n",
    "    config = {\"model_type\": \"bert\"}\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4feea-6912-4241-a6d2-cf72da42ad86",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-14T12:32:47.873862Z",
     "iopub.status.idle": "2025-02-14T12:32:47.874027Z",
     "shell.execute_reply": "2025-02-14T12:32:47.873952Z",
     "shell.execute_reply.started": "2025-02-14T12:32:47.873943Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Agora o AutoTokenizer conseguirá carregar o tokenizer corretamente\n",
    "# tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb10adc-3f40-490d-9406-a3e8873456e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T04:03:27.802597Z",
     "iopub.status.busy": "2025-02-14T04:03:27.802362Z",
     "iopub.status.idle": "2025-02-14T04:03:27.805868Z",
     "shell.execute_reply": "2025-02-14T04:03:27.805314Z",
     "shell.execute_reply.started": "2025-02-14T04:03:27.802581Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "#     os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "# import json\n",
    "# from itertools import islice\n",
    "\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import WordPiece\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "# from tokenizers.trainers import WordPieceTrainer\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# #  baixar arquivo de https://github.com/emdemor/News-of-the-Brazilian-Newspaper/blob/main/data/brazilian-news.parquet\n",
    "# df = pd.read_parquet(\"../data/brazilian-news.parquet\")\n",
    "\n",
    "# temp = df.sample(min(NUM_EXAMPLES_TO_TRAIN, len(df)))\n",
    "# texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "# texts = [x[:MAX_CHAR_LENGTH] for x in texts if x]\n",
    "\n",
    "# dataset = Dataset.from_dict({\"text\": texts})\n",
    "# dataset_iterator = iter(dataset)\n",
    "\n",
    "# tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer = WordPieceTrainer(\n",
    "#     vocab_size=VOCAB_SIZE,\n",
    "#     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "#     min_frequency=2,\n",
    "# )\n",
    "\n",
    "\n",
    "# def batch_iterator(batch_size=BATCH_SIZE):\n",
    "#     total_batches = (NUM_EXAMPLES_TO_TRAIN + batch_size - 1) // batch_size\n",
    "#     with tqdm(\n",
    "#         total=NUM_EXAMPLES_TO_TRAIN, desc=\"Treinando tokenizer\", unit=\"exemplos\"\n",
    "#     ) as pbar:\n",
    "#         for i in range(0, NUM_EXAMPLES_TO_TRAIN, batch_size):\n",
    "#             batch_texts = dataset[i : i + batch_size][\"text\"]\n",
    "#             pbar.update(len(batch_texts))\n",
    "#             yield batch_texts\n",
    "\n",
    "\n",
    "# tokenizer.train_from_iterator(\n",
    "#     batch_iterator(), trainer=trainer, length=NUM_EXAMPLES_TO_TRAIN\n",
    "# )\n",
    "# os.makedirs(TOKENIZER_SAVE_PATH, exist_ok=True)\n",
    "# tokenizer.save(os.path.join(TOKENIZER_SAVE_PATH, \"tokenizer.json\"))\n",
    "# print(f\"Tokenizer trained and saved to {TOKENIZER_SAVE_PATH}\")\n",
    "\n",
    "# # Cria automaticamente o arquivo config.json se não existir\n",
    "# config_path = os.path.join(TOKENIZER_SAVE_PATH, \"config.json\")\n",
    "# if not os.path.exists(config_path):\n",
    "#     config = {\"model_type\": \"bert\"}  # Altere para o tipo adequado se necessário\n",
    "#     with open(config_path, \"w\") as f:\n",
    "#         json.dump(config, f)\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7de81-e67e-453c-9cdf-5a9672bf17d4",
   "metadata": {},
   "source": [
    "## Fine-tuning the ModernBERT-base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c776201-40c6-4f5b-8bd1-b6e6c0a06e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T04:30:51.155074Z",
     "iopub.status.busy": "2025-02-14T04:30:51.153879Z",
     "iopub.status.idle": "2025-02-14T04:31:14.859828Z",
     "shell.execute_reply": "2025-02-14T04:31:14.858556Z",
     "shell.execute_reply.started": "2025-02-14T04:30:51.155018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-llqjm_pk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-llqjm_pk\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 336dc69d63d56f232a183a3e7f52790429b871ef\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.49.0.dev0)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0.dev0)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.49.0.dev0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0.dev0) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.49.0.dev0) (2024.2.2)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.49.0.dev0-py3-none-any.whl size=10758753 sha256=b244d8a44ed235cdb79e729b846418d8ff74965b409e23c51743a46c531af88b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p8_21gox/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed huggingface-hub-0.28.1 tokenizers-0.21.0 transformers-4.49.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torchvision -y\n",
    "# !pip install torchvision==0.18.0 -f https://download.pytorch.org/whl/torch_stable.html -qqq\n",
    "# !pip install --upgrade 'optree>=0.13.0' -qqq\n",
    "# !pip install -U torch torch-adopt torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install -U torch-adopt -qqq\n",
    "\n",
    "# !pip uninstall transformers -y\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af1fee5-a158-4e30-ab9e-111390cfcfe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:11:45.175631Z",
     "iopub.status.busy": "2025-02-14T13:11:45.175452Z",
     "iopub.status.idle": "2025-02-14T13:11:59.371101Z",
     "shell.execute_reply": "2025-02-14T13:11:59.370540Z",
     "shell.execute_reply.started": "2025-02-14T13:11:45.175616Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "#  baixar arquivo de https://github.com/emdemor/News-of-the-Brazilian-Newspaper/blob/main/data/brazilian-news.parquet\n",
    "df = pd.read_parquet(\"../data/brazilian-news.parquet\")\n",
    "\n",
    "temp = df.sample(min(NUM_EXAMPLES_TO_TRAIN, len(df)))\n",
    "texts = temp[\"text\"].to_list() + temp[\"title\"].to_list()\n",
    "\n",
    "\n",
    "\n",
    "def dividir_em_frases(texto):\n",
    "    frases = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "    return [frase.strip() for frase in frases if frase.strip()]\n",
    "\n",
    "texts = []\n",
    "for string in temp[\"text\"].to_list() + temp[\"title\"].to_list():\n",
    "    if string:\n",
    "        frases = dividir_em_frases(string)\n",
    "        texts.extend(frases)\n",
    "\n",
    "texts = list(set(texts))\n",
    "len(texts)\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts})\n",
    "dataset_iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381abfb9-7fe2-4204-b0a7-4be77f7d20de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:02.132273Z",
     "iopub.status.busy": "2025-02-14T13:12:02.131565Z",
     "iopub.status.idle": "2025-02-14T13:12:05.193912Z",
     "shell.execute_reply": "2025-02-14T13:12:05.193365Z",
     "shell.execute_reply.started": "2025-02-14T13:12:02.132236Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"TRANSFORMERS_CACHE\"):\n",
    "    os.environ[\"HF_HOME\"] = os.environ.pop(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "\n",
    "import math\n",
    "import shutil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import Repository, whoami\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd7718f-2451-4656-b940-2fd1dab13a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:05.194973Z",
     "iopub.status.busy": "2025-02-14T13:12:05.194674Z",
     "iopub.status.idle": "2025-02-14T13:12:05.198712Z",
     "shell.execute_reply": "2025-02-14T13:12:05.198138Z",
     "shell.execute_reply.started": "2025-02-14T13:12:05.194957Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_checkpoint = \"answerdotai/ModernBERT-base\"\n",
    "username = \"emdemor\"\n",
    "tokenizer_path = \"domain_tokenizer\"  # Path to custom tokenizer directory\n",
    "\n",
    "# --- Dataset size (in rows) ---\n",
    "estimated_dataset_size_in_rows = 3_500_000\n",
    "\n",
    "# --- Training Config ---\n",
    "num_train_epochs = 1\n",
    "# Reduce or remove chunk size to allow for dynamic batching\n",
    "chunk_size = None  # Remove chunk size\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "eval_size_ratio = 0.05\n",
    "total_save_limit = 2\n",
    "\n",
    "effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "total_steps_per_epoch = math.ceil(estimated_dataset_size_in_rows / effective_batch_size)\n",
    "total_train_steps = total_steps_per_epoch * num_train_epochs\n",
    "eval_size_per_chunk = int(estimated_dataset_size_in_rows * eval_size_ratio)\n",
    "\n",
    "# --- Testing Mode ---\n",
    "TESTING = True  # Set to True for testing, False for full training\n",
    "FLASH_ATTENTION = True\n",
    "\n",
    "if TESTING:\n",
    "    push_interval = 10_000\n",
    "else:\n",
    "    push_interval = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c6c718-857e-4079-9b6d-8a23e7cd6e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:05.199452Z",
     "iopub.status.busy": "2025-02-14T13:12:05.199265Z",
     "iopub.status.idle": "2025-02-14T13:12:05.212858Z",
     "shell.execute_reply": "2025-02-14T13:12:05.212343Z",
     "shell.execute_reply.started": "2025-02-14T13:12:05.199433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is already installed.\n"
     ]
    }
   ],
   "source": [
    "if FLASH_ATTENTION:\n",
    "    try:\n",
    "        import flash_attn\n",
    "\n",
    "        print(\"FlashAttention is already installed.\")\n",
    "    except ImportError:\n",
    "        print(\"FlashAttention is not installed. Installing...\")\n",
    "        try:\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run(\n",
    "                [\"pip\", \"install\", \"flash-attn\", \"--no-build-isolation\"], check=True\n",
    "            )\n",
    "            import flash_attn\n",
    "\n",
    "            print(\"FlashAttention installed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error installing FlashAttention: {e}\")\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ec0864-16f5-41d4-9a83-5f4dc158b081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:05.214009Z",
     "iopub.status.busy": "2025-02-14T13:12:05.213824Z",
     "iopub.status.idle": "2025-02-14T13:12:05.216597Z",
     "shell.execute_reply": "2025-02-14T13:12:05.216208Z",
     "shell.execute_reply.started": "2025-02-14T13:12:05.213993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not available. Using standard attention.\n"
     ]
    }
   ],
   "source": [
    "# --- Flash-attn Integration Check ---\n",
    "try:\n",
    "    from flash_attn.flash_attention import FlashAttention\n",
    "\n",
    "    print(\"FlashAttention is available.\")\n",
    "    flash_attn_available = True\n",
    "except ImportError:\n",
    "    print(\"FlashAttention is not available. Using standard attention.\")\n",
    "    flash_attn_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488edb1a-c38d-4e58-b953-e7767eaff8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:05.217544Z",
     "iopub.status.busy": "2025-02-14T13:12:05.217227Z",
     "iopub.status.idle": "2025-02-14T13:12:08.636340Z",
     "shell.execute_reply": "2025-02-14T13:12:08.635904Z",
     "shell.execute_reply.started": "2025-02-14T13:12:05.217529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from answerdotai/ModernBERT-base...\n",
      "Loading custom tokenizer from domain_tokenizer...\n",
      "Loading model config from answerdotai/ModernBERT-base...\n",
      "Model config loaded and modified: ModernBertConfig {\n",
      "  \"_name_or_path\": \"answerdotai/ModernBERT-base\",\n",
      "  \"architectures\": [\n",
      "    \"ModernBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 50281,\n",
      "  \"classifier_activation\": \"gelu\",\n",
      "  \"classifier_bias\": false,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"classifier_pooling\": \"mean\",\n",
      "  \"cls_token_id\": 50281,\n",
      "  \"decoder_bias\": true,\n",
      "  \"deterministic_flash_attn\": false,\n",
      "  \"embedding_dropout\": 0.0,\n",
      "  \"eos_token_id\": 50282,\n",
      "  \"global_attn_every_n_layers\": 3,\n",
      "  \"global_rope_theta\": 160000.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_activation\": \"gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_cutoff_factor\": 2.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1152,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"local_attention\": 128,\n",
      "  \"local_rope_theta\": 10000.0,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"mlp_dropout\": 0.0,\n",
      "  \"model_type\": \"modernbert\",\n",
      "  \"norm_bias\": false,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"pad_token_id\": 50283,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"reference_compile\": null,\n",
      "  \"repad_logits_with_grad\": false,\n",
      "  \"sep_token_id\": 50282,\n",
      "  \"sparse_pred_ignore_index\": -100,\n",
      "  \"sparse_prediction\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0.dev0\",\n",
      "  \"vocab_size\": 50368\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Tokenizer and Model ---\n",
    "print(f\"Loading model and tokenizer from {model_checkpoint}...\")\n",
    "\n",
    "# Check if custom tokenizer exists, otherwise use default\n",
    "if os.path.exists(tokenizer_path) and any(\n",
    "    fname.startswith(\"spm\") for fname in os.listdir(tokenizer_path)\n",
    "):\n",
    "    print(f\"Loading custom SentencePiece tokenizer from {tokenizer_path}...\")\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    # Add the pad_token if it's not already in the tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "elif os.path.exists(tokenizer_path) and os.path.isfile(\n",
    "    os.path.join(tokenizer_path, \"tokenizer.json\")\n",
    "):\n",
    "    print(f\"Loading custom tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    print(f\"Using default tokenizer from {model_checkpoint}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_checkpoint, use_auth_token=huggingface_token\n",
    "    )\n",
    "\n",
    "print(f\"Loading model config from {model_checkpoint}...\")\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_checkpoint,  # use_auth_token=huggingface_token\n",
    ")\n",
    "config.torch_dtype = \"float16\"\n",
    "print(f\"Model config loaded and modified: {config}\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config,  # use_auth_token=huggingface_token\n",
    ")\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a802cbc-0ee1-4cf0-93b8-ef2028b09706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:08.637312Z",
     "iopub.status.busy": "2025-02-14T13:12:08.636972Z",
     "iopub.status.idle": "2025-02-14T13:12:08.641132Z",
     "shell.execute_reply": "2025-02-14T13:12:08.640578Z",
     "shell.execute_reply.started": "2025-02-14T13:12:08.637294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing standard attention with FlashAttention...\n",
      "FlashAttention integrated.\n"
     ]
    }
   ],
   "source": [
    "flash_attn_available = True\n",
    "\n",
    "# --- Integrate Flash-attn (if available) ---\n",
    "if flash_attn_available:\n",
    "    print(\"Replacing standard attention with FlashAttention...\")\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.MultiheadAttention):\n",
    "            module.attention = FlashAttention()\n",
    "    print(\"FlashAttention integrated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123af378-6178-40d4-93f6-2a02be9507d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:08.642589Z",
     "iopub.status.busy": "2025-02-14T13:12:08.642064Z",
     "iopub.status.idle": "2025-02-14T13:12:08.650241Z",
     "shell.execute_reply": "2025-02-14T13:12:08.649677Z",
     "shell.execute_reply.started": "2025-02-14T13:12:08.642546Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Tokenization Function ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        # No truncation and max_length to allow dynamic padding truncation=True, max_length=chunk_size, padding=\"longest\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb89ae4-6e3d-4413-a134-4611e7daffd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:12:08.651129Z",
     "iopub.status.busy": "2025-02-14T13:12:08.650899Z",
     "iopub.status.idle": "2025-02-14T13:13:51.385121Z",
     "shell.execute_reply": "2025-02-14T13:13:51.384687Z",
     "shell.execute_reply.started": "2025-02-14T13:12:08.651106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0306e6464271484b8202696e1691a17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3634908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized.\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenize Dataset ---\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "print(\"Dataset tokenized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d384c19-d02a-43aa-8b45-751c9a7e5efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.385831Z",
     "iopub.status.busy": "2025-02-14T13:13:51.385682Z",
     "iopub.status.idle": "2025-02-14T13:13:51.388328Z",
     "shell.execute_reply": "2025-02-14T13:13:51.387839Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.385817Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = f\"{model_name}-ptbr-{'test' if TESTING else 'full'}\"\n",
    "repo_name = f\"{username}/{output_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9becb470-2dec-4637-ad30-dd7197bb4868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.390128Z",
     "iopub.status.busy": "2025-02-14T13:13:51.389735Z",
     "iopub.status.idle": "2025-02-14T13:13:51.454325Z",
     "shell.execute_reply": "2025-02-14T13:13:51.453855Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.390109Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355d018f-ab98-4afd-a1e9-3a9200dac0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.454991Z",
     "iopub.status.busy": "2025-02-14T13:13:51.454852Z",
     "iopub.status.idle": "2025-02-14T13:13:51.459359Z",
     "shell.execute_reply": "2025-02-14T13:13:51.458625Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.454978Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# --- Optimizer and Scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=total_train_steps\n",
    ")\n",
    "\n",
    "# --- AMP scaler for mixed precision ---\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7431ef5c-edf8-401c-9d25-5433687b62b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.461433Z",
     "iopub.status.busy": "2025-02-14T13:13:51.460715Z",
     "iopub.status.idle": "2025-02-14T13:13:51.467982Z",
     "shell.execute_reply": "2025-02-14T13:13:51.467515Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.461385Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper Function to Fix Batch Inputs ---\n",
    "def fix_batch_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Ensures that input tensors have the correct shape and dtype.\n",
    "    - Removes any extra dimensions (e.g., [1, batch, seq_len] -> [batch, seq_len]).\n",
    "    - Casts input_ids to torch.long.\n",
    "    \"\"\"\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "        if key in inputs:\n",
    "            if inputs[key].dim() == 3 and inputs[key].shape[0] == 1:\n",
    "                inputs[key] = inputs[key].squeeze(0)\n",
    "            elif inputs[key].dim() > 2:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected tensor shape for {key}: {inputs[key].shape}\"\n",
    "                )\n",
    "    if \"input_ids\" in inputs and inputs[\"input_ids\"].dtype != torch.long:\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].long()\n",
    "    return inputs\n",
    "\n",
    "# --- Forward Pass Function ---\n",
    "def forward_pass(model, inputs):\n",
    "    \"\"\"\n",
    "    Performs a forward pass with autocast for FP16.\n",
    "    Returns the loss.\n",
    "    \"\"\"\n",
    "    inputs = fix_batch_inputs(inputs)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "    if outputs.loss is None:\n",
    "        raise ValueError(\"Model did not return a loss.\")\n",
    "    return outputs.loss\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(model, eval_dataset, data_collator):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the evaluation dataset.\n",
    "    Returns the average loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_iterator = eval_dataset.iter(batch_size=per_device_train_batch_size)\n",
    "    for batch in tqdm(eval_iterator, desc=\"Evaluating\"):\n",
    "        with torch.no_grad(), torch.amp.autocast('cuda',\n",
    "            enabled=(device.type == \"cuda\")\n",
    "        ):\n",
    "            inputs = data_collator(batch)\n",
    "            try:\n",
    "                loss = forward_pass(model, inputs)\n",
    "                losses.append(loss.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Evaluation batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "    model.train()\n",
    "    average_loss = sum(losses) / len(losses) if losses else float(\"inf\")\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f03f21b0-147d-4a8d-98a1-e7b87c20fc1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.468771Z",
     "iopub.status.busy": "2025-02-14T13:13:51.468561Z",
     "iopub.status.idle": "2025-02-14T13:13:51.476216Z",
     "shell.execute_reply": "2025-02-14T13:13:51.475102Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.468751Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicPaddingDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs for language modeling.\n",
    "    This ensures that all sequences within a batch have the same length,\n",
    "    but the overall length can vary between batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        # Find the maximum length within the current batch\n",
    "        max_length = max(len(input_ids) for input_ids in examples['input_ids'])\n",
    "\n",
    "        # Pad or truncate each example to the max_length\n",
    "        batch = []\n",
    "        input_ids = examples[\"input_ids\"]\n",
    "        attention_mask = examples[\"attention_mask\"]\n",
    "\n",
    "        for ids, mask in zip(input_ids, attention_mask):\n",
    "            padding_length = max_length - len(ids)\n",
    "            if padding_length > 0:\n",
    "                # Pad\n",
    "                ids = torch.tensor(ids + [self.tokenizer.pad_token_id] * padding_length)\n",
    "                mask = torch.tensor(mask + [0] * padding_length)\n",
    "            elif padding_length <= 0:\n",
    "                # Truncate (if enabled in your tokenizer)\n",
    "                ids = torch.tensor(ids[:max_length])\n",
    "                mask = torch.tensor(mask[:max_length])\n",
    "\n",
    "            batch.append({\"input_ids\": ids, \"attention_mask\": mask})\n",
    "\n",
    "        # Apply the rest of the data collation logic (MLM masking, etc.)\n",
    "        batch = self.torch_call(batch)  # Use torch_call instead of __call__ to call the parent's method\n",
    "\n",
    "        # Ensure correct shapes and dtypes\n",
    "        batch = fix_batch_inputs(batch)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc45909-0643-450f-b63b-b98c2854556e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.477939Z",
     "iopub.status.busy": "2025-02-14T13:13:51.477399Z",
     "iopub.status.idle": "2025-02-14T13:13:51.482816Z",
     "shell.execute_reply": "2025-02-14T13:13:51.482379Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.477894Z"
    }
   },
   "outputs": [],
   "source": [
    "mlm_probabilities = [0.3, 0.2, 0.18, 0.16, 0.14]\n",
    "\n",
    "chunk_size_dataset = len(dataset) // len(mlm_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c51411e-67f9-4cde-90d5-4b5d7bbee7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.483616Z",
     "iopub.status.busy": "2025-02-14T13:13:51.483336Z",
     "iopub.status.idle": "2025-02-14T13:13:51.489110Z",
     "shell.execute_reply": "2025-02-14T13:13:51.488098Z",
     "shell.execute_reply.started": "2025-02-14T13:13:51.483601Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5303c51-bf87-4f3d-9b33-ecbffc22104b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T13:13:51.490454Z",
     "iopub.status.busy": "2025-02-14T13:13:51.490129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1, MLM Probability: 0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7279b114c7491f8f5558e9a1d5d80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.3): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving and pushing model at step 10000...\n",
      "Model saved and pushed at step 10000.\n",
      "Saving and pushing model at step 20000...\n",
      "Model saved and pushed at step 20000.\n",
      "Saving and pushing model at step 30000...\n",
      "Model saved and pushed at step 30000.\n",
      "Saving and pushing model at step 40000...\n",
      "Model saved and pushed at step 40000.\n",
      "Saving and pushing model at step 50000...\n",
      "Model saved and pushed at step 50000.\n",
      "Saving and pushing model at step 60000...\n",
      "Model saved and pushed at step 60000.\n",
      "Saving and pushing model at step 70000...\n",
      "Model saved and pushed at step 70000.\n",
      "Saving and pushing model at step 80000...\n",
      "Model saved and pushed at step 80000.\n",
      "Saving and pushing model at step 90000...\n",
      "Model saved and pushed at step 90000.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bb7a7816184d30bc710773563644ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.2): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving and pushing model at step 100000...\n",
      "Model saved and pushed at step 100000.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4d38ab7268446286af6f8096db36f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 109375: 3.5834869703536376\n",
      "Saving and pushing model at step 110000...\n",
      "Model saved and pushed at step 110000.\n",
      "Saving and pushing model at step 120000...\n",
      "Model saved and pushed at step 120000.\n",
      "Saving and pushing model at step 130000...\n",
      "Model saved and pushed at step 130000.\n",
      "Saving and pushing model at step 140000...\n",
      "Model saved and pushed at step 140000.\n",
      "Saving and pushing model at step 150000...\n",
      "Model saved and pushed at step 150000.\n",
      "Saving and pushing model at step 160000...\n",
      "Model saved and pushed at step 160000.\n",
      "Saving and pushing model at step 170000...\n",
      "Model saved and pushed at step 170000.\n",
      "Saving and pushing model at step 180000...\n",
      "Model saved and pushed at step 180000.\n",
      "\n",
      "Epoch 1/1, MLM Probability: 0.18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725efe3717b48049af1dc685facfba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (MLM 0.18): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving and pushing model at step 190000...\n",
      "Model saved and pushed at step 190000.\n",
      "Saving and pushing model at step 200000...\n",
      "Model saved and pushed at step 200000.\n",
      "Saving and pushing model at step 210000...\n",
      "Model saved and pushed at step 210000.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f1240907b64bf7ae324fc4d1cb31ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss at step 218750: 3.2397417649450686\n",
      "Saving and pushing model at step 220000...\n",
      "Model saved and pushed at step 220000.\n",
      "Saving and pushing model at step 230000...\n",
      "Model saved and pushed at step 230000.\n",
      "Saving and pushing model at step 240000...\n",
      "Model saved and pushed at step 240000.\n",
      "Saving and pushing model at step 250000...\n",
      "Model saved and pushed at step 250000.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    for i, mlm_probability in enumerate(mlm_probabilities):\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{num_train_epochs}, MLM Probability: {mlm_probability}\"\n",
    "        )\n",
    "\n",
    "        data_collator = DynamicPaddingDataCollator(\n",
    "            tokenizer=tokenizer, mlm_probability=mlm_probability\n",
    "        )\n",
    "\n",
    "        train_dataset = (\n",
    "            tokenized_dataset.skip(\n",
    "                i * chunk_size_dataset + eval_size_per_chunk\n",
    "            )\n",
    "            .take(chunk_size_dataset)\n",
    "            .shuffle(seed=42)\n",
    "        )\n",
    "        eval_dataset = tokenized_dataset.skip(i * chunk_size_dataset).take(\n",
    "            eval_size_per_chunk\n",
    "        )\n",
    "\n",
    "        train_iterator = train_dataset.iter(batch_size=per_device_train_batch_size)\n",
    "        for step, batch in enumerate(\n",
    "            tqdm(train_iterator, desc=f\"Training (MLM {mlm_probability})\")\n",
    "        ):\n",
    "            try:\n",
    "                inputs = data_collator(batch)\n",
    "                loss = forward_pass(model, inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Training batch failed: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss / gradient_accumulation_steps).backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()  # Clear cache\n",
    "                global_step += 1\n",
    "\n",
    "\n",
    "                # Evaluation\n",
    "                eval_interval = total_steps_per_epoch // (num_train_epochs * 4)\n",
    "                if eval_interval > 0 and (global_step % eval_interval == 0):\n",
    "                    eval_loss = evaluate(model, eval_dataset, data_collator)\n",
    "                    print(f\"Evaluation loss at step {global_step}: {eval_loss}\")\n",
    "\n",
    "                # Push to hub incl TESTING\n",
    "                if global_step % push_interval == 0:\n",
    "                    print(f\"Saving and pushing model at step {global_step}...\")\n",
    "                    model.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "                    print(f\"Model saved and pushed at step {global_step}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70f625-906d-49f8-8a56-e52f93cdaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Save and Push\n",
    "print(\"\\nSaving and pushing final model...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Final model saved and pushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2180404-7da9-4ca3-9b06-0e0da87b9eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c7f07-d4f7-4b73-9a43-75cb11dca8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb004ae9-f26f-4c6b-831b-178bbce6a14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edbf6eb-bab9-4303-a355-b089dfab846e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
