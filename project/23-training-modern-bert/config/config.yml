# Configuração para Treinamento de Modelo de Linguagem Mascarada (MLM)

# Identificadores e caminhos
model:
  base_id: "neuralmind/bert-base-portuguese-cased" #"answerdotai/ModernBERT-base"  # ID do modelo base no Hugging Face Hub
  tokenizer_path: "models/domain_tokenizer"       # Caminho para o tokenizador
  output_dir: "models/bert-base-portuguese-cased-ptbr-test"  # Diretório para salvar o modelo treinado

dataset:
  id: "emdemor/news-of-the-brazilian-newspaper"  # ID do dataset no HF Hub
  max_news: null            # Número máximo de notícias a usar
  max_sentences: 1000       # Número máximo de sentenças
  eval_ratio: 0.10           # Fração dos dados usada para validação

# Parâmetros de treinamento
training:
  num_train_epochs: 10                # Número de épocas de treinamento
  train_batch_size: 4                # Tamanho do batch por dispositivo
  gradient_accumulation_steps: 2     # Passos de acumulação de gradiente
  learning_rate: 1.0e-4              # Taxa de aprendizado inicial
  weight_decay: 0.01                 # Peso de decaimento
  warmup_steps: 0                    # Passos de aquecimento para o scheduler
  mlm_probabilities:                 # Probabilidades de mascaramento para diferentes chunks
    - 0.05
    - 0.10
    - 0.15
    - 0.20
    - 0.30
  total_save_limit: 2            # Número máximo de checkpoints a manter
  fp16: true                     # Usar treinamento em precisão mista
  use_flash_attention: false     # Usar Flash Attention se disponível
  push_interval: 1000           # Intervalo de steps para push para o Hub
  num_workers: 4                 # Número de workers para processamento de dados
  clear_memory_steps: 50         # Número de passos para limpar a memoria
  memory_log_steps: 50           # Númreo de passos para retornar um log de uso da memória

# Modo de execução
execution:
  testing: false                 # Executar em modo de teste
  seed: 42                       # Semente para reprodutibilidade
  device: "auto"                 # "auto", "cuda", ou "cpu"