{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T22:25:41.507851Z",
     "iopub.status.busy": "2024-12-22T22:25:41.507629Z",
     "iopub.status.idle": "2024-12-22T22:25:41.859699Z",
     "shell.execute_reply": "2024-12-22T22:25:41.859241Z",
     "shell.execute_reply.started": "2024-12-22T22:25:41.507836Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:25:42.972233Z",
     "iopub.status.busy": "2024-12-22T22:25:42.971437Z",
     "iopub.status.idle": "2024-12-22T22:25:43.303292Z",
     "shell.execute_reply": "2024-12-22T22:25:43.302721Z",
     "shell.execute_reply.started": "2024-12-22T22:25:42.972159Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_secrets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m UserSecretsClient()\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE-TOKEN-TO-UPLOAD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = UserSecretsClient().get_secret(\"HUGGINGFACE-TOKEN-TO-UPLOAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:00:45.587147Z",
     "iopub.status.busy": "2024-08-19T14:00:45.586879Z",
     "iopub.status.idle": "2024-08-19T14:02:11.032468Z",
     "shell.execute_reply": "2024-08-19T14:02:11.031544Z",
     "shell.execute_reply.started": "2024-08-19T14:00:45.587124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> bitsandbytes\n",
      ">>> peft\n",
      ">>> transformers\n",
      ">>> trl\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m>>> accelerate\n",
      ">>> datasets\n"
     ]
    }
   ],
   "source": [
    "def install_lib(libname):\n",
    "    print(f\">>> {libname}\")\n",
    "    get_ipython().system(f\"pip install -qqq {libname}\")\n",
    "\n",
    "libs = [\n",
    "    \"bitsandbytes\",\n",
    "    \"peft\",\n",
    "    \"transformers\",\n",
    "    \"trl\",\n",
    "    \"accelerate\",\n",
    "    \"datasets\",\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    install_lib(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:11.035380Z",
     "iopub.status.busy": "2024-08-19T14:02:11.035063Z",
     "iopub.status.idle": "2024-08-19T14:02:11.044263Z",
     "shell.execute_reply": "2024-08-19T14:02:11.043521Z",
     "shell.execute_reply.started": "2024-08-19T14:02:11.035350Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\")\n",
    "\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:11.045475Z",
     "iopub.status.busy": "2024-08-19T14:02:11.045242Z",
     "iopub.status.idle": "2024-08-19T14:02:21.545800Z",
     "shell.execute_reply": "2024-08-19T14:02:21.544916Z",
     "shell.execute_reply.started": "2024-08-19T14:02:11.045454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2\n",
      "bitsandbytes version: 0.43.3\n",
      "peft version: 0.12.0\n",
      "accelerate version: 0.32.1\n",
      "datasets version: 2.20.0\n",
      "trl version: 0.9.6\n",
      "transformers version: 4.42.3\n",
      "Device name: 'Tesla T4'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40)'\n",
      "Não suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import trl\n",
    "import warnings\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\n",
    "print(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:21.547426Z",
     "iopub.status.busy": "2024-08-19T14:02:21.547003Z",
     "iopub.status.idle": "2024-08-19T14:02:42.983126Z",
     "shell.execute_reply": "2024-08-19T14:02:42.982347Z",
     "shell.execute_reply.started": "2024-08-19T14:02:21.547401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 14:02:27.011796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-19 14:02:27.011907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-19 14:02:27.276582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:42.984706Z",
     "iopub.status.busy": "2024-08-19T14:02:42.984174Z",
     "iopub.status.idle": "2024-08-19T14:02:42.998146Z",
     "shell.execute_reply": "2024-08-19T14:02:42.997324Z",
     "shell.execute_reply.started": "2024-08-19T14:02:42.984679Z"
    }
   },
   "outputs": [],
   "source": [
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=3, early_stopping_threshold=0.02):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.best_metric = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n",
    "        current_metric = metrics.get(\"eval_loss\")  # Use the relevant metric for your task\n",
    "\n",
    "        if current_metric is None:\n",
    "            return\n",
    "\n",
    "        if self.best_metric is None or current_metric < self.best_metric - self.early_stopping_threshold:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.early_stopping_patience:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Early stopping at step {state.global_step} with best eval_loss = {self.best_metric}\")\n",
    "\n",
    "\n",
    "class SaveMetricsCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        self.output_dir = output_dir\n",
    "        self.output_path = os.path.join(output_dir, \"metrics.json\")\n",
    "        self.state_path = os.path.join(output_dir, \"state.json\")\n",
    "        print(f\"Output directory initialized at {output_dir}\")\n",
    "        self.metrics = self.load_existing_metrics()\n",
    "\n",
    "    def load_existing_metrics(self):\n",
    "        if os.path.isfile(self.output_path):\n",
    "            return pd.read_json(self.output_path, lines=True).to_dict('records')\n",
    "        return []\n",
    "\n",
    "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        state.save_to_json(self.state_path)\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            step = state.global_step\n",
    "            _metrics = {\"Step\": step, **metrics}\n",
    "            self.metrics.append(_metrics)\n",
    "            metrics_df = pd.DataFrame(self.metrics).drop_duplicates(subset=['Step'], keep='last')\n",
    "            metrics_df.to_json(self.output_path, orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:42.999618Z",
     "iopub.status.busy": "2024-08-19T14:02:42.999265Z",
     "iopub.status.idle": "2024-08-19T14:02:43.038428Z",
     "shell.execute_reply": "2024-08-19T14:02:43.037608Z",
     "shell.execute_reply.started": "2024-08-19T14:02:42.999585Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    try:\n",
    "        checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        return latest_checkpoint\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:43.039917Z",
     "iopub.status.busy": "2024-08-19T14:02:43.039548Z",
     "iopub.status.idle": "2024-08-19T14:02:43.048636Z",
     "shell.execute_reply": "2024-08-19T14:02:43.047963Z",
     "shell.execute_reply.started": "2024-08-19T14:02:43.039888Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:57.818535Z",
     "iopub.status.busy": "2024-08-19T14:02:57.818261Z",
     "iopub.status.idle": "2024-08-19T14:02:58.930530Z",
     "shell.execute_reply": "2024-08-19T14:02:58.929800Z",
     "shell.execute_reply.started": "2024-08-19T14:02:57.818511Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nunorc/squad_v1_pt\")\n",
    "dataset = dataset.map(lambda row: {\"response\": row['answers']['text'][0] })\n",
    "\n",
    "\n",
    "columns = ['id', 'context', 'question','response']\n",
    "train_dataset = dataset[\"train\"].shuffle(42).select(range(5000)).select_columns(columns)\n",
    "test_dataset = dataset[\"validation\"].shuffle(42).select(range(100)).select_columns(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:58.931839Z",
     "iopub.status.busy": "2024-08-19T14:02:58.931577Z",
     "iopub.status.idle": "2024-08-19T14:02:58.937531Z",
     "shell.execute_reply": "2024-08-19T14:02:58.936707Z",
     "shell.execute_reply.started": "2024-08-19T14:02:58.931817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    # attn_implementation = 'flash_attention_2'\n",
    "    attn_implementation = 'eager'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'eager'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:58.938982Z",
     "iopub.status.busy": "2024-08-19T14:02:58.938677Z",
     "iopub.status.idle": "2024-08-19T14:02:58.991904Z",
     "shell.execute_reply": "2024-08-19T14:02:58.991054Z",
     "shell.execute_reply.started": "2024-08-19T14:02:58.938948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "model_id = \"emdemor/question-generator\"\n",
    "LOCAL_MODELPATH = \"models/question-generator/\" + model_id.lower().replace(\"/\",\"-\").replace(\".\",\"_\")\n",
    "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:02:58.993295Z",
     "iopub.status.busy": "2024-08-19T14:02:58.992972Z",
     "iopub.status.idle": "2024-08-19T14:03:52.368380Z",
     "shell.execute_reply": "2024-08-19T14:03:52.367389Z",
     "shell.execute_reply.started": "2024-08-19T14:02:58.993264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07fffc29a594e51b71477631d626d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1123efc859ff4a9594121fbb80c3c69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6236faefba6746fdae364d2283e94408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecddca14f3534853afa73b89dc9d5708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1070d5e8813048acb130fecf39ea14fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dafdbb0b7214b9f8e5631fcdf7c8bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0930ca2a535f4bed9c2df2c702d8fec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a63db186a746368265506dc457522c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b866c7e9a906428e878d2ee26c259ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8019b42976443e825006b71d4e12f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebab02f93da40d09dad8fb4a3bfc1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0ef7efdff5477682abe09416d2c025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ff036a653499596f54f90231dfee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d3314926564a889e66bc7f83c1b2a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494ee9fe342045fcb9f3b1959186210f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0407cbf1f8a847abb21080510a6484f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "tokenizer = set_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:03:52.370031Z",
     "iopub.status.busy": "2024-08-19T14:03:52.369727Z",
     "iopub.status.idle": "2024-08-19T14:03:52.379768Z",
     "shell.execute_reply": "2024-08-19T14:03:52.378843Z",
     "shell.execute_reply.started": "2024-08-19T14:03:52.370004Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs['attention_mask'],\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:03:52.383126Z",
     "iopub.status.busy": "2024-08-19T14:03:52.380980Z",
     "iopub.status.idle": "2024-08-19T14:05:17.796215Z",
     "shell.execute_reply": "2024-08-19T14:05:17.795323Z",
     "shell.execute_reply.started": "2024-08-19T14:03:52.383090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 73.5 ms, total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O Rio de Janeiro é a capital do Brasil, onde está localizado o governo federal localizado no Palácio do Planalto. No entanto, o Rio de Janeiro é apenas a capital do Centro-Sul, enquanto o Brasil tem várias cidades capitais entre diferentes unidades federativas: Brasília é a capital do centro do país e sede do governo federal brasileiro, separada pelos planaltos de Goiás e Minas Gerais; Recife é a capital do norte do Brasil, com sede no estado de Pernambuco; Salvador, no sul da Bahia, é a capital do estado da Bahia e sede do governo estadual; São Luís, no nordeste do Maranhão, é a capital e a capital regional do Norte Regional em São Luis - no Norte da Região Norte; Campo Grande é a capital do interior do Mato Grosso do Sul (antigamente o Mato Grosso) e sede do órgão legislativo, com a finalidade de garantir a representação de seus quatros milhões de habitantes; Curitiba é a capital regional do Oeste Paulista do estado do Paraná, sede do governo estadual do estado de mesmo nome; Florianópolis é a capital da região Sul de Santa Catarina e sede do estado de Santa Catarina; Goiânia é a capital do sudeste de Goiás e sede do município; Plácido de Castro é a capital do estado de Acre e a capital regional Nordeste do estado do Acre; Rio Quente é a capital do estado de Mato Grosso do Sul, capital de Mato Grosso do Sul, capital e sede do município de \\nPonta Porã e capital do região noroeste de Mato Grosso do Sul.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "prompt = \"Qual a capital do Brasil?\"\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"Olá. Você é um expert em geografia e vai me ajudar a responder algumas questões.\"},\n",
    "    # {\"role\": \"assistent\", \"content\": \"Tudo bem! Como posso ajudar?\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "llm.generate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:05:17.798363Z",
     "iopub.status.busy": "2024-08-19T14:05:17.797579Z",
     "iopub.status.idle": "2024-08-19T14:05:18.862715Z",
     "shell.execute_reply": "2024-08-19T14:05:18.861883Z",
     "shell.execute_reply.started": "2024-08-19T14:05:17.798327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4e0023fa524297a3e3626add131c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737345981d3a47ac8576e7d3158d887f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_dataset_chatml(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{row['context']}\\n```\\nPergunta:\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"{row['question']}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "train_dataset_chatml = train_dataset.map(format_dataset_chatml)\n",
    "test_dataset_chatml = test_dataset.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:05:18.864268Z",
     "iopub.status.busy": "2024-08-19T14:05:18.863997Z",
     "iopub.status.idle": "2024-08-19T14:05:18.900297Z",
     "shell.execute_reply": "2024-08-19T14:05:18.899462Z",
     "shell.execute_reply.started": "2024-08-19T14:05:18.864244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.36 ms, sys: 0 ns, total: 3.36 ms\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=LOCAL_MODELPATH,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    # Validation\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 1000 passos\n",
    "    save_steps=50,         # Salvando a cada 1000 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias? TensorBoard?\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"emdemor/question-generator\"\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=1,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T14:05:18.901615Z",
     "iopub.status.busy": "2024-08-19T14:05:18.901317Z",
     "iopub.status.idle": "2024-08-19T15:46:51.235965Z",
     "shell.execute_reply": "2024-08-19T15:46:51.234968Z",
     "shell.execute_reply.started": "2024-08-19T14:05:18.901581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory initialized at models/question-generator/emdemor-question-generator\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59cf9b2c4bc452b942bf2a5657c7573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d69ad326fe47f293ad2c907645b058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/1875 1:41:18 < 2:32:21, 0.12 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.885200</td>\n",
       "      <td>1.707170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.603600</td>\n",
       "      <td>1.485588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.518600</td>\n",
       "      <td>1.466216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.482200</td>\n",
       "      <td>1.460276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>1.457846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.481300</td>\n",
       "      <td>1.455780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.487800</td>\n",
       "      <td>1.453382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.476500</td>\n",
       "      <td>1.452285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.480300</td>\n",
       "      <td>1.448461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.492500</td>\n",
       "      <td>1.447762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.490000</td>\n",
       "      <td>1.446656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.488800</td>\n",
       "      <td>1.446058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.473200</td>\n",
       "      <td>1.447001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.467700</td>\n",
       "      <td>1.447568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.447545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 50\n",
      "Saving checkpoint at step 100\n",
      "Saving checkpoint at step 150\n",
      "Saving checkpoint at step 200\n",
      "Saving checkpoint at step 250\n",
      "Saving checkpoint at step 300\n",
      "Saving checkpoint at step 350\n",
      "Saving checkpoint at step 400\n",
      "Saving checkpoint at step 450\n",
      "Saving checkpoint at step 500\n",
      "Saving checkpoint at step 550\n",
      "Saving checkpoint at step 600\n",
      "Saving checkpoint at step 650\n",
      "Saving checkpoint at step 700\n",
      "Early stopping at step 750 with best eval_loss = 1.4460580348968506\n",
      "Saving checkpoint at step 750\n",
      "CPU times: user 1h 41min 24s, sys: 6.91 s, total: 1h 41min 31s\n",
      "Wall time: 1h 41min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset_chatml,\n",
    "    eval_dataset=test_dataset_chatml,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback(), EarlyStoppingCallback( early_stopping_threshold=0.0005), SaveMetricsCallback(LOCAL_MODELPATH)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T15:46:51.237523Z",
     "iopub.status.busy": "2024-08-19T15:46:51.237230Z",
     "iopub.status.idle": "2024-08-19T15:46:52.412646Z",
     "shell.execute_reply": "2024-08-19T15:46:52.411711Z",
     "shell.execute_reply.started": "2024-08-19T15:46:51.237494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829f298d492543f3873b8a7f9d8da2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/emdemor/question-generator/commit/e44fd6a0c1a58a5d2864a090439ddb22a62d22a4', commit_message='Upload model', commit_description='Using more data', oid='e44fd6a0c1a58a5d2864a090439ddb22a62d22a4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(repo_id=\"emdemor/question-generator\", commit_description=\"Using more data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-19T15:46:52.414156Z",
     "iopub.status.busy": "2024-08-19T15:46:52.413831Z",
     "iopub.status.idle": "2024-08-19T15:46:52.419680Z",
     "shell.execute_reply": "2024-08-19T15:46:52.418863Z",
     "shell.execute_reply.started": "2024-08-19T15:46:52.414130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOCAL_MODELPATH,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Em nota a família informou que o desejo de Silvio Santos em relação a sua morte era de que quando partisse, fosse levado diretamente ao cemitério particular onde fosse realizada uma cerimônia judaica. Silvio Santos também havia desejado que sua morte não fosse explorada, pois gostava de ser celebrado em vida. Gostaria de ser lembrado com a alegria que viveu e que seu desejo fosse respeitado.\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    result = llm.generate(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5235696,
     "sourceId": 8724462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
