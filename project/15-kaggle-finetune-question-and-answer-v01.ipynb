{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8724462,"sourceType":"datasetVersion","datasetId":5235696}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-19T14:00:43.612190Z","iopub.execute_input":"2024-08-19T14:00:43.612527Z","iopub.status.idle":"2024-08-19T14:00:45.449880Z","shell.execute_reply.started":"2024-08-19T14:00:43.612499Z","shell.execute_reply":"2024-08-19T14:00:45.448921Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/portuguese-text2sql-database/sql-pt.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nos.environ[\"HUGGINGFACE_TOKEN\"] = UserSecretsClient().get_secret(\"HUGGINGFACE-TOKEN-TO-UPLOAD\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:00:45.451426Z","iopub.execute_input":"2024-08-19T14:00:45.451845Z","iopub.status.idle":"2024-08-19T14:00:45.585623Z","shell.execute_reply.started":"2024-08-19T14:00:45.451817Z","shell.execute_reply":"2024-08-19T14:00:45.584825Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def install_lib(libname):\n    print(f\">>> {libname}\")\n    get_ipython().system(f\"pip install -qqq {libname}\")\n\nlibs = [\n    \"bitsandbytes\",\n    \"peft\",\n    \"transformers\",\n    \"trl\",\n    \"accelerate\",\n    \"datasets\",\n]\n\nfor lib in libs:\n    install_lib(lib)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:00:45.586879Z","iopub.execute_input":"2024-08-19T14:00:45.587147Z","iopub.status.idle":"2024-08-19T14:02:11.032468Z","shell.execute_reply.started":"2024-08-19T14:00:45.587124Z","shell.execute_reply":"2024-08-19T14:02:11.031544Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":">>> bitsandbytes\n>>> peft\n>>> transformers\n>>> trl\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m>>> accelerate\n>>> datasets\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nimport warnings\n\n# Suprimir avisos específicos de FutureWarning e UserWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\")\n\n\n# Configurar o nível de log para a biblioteca transformers\nlogging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\nlogging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\nlogging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:11.035063Z","iopub.execute_input":"2024-08-19T14:02:11.035380Z","iopub.status.idle":"2024-08-19T14:02:11.044263Z","shell.execute_reply.started":"2024-08-19T14:02:11.035350Z","shell.execute_reply":"2024-08-19T14:02:11.043521Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport bitsandbytes\nimport peft\nimport accelerate\nimport datasets\nimport trl\nimport warnings\nimport transformers\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"torch version:\", torch.__version__)\nprint(\"bitsandbytes version:\", bitsandbytes.__version__)\nprint(\"peft version:\", peft.__version__)\nprint(\"accelerate version:\", accelerate.__version__)\nprint(\"datasets version:\", datasets.__version__)\nprint(\"trl version:\", trl.__version__)\nprint(\"transformers version:\", transformers.__version__)\nprint(f\"Device name: '{torch.cuda.get_device_name()}'\")\nprint(\"Device:\", device)\nprint(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\nprint(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:11.045242Z","iopub.execute_input":"2024-08-19T14:02:11.045475Z","iopub.status.idle":"2024-08-19T14:02:21.545800Z","shell.execute_reply.started":"2024-08-19T14:02:11.045454Z","shell.execute_reply":"2024-08-19T14:02:21.544916Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"torch version: 2.1.2\nbitsandbytes version: 0.43.3\npeft version: 0.12.0\naccelerate version: 0.32.1\ndatasets version: 2.20.0\ntrl version: 0.9.6\ntransformers version: 4.42.3\nDevice name: 'Tesla T4'\nDevice: cuda\nDevice properties: '_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40)'\nNão suporta bfloat16.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom random import randrange\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom huggingface_hub import login\nfrom datasets import load_dataset, Dataset\n\n\nfrom trl import SFTConfig, SFTTrainer\nfrom peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    TrainerCallback,\n    set_seed,\n    pipeline,\n    TrainerCallback,\n    TrainerControl,\n    TrainerState,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:21.547003Z","iopub.execute_input":"2024-08-19T14:02:21.547426Z","iopub.status.idle":"2024-08-19T14:02:42.983126Z","shell.execute_reply.started":"2024-08-19T14:02:21.547401Z","shell.execute_reply":"2024-08-19T14:02:42.982347Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-08-19 14:02:27.011796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-19 14:02:27.011907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-19 14:02:27.276582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class SaveCheckpointCallback(TrainerCallback):\n    def on_save(self, args, state, control, **kwargs):\n        print(f\"Saving checkpoint at step {state.global_step}\")\n\nclass EarlyStoppingCallback(TrainerCallback):\n    def __init__(self, early_stopping_patience=3, early_stopping_threshold=0.02):\n        self.early_stopping_patience = early_stopping_patience\n        self.early_stopping_threshold = early_stopping_threshold\n        self.best_metric = None\n        self.counter = 0\n\n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n        current_metric = metrics.get(\"eval_loss\")  # Use the relevant metric for your task\n\n        if current_metric is None:\n            return\n\n        if self.best_metric is None or current_metric < self.best_metric - self.early_stopping_threshold:\n            self.best_metric = current_metric\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.early_stopping_patience:\n                control.should_training_stop = True\n                print(f\"Early stopping at step {state.global_step} with best eval_loss = {self.best_metric}\")\n\n\nclass SaveMetricsCallback(TrainerCallback):\n    def __init__(self, output_dir):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        self.output_dir = output_dir\n        self.output_path = os.path.join(output_dir, \"metrics.json\")\n        self.state_path = os.path.join(output_dir, \"state.json\")\n        print(f\"Output directory initialized at {output_dir}\")\n        self.metrics = self.load_existing_metrics()\n\n    def load_existing_metrics(self):\n        if os.path.isfile(self.output_path):\n            return pd.read_json(self.output_path, lines=True).to_dict('records')\n        return []\n\n    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n        state.save_to_json(self.state_path)\n\n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n        if metrics:\n            step = state.global_step\n            _metrics = {\"Step\": step, **metrics}\n            self.metrics.append(_metrics)\n            metrics_df = pd.DataFrame(self.metrics).drop_duplicates(subset=['Step'], keep='last')\n            metrics_df.to_json(self.output_path, orient=\"records\", lines=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:42.984174Z","iopub.execute_input":"2024-08-19T14:02:42.984706Z","iopub.status.idle":"2024-08-19T14:02:42.998146Z","shell.execute_reply.started":"2024-08-19T14:02:42.984679Z","shell.execute_reply":"2024-08-19T14:02:42.997324Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_latest_checkpoint(checkpoint_dir):\n    try:\n        checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n        if not checkpoints:\n            return None\n        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n        return latest_checkpoint\n\n    except FileNotFoundError:\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:42.999265Z","iopub.execute_input":"2024-08-19T14:02:42.999618Z","iopub.status.idle":"2024-08-19T14:02:43.038428Z","shell.execute_reply.started":"2024-08-19T14:02:42.999585Z","shell.execute_reply":"2024-08-19T14:02:43.037608Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def set_tokenizer(model_id):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.padding_side = 'right'\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:43.039548Z","iopub.execute_input":"2024-08-19T14:02:43.039917Z","iopub.status.idle":"2024-08-19T14:02:43.048636Z","shell.execute_reply.started":"2024-08-19T14:02:43.039888Z","shell.execute_reply":"2024-08-19T14:02:43.047963Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"nunorc/squad_v1_pt\")\ndataset = dataset.map(lambda row: {\"response\": row['answers']['text'][0] })\n\n\ncolumns = ['id', 'context', 'question','response']\ntrain_dataset = dataset[\"train\"].shuffle(42).select(range(5000)).select_columns(columns)\ntest_dataset = dataset[\"validation\"].shuffle(42).select(range(100)).select_columns(columns)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:57.818261Z","iopub.execute_input":"2024-08-19T14:02:57.818535Z","iopub.status.idle":"2024-08-19T14:02:58.930530Z","shell.execute_reply.started":"2024-08-19T14:02:57.818511Z","shell.execute_reply":"2024-08-19T14:02:58.929800Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_bf16_supported():\n    compute_dtype = torch.bfloat16\n    # attn_implementation = 'flash_attention_2'\n    attn_implementation = 'eager'\nelse:\n    compute_dtype = torch.float16\n    attn_implementation = 'eager'\n\nprint(attn_implementation)\nprint(compute_dtype)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:58.931577Z","iopub.execute_input":"2024-08-19T14:02:58.931839Z","iopub.status.idle":"2024-08-19T14:02:58.937531Z","shell.execute_reply.started":"2024-08-19T14:02:58.931817Z","shell.execute_reply":"2024-08-19T14:02:58.936707Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"eager\ntorch.float16\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'true'\nos.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n\nmodel_id = \"emdemor/question-generator\"\nLOCAL_MODELPATH = \"models/question-generator/\" + model_id.lower().replace(\"/\",\"-\").replace(\".\",\"_\")\nlogin(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\nset_seed(1234)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:58.938677Z","iopub.execute_input":"2024-08-19T14:02:58.938982Z","iopub.status.idle":"2024-08-19T14:02:58.991904Z","shell.execute_reply.started":"2024-08-19T14:02:58.938948Z","shell.execute_reply":"2024-08-19T14:02:58.991054Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=\"bfloat16\",\n        bnb_4bit_use_double_quant=True,\n)\n\n# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n# Parâmetros importantes incluem:\n#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n#  - attn_implementation=attn_implementation: Define a implementação da atenção.\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=compute_dtype,\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation,\n)\n\n# adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\nmodel = prepare_model_for_kbit_training(model)\n\ntokenizer = set_tokenizer(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:02:58.992972Z","iopub.execute_input":"2024-08-19T14:02:58.993295Z","iopub.status.idle":"2024-08-19T14:03:52.368380Z","shell.execute_reply.started":"2024-08-19T14:02:58.993264Z","shell.execute_reply":"2024-08-19T14:03:52.367389Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07fffc29a594e51b71477631d626d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1123efc859ff4a9594121fbb80c3c69f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6236faefba6746fdae364d2283e94408"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecddca14f3534853afa73b89dc9d5708"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1070d5e8813048acb130fecf39ea14fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dafdbb0b7214b9f8e5631fcdf7c8bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0930ca2a535f4bed9c2df2c702d8fec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a63db186a746368265506dc457522c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b866c7e9a906428e878d2ee26c259ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc8019b42976443e825006b71d4e12f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebab02f93da40d09dad8fb4a3bfc1a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac0ef7efdff5477682abe09416d2c025"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25ff036a653499596f54f90231dfee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d3314926564a889e66bc7f83c1b2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494ee9fe342045fcb9f3b1959186210f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0407cbf1f8a847abb21080510a6484f8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"class LanguageModel:\n\n    def __init__(self, tokenizer, model, device):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.device = device\n        if self.tokenizer.pad_token_id is None:\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n    def tokenize(self, messages):\n        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n        return model_inputs\n\n    def generate(self, messages):\n        model_inputs = self.tokenize(messages)\n        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n        generated_ids = model.generate(\n            model_inputs.input_ids,\n            max_new_tokens=512,\n            do_sample=True,\n            attention_mask=model_inputs['attention_mask'],\n            pad_token_id=self.tokenizer.pad_token_id\n        )\n        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:03:52.369727Z","iopub.execute_input":"2024-08-19T14:03:52.370031Z","iopub.status.idle":"2024-08-19T14:03:52.379768Z","shell.execute_reply.started":"2024-08-19T14:03:52.370004Z","shell.execute_reply":"2024-08-19T14:03:52.378843Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\n\nllm = LanguageModel(tokenizer, model, device=\"cuda\")\n\nprompt = \"Qual a capital do Brasil?\"\n\nmessages = [\n    # {\"role\": \"user\", \"content\": \"Olá. Você é um expert em geografia e vai me ajudar a responder algumas questões.\"},\n    # {\"role\": \"assistent\", \"content\": \"Tudo bem! Como posso ajudar?\"},\n    {\"role\": \"user\", \"content\": prompt},\n]\n\nllm.generate(messages)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:03:52.380980Z","iopub.execute_input":"2024-08-19T14:03:52.383126Z","iopub.status.idle":"2024-08-19T14:05:17.796215Z","shell.execute_reply.started":"2024-08-19T14:03:52.383090Z","shell.execute_reply":"2024-08-19T14:05:17.795323Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 1min 19s, sys: 73.5 ms, total: 1min 19s\nWall time: 1min 20s\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'O Rio de Janeiro é a capital do Brasil, onde está localizado o governo federal localizado no Palácio do Planalto. No entanto, o Rio de Janeiro é apenas a capital do Centro-Sul, enquanto o Brasil tem várias cidades capitais entre diferentes unidades federativas: Brasília é a capital do centro do país e sede do governo federal brasileiro, separada pelos planaltos de Goiás e Minas Gerais; Recife é a capital do norte do Brasil, com sede no estado de Pernambuco; Salvador, no sul da Bahia, é a capital do estado da Bahia e sede do governo estadual; São Luís, no nordeste do Maranhão, é a capital e a capital regional do Norte Regional em São Luis - no Norte da Região Norte; Campo Grande é a capital do interior do Mato Grosso do Sul (antigamente o Mato Grosso) e sede do órgão legislativo, com a finalidade de garantir a representação de seus quatros milhões de habitantes; Curitiba é a capital regional do Oeste Paulista do estado do Paraná, sede do governo estadual do estado de mesmo nome; Florianópolis é a capital da região Sul de Santa Catarina e sede do estado de Santa Catarina; Goiânia é a capital do sudeste de Goiás e sede do município; Plácido de Castro é a capital do estado de Acre e a capital regional Nordeste do estado do Acre; Rio Quente é a capital do estado de Mato Grosso do Sul, capital de Mato Grosso do Sul, capital e sede do município de \\nPonta Porã e capital do região noroeste de Mato Grosso do Sul.'"},"metadata":{}}]},{"cell_type":"code","source":"def format_dataset_chatml(row):\n    messages = [\n        {\n            \"content\": \"Você é um assistente especializado em interpretação de texto\",\n            \"role\": \"system\"\n        },\n        {\n            \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{row['context']}\\n```\\nPergunta:\",\n            \"role\": \"user\"\n        },\n        {\n            \"content\": f\"{row['question']}\",\n            \"role\": \"assistant\"\n        }\n    ]\n\n    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n\ntrain_dataset_chatml = train_dataset.map(format_dataset_chatml)\ntest_dataset_chatml = test_dataset.map(format_dataset_chatml)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:05:17.797579Z","iopub.execute_input":"2024-08-19T14:05:17.798363Z","iopub.status.idle":"2024-08-19T14:05:18.862715Z","shell.execute_reply.started":"2024-08-19T14:05:17.798327Z","shell.execute_reply":"2024-08-19T14:05:18.861883Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4e0023fa524297a3e3626add131c3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737345981d3a47ac8576e7d3158d887f"}},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\nsft_config = SFTConfig(\n    seed=42,\n    output_dir=LOCAL_MODELPATH,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n\n    # Training Hyperparameters\n    learning_rate=5e-4,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"linear\",\n\n    # Validation\n    do_eval=True,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n\n    # Chackpoints\n    save_strategy=\"steps\",  # Salvando a cada 1000 passos\n    save_steps=50,         # Salvando a cada 1000 passos\n    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n\n    # Loggings\n    log_level=\"warning\",\n    logging_steps=50,\n\n    # Weights yype\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n\n    # Report to Weights And Bias? TensorBoard?\n    report_to=\"none\",\n    \n    push_to_hub=True,\n    hub_model_id=\"emdemor/question-generator\"\n)\n\npeft_config = LoraConfig(\n        r=16,\n        lora_alpha=1,\n        lora_dropout=0.05,\n        task_type=TaskType.CAUSAL_LM,\n        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:05:18.863997Z","iopub.execute_input":"2024-08-19T14:05:18.864268Z","iopub.status.idle":"2024-08-19T14:05:18.900297Z","shell.execute_reply.started":"2024-08-19T14:05:18.864244Z","shell.execute_reply":"2024-08-19T14:05:18.899462Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"CPU times: user 3.36 ms, sys: 0 ns, total: 3.36 ms\nWall time: 29 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntrainer = SFTTrainer(\n    model,\n    train_dataset=train_dataset_chatml,\n    eval_dataset=test_dataset_chatml,\n    args=sft_config,\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    callbacks=[SaveCheckpointCallback(), EarlyStoppingCallback( early_stopping_threshold=0.0005), SaveMetricsCallback(LOCAL_MODELPATH)],\n)\n\ntrainer.train()\n\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:05:18.901317Z","iopub.execute_input":"2024-08-19T14:05:18.901615Z","iopub.status.idle":"2024-08-19T15:46:51.235965Z","shell.execute_reply.started":"2024-08-19T14:05:18.901581Z","shell.execute_reply":"2024-08-19T15:46:51.234968Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Output directory initialized at models/question-generator/emdemor-question-generator\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b59cf9b2c4bc452b942bf2a5657c7573"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d69ad326fe47f293ad2c907645b058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 750/1875 1:41:18 < 2:32:21, 0.12 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.885200</td>\n      <td>1.707170</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.603600</td>\n      <td>1.485588</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.518600</td>\n      <td>1.466216</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.482200</td>\n      <td>1.460276</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.503500</td>\n      <td>1.457846</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.481300</td>\n      <td>1.455780</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.487800</td>\n      <td>1.453382</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.476500</td>\n      <td>1.452285</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.480300</td>\n      <td>1.448461</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.492500</td>\n      <td>1.447762</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.490000</td>\n      <td>1.446656</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.488800</td>\n      <td>1.446058</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.473200</td>\n      <td>1.447001</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.467700</td>\n      <td>1.447568</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.440200</td>\n      <td>1.447545</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving checkpoint at step 50\nSaving checkpoint at step 100\nSaving checkpoint at step 150\nSaving checkpoint at step 200\nSaving checkpoint at step 250\nSaving checkpoint at step 300\nSaving checkpoint at step 350\nSaving checkpoint at step 400\nSaving checkpoint at step 450\nSaving checkpoint at step 500\nSaving checkpoint at step 550\nSaving checkpoint at step 600\nSaving checkpoint at step 650\nSaving checkpoint at step 700\nEarly stopping at step 750 with best eval_loss = 1.4460580348968506\nSaving checkpoint at step 750\nCPU times: user 1h 41min 24s, sys: 6.91 s, total: 1h 41min 31s\nWall time: 1h 41min 32s\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.push_to_hub(repo_id=\"emdemor/question-generator\", commit_description=\"Using more data\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:51.237230Z","iopub.execute_input":"2024-08-19T15:46:51.237523Z","iopub.status.idle":"2024-08-19T15:46:52.412646Z","shell.execute_reply.started":"2024-08-19T15:46:51.237494Z","shell.execute_reply":"2024-08-19T15:46:52.411711Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829f298d492543f3873b8a7f9d8da2ec"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/emdemor/question-generator/commit/e44fd6a0c1a58a5d2864a090439ddb22a62d22a4', commit_message='Upload model', commit_description='Using more data', oid='e44fd6a0c1a58a5d2864a090439ddb22a62d22a4', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"1","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:46:52.413831Z","iopub.execute_input":"2024-08-19T15:46:52.414156Z","iopub.status.idle":"2024-08-19T15:46:52.419680Z","shell.execute_reply.started":"2024-08-19T15:46:52.414130Z","shell.execute_reply":"2024-08-19T15:46:52.418863Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nft_model = AutoModelForCausalLM.from_pretrained(\n    LOCAL_MODELPATH,\n    torch_dtype=compute_dtype,\n    trust_remote_code=True,\n    #quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = \"Em nota a família informou que o desejo de Silvio Santos em relação a sua morte era de que quando partisse, fosse levado diretamente ao cemitério particular onde fosse realizada uma cerimônia judaica. Silvio Santos também havia desejado que sua morte não fosse explorada, pois gostava de ser celebrado em vida. Gostaria de ser lembrado com a alegria que viveu e que seu desejo fosse respeitado.\"\n\n\nmessages = [\n    {\n        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n        \"role\": \"system\"\n    },\n    {\n        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n        \"role\": \"user\"\n    },\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith torch.cuda.amp.autocast():\n    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n    result = llm.generate(messages)\n\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}