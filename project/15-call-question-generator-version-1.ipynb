{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8976cac-8c2d-44f7-955d-40069317fb18",
   "metadata": {},
   "source": [
    "# Utilização de LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc86a87a-482f-4303-8d9d-d0df123f017c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:41:17.090291Z",
     "iopub.status.busy": "2024-09-14T04:41:17.089966Z",
     "iopub.status.idle": "2024-09-14T04:41:17.097199Z",
     "shell.execute_reply": "2024-09-14T04:41:17.096661Z",
     "shell.execute_reply.started": "2024-09-14T04:41:17.090260Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*use_reentrant parameter should be passed explicitly.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b702c-fa21-4c82-a531-2538593ad3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:41:48.940447Z",
     "iopub.status.busy": "2024-09-14T04:41:48.938607Z",
     "iopub.status.idle": "2024-09-14T04:41:51.040584Z",
     "shell.execute_reply": "2024-09-14T04:41:51.040142Z",
     "shell.execute_reply.started": "2024-09-14T04:41:48.940364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.3\n",
      "peft version: 0.12.0\n",
      "accelerate version: 0.34.2\n",
      "datasets version: 2.21.0\n",
      "trl version: 0.10.1\n",
      "transformers version: 4.43.3\n",
      "Device name: 'NVIDIA GeForce RTX 4060 Ti'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Ti', major=8, minor=9, total_memory=16059MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(\n",
    "    f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\"\n",
    ")\n",
    "print(\n",
    "    \"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25699ee-09f7-4ee2-952d-adda0691d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513a330f-7957-435a-92f8-6ccf87a66926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:41:52.930752Z",
     "iopub.status.busy": "2024-09-14T04:41:52.930080Z",
     "iopub.status.idle": "2024-09-14T04:41:55.384746Z",
     "shell.execute_reply": "2024-09-14T04:41:55.384284Z",
     "shell.execute_reply.started": "2024-09-14T04:41:52.930693Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, PeftModel, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867fb4c8-6542-4d96-84de-92bf5859ec99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:43:12.240539Z",
     "iopub.status.busy": "2024-09-14T04:43:12.240280Z",
     "iopub.status.idle": "2024-09-14T04:43:12.245095Z",
     "shell.execute_reply": "2024-09-14T04:43:12.244612Z",
     "shell.execute_reply.started": "2024-09-14T04:43:12.240520Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages, max_new_tokens=2000, **kwargs):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].to(\n",
    "            model_inputs[\"input_ids\"].device\n",
    "        )\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs[\"attention_mask\"],\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09761d40-2d18-4f3e-ae49-e44aee9f3838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:41:56.680444Z",
     "iopub.status.busy": "2024-09-14T04:41:56.679550Z",
     "iopub.status.idle": "2024-09-14T04:41:56.697750Z",
     "shell.execute_reply": "2024-09-14T04:41:56.697177Z",
     "shell.execute_reply.started": "2024-09-14T04:41:56.680368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    # compute_dtype = torch.bfloat16\n",
    "    # attn_implementation = \"flash_attention_2\"\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    # attn_implementation = 'eager'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf36c5-f3ed-451b-b8ea-652574320e21",
   "metadata": {},
   "source": [
    "# Question Generator - Versão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97dec8e-3ae1-4df6-894f-338166c64908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:42:31.937674Z",
     "iopub.status.busy": "2024-09-14T04:42:31.937479Z",
     "iopub.status.idle": "2024-09-14T04:42:39.656856Z",
     "shell.execute_reply": "2024-09-14T04:42:39.656329Z",
     "shell.execute_reply.started": "2024-09-14T04:42:31.937659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0623eb7b16f4e3d8eee637566b6d613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcad84eec354adc8568a35280187503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/question-generator\"\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# # adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = set_tokenizer(model_id)\n",
    "\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15893b9-f576-4c2e-9ac9-3689534faccb",
   "metadata": {},
   "source": [
    "# Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e6f782-9dda-4627-bbec-c5730c322f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:44:21.329096Z",
     "iopub.status.busy": "2024-09-14T04:44:21.328831Z",
     "iopub.status.idle": "2024-09-14T04:44:21.332494Z",
     "shell.execute_reply": "2024-09-14T04:44:21.332094Z",
     "shell.execute_reply.started": "2024-09-14T04:44:21.329079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 2 µs, total: 16 µs\n",
      "Wall time: 19.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "O Ministério Público Eleitoral entrou com uma ação contra o candidato do PRTB à Prefeitura de São Paulo, Pablo Marçal, para pedir a suspensão do registro de candidatura do coach e a abertura de uma investigação por abuso de poder econômico.\n",
    "\"\"\"\n",
    "\n",
    "def generate_question(context, temperature=0.01):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return llm.generate(messages, temperature=temperature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "635fe87e-2c2f-4832-87a8-64ac864be94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:48:00.101875Z",
     "iopub.status.busy": "2024-09-14T04:48:00.101492Z",
     "iopub.status.idle": "2024-09-14T04:48:03.200720Z",
     "shell.execute_reply": "2024-09-14T04:48:03.200219Z",
     "shell.execute_reply.started": "2024-09-14T04:48:00.101858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onde o rapaz estava sentado?\n",
      "Onde o rapaz estava sentado?\n",
      "Onde o rapaz estava sentado?\n",
      "Onde o rapaz estava sentado?\n",
      "Onde o rapaz estava sentado?\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Uma noite destas, vindo da cidade para o Engenho Novo, encontrei no trem da\n",
    "Central um rapaz aqui do bairro, que eu conheço de vista e de chapéu.\n",
    "Cumprimentou-me, sentou-se ao pé de mim, falou da Lua e dos ministros, e\n",
    "acabou recitando-me versos. A viagem era curta, e os versos pode ser que não\n",
    "fossem inteiramente maus. Sucedeu, porém, que, como eu estava cansado, fechei\n",
    "os olhos três ou quatro vezes; tanto bastou para que ele interrompesse a leitura e\n",
    "metesse os versos no bolso. \n",
    "\"\"\"\n",
    "\n",
    "for i in range(5):\n",
    "    print(generate_question(context, temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be4e044-8941-47e3-8e5f-26d923351e76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T04:48:04.862068Z",
     "iopub.status.busy": "2024-09-14T04:48:04.861692Z",
     "iopub.status.idle": "2024-09-14T04:48:07.903877Z",
     "shell.execute_reply": "2024-09-14T04:48:07.903311Z",
     "shell.execute_reply.started": "2024-09-14T04:48:04.862047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quem estava no trem com o narrador?\n",
      "Quem estava cansado?\n",
      "Onde ele estava sentado?\n",
      "Que tipo de pessoa era o rapaz?\n",
      "O que o rapaz recitou?\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_question(context, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002dbf0-b030-496f-90e0-1d98e87ff60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
