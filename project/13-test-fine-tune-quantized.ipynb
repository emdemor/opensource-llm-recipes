{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8976cac-8c2d-44f7-955d-40069317fb18",
   "metadata": {},
   "source": [
    "# Utilização de LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc86a87a-482f-4303-8d9d-d0df123f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\")\n",
    "\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b702c-fa21-4c82-a531-2538593ad3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.1\n",
      "peft version: 0.11.1\n",
      "accelerate version: 0.31.0\n",
      "datasets version: 2.19.2\n",
      "trl version: 0.9.6\n",
      "transformers version: 4.44.0\n",
      "Device name: 'NVIDIA GeForce RTX 2060 SUPER'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 2060 SUPER', major=7, minor=5, total_memory=7957MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import trl\n",
    "import warnings\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\n",
    "print(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25699ee-09f7-4ee2-952d-adda0691d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513a330f-7957-435a-92f8-6ccf87a66926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867fb4c8-6542-4d96-84de-92bf5859ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs['attention_mask'],\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09761d40-2d18-4f3e-ae49-e44aee9f3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# if torch.cuda.is_bf16_supported():\n",
    "#     compute_dtype = torch.bfloat16\n",
    "#     # attn_implementation = 'flash_attention_2'\n",
    "#     attn_implementation = 'eager'\n",
    "# else:\n",
    "compute_dtype = torch.float16\n",
    "attn_implementation = 'eager'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97dec8e-3ae1-4df6-894f-338166c64908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c9c0247fca4753bab530b076d11d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5ef87a897493099d50bb7b41e43db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ff2a31387f4ce690738cfbe62ebd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1111678b894eec806863d7ed7059e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20314b649ac94a539b92365dbd859b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7929e1a386974b22801db5c76721ac87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/question-generator\"\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return tokenizer\n",
    "tokenizer = set_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e6f782-9dda-4627-bbec-c5730c322f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 s, sys: 107 ms, total: 1.74 s\n",
      "Wall time: 1.74 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Em que sentimento Silvio Santos gostava em relação à sua morte?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"Em nota a família informou que o desejo de Silvio Santos em relação a sua morte era de que quando partisse, fosse levado diretamente ao cemitério particular onde fosse realizada uma cerimônia judaica. Silvio Santos também havia desejado que sua morte não fosse explorada, pois gostava de ser celebrado em vida. Gostaria de ser lembrado com a alegria que viveu e que seu desejo fosse respeitado.\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    result = llm.generate(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbd99d-e4ad-4791-b4f3-6ff8b2e526c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c3336c-a3f0-4bdd-9a25-1f97d6d8af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = Onde a arma foi colocada no final da rua onde o segundo suspeito foi encontrado?\n",
      "response = A arma foi colocada em uma lixeira na Rua Abrieiro.\n",
      "CPU times: user 1.63 s, sys: 171 ms, total: 1.8 s\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "Dois jovens de 19 e 23 anos foram presos na noite desta sexta-feira,16, após disparos de arma de fogo próximo a um bar em Poços de Caldas.\n",
    "\n",
    "Segundo a Polícia Militar, durante uma operação na Avenida Marechal Castelo Branco, no bairro Jardim São Paulo, os policiais ouviram dois disparos de arma de fogo.\n",
    "\n",
    "Clique aqui e participe do grupo de notícias da Onda Poços no WhatsApp\n",
    "\n",
    "Em seguida foi recebida uma denúncia, via 190, relatando que o jovem de 19 anos trajando camiseta do flamengo havia efetuado dois disparos de arma de fogo próximo a um bar e que ele havia repassado a arma a um segundo indivíduo que trajava blusa com capuz preta e bermuda de cor preta, que após pegar arma saiu correndo em direção à Rua Engenheiro Ubirajara Machado de Moraes.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, o jovem de 19 anos já conhecido no meio policial por posse ilegal de arma e tráfico de drogas, foi localizado e abordado.\n",
    "\n",
    " \n",
    "\n",
    "Uma outra equipe da Polícia Militar recebeu denúncias, via 190, de que o segundo suspeito que havia fugido do local com a arma de fogo, a deixou em uma lixeira na Rua Abrieiro. A arma foi apreendida.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, após capturar o primeiro suspeito, a equipe continuou o rastreamento para localizar o segundo envolvido. O jovem foi encontrado ao final da Rua Engenheiro Ubirajara Machado de Moraes com as mesmas vestes repassadas pela denúncia.\n",
    "\n",
    " \n",
    "A arma de fogo se tratava de um revólver calibre 38 marca Taurus, que foi apreendida. Ainda segundo a PM, no local e nas adjacências não foram localizadas vítimas e não foi detectado o local exato em que o projétil atingiu.\n",
    "\n",
    "A dupla foi presa, encaminhada à UPA e em seguida levada para a Delegacia de Polícia Civil.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    question = llm.generate(messages)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Dado o seguinte contexto ```{context}``` responda a pergunta: `{question}`.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"question =\", question)\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51501907-a5a5-4013-8659-8c6e28cf6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = Qual é a importância epistemológica dessa abordagem da derivação matemática da Mecânica Quântica?\n",
      "response = A importância epistemológica da abordagem da derivação matemática da Mecânica Quântica, conforme descrito no artigo, reside em vários aspectos:\n",
      "\n",
      "1. **Fundamentação Teórica**: Ao partir de axiomas para derivar a equação de Schrödinger, a abordagem reforça a ideia de que é possível construir teorias físicas de forma rigorosa e estruturada, o que é fundamental para a compreensão profunda dos conceitos subjacentes à Mecânica Quântica.\n",
      "\n",
      "2. **Conexão com Métodos Alternativos**: A equivalência entre a derivação matemática apresentada e a abordagem de integrais de trajetória de Feynman destaca a versatilidade das interpretações e metodologias dentro da Mecânica Quântica. Isso enriquece a compreensão dos estudantes ao mostrar que diferentes abordagens podem levar aos mesmos resultados, promovendo uma visão mais ampla da teoria.\n",
      "\n",
      "3. **Contextualização no Ensino**: A transposição didática do tema da quantização para o contexto do ensino de Mecânica Quântica é crucial para facilitar a compreensão por parte dos alunos. A derivação axiomática pode servir como uma ferramenta pedagógica, ajudando os estudantes a internalizar conceitos complexos de maneira mais intuitiva.\n",
      "\n",
      "4. **Interpretação dos Símbolos**: A discussão sobre a interpretação dos símbolos da teoria é essencial para clarear o significado físico e matemático dos elementos envolvidos na Mecânica Quântica. Isso contribui para um entendimento mais robusto e crítico por parte dos estudantes, que podem ver além da matemática pura e considerar as implicações físicas.\n",
      "\n",
      "5. **Regras de Quantização**: A possibilidade de derivar as regras de quantização de Bohr-Sommerfeld a partir dos axiomas demonstrados fornece uma conexão histórica e teórica com os desenvolvimentos da Mecânica Quântica, mostrando como a teoria evoluiu e se fundou em fundamentos sólidos.\n",
      "\n",
      "6. **Importância do Teorema do Limite Central**: O uso de parâmetros pequenos e o esclarecimento em termos do Teorema do Limite Central introduzem conceitos estatísticos fundamentais que são essenciais para a compreensão de fenômenos quânticos, ligando a Mecânica Quântica a uma base probabilística.\n",
      "\n",
      "Em resumo, essa abordagem axiomática para a derivação matemática da Mecânica Quântica não apenas fundamenta a teoria de maneira robusta, mas também a torna mais acessível e compreensível para o ensino, estimulando uma reflexão crítica sobre os fundamentos e a interpretação da física quântica.\n",
      "CPU times: user 1.97 s, sys: 150 ms, total: 2.12 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "Neste artigo, apresentamos uma derivação matemática da equação de Schrödinger partindo de apenas dois axiomas. Mostramos também que, utilizando este processo de derivação formal, é possível derivar diretamente a equação de Schrödinger em sistemas de coordenadas curvilíneas generalizadas. Esta derivação também se mostra equivalente à abordagem de integrais de trajetória de Feynman, mas vai além, permitindo-nos derivar matematicamente as regras de quantização de Bohr-Sommerfeld. O uso de um parâmetro pequeno, tanto na presente derivação, em que aparece como δr, quanto na derivação de Feynman, em que aparece como ϵ = δt, também é esclarecido em termos do Teorema do Limite Central. O artigo faz, pois, uma transposição didática do tema da quantização, permitindo que seja abordado no contexto do ensino de Mecânica Quântica. A importância epistemológica de abordagens axiomáticas para a derivação matemática e a interpretação dos símbolos da teoria também é tratada.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    question = llm.generate(messages)\n",
    "    print(\"question =\", question)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Dado o seguinte contexto ```{context}``` responda a pergunta: `{question}`.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3da1a6-cc48-449d-8afe-c2b7505f6467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
