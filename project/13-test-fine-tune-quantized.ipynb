{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8976cac-8c2d-44f7-955d-40069317fb18",
   "metadata": {},
   "source": [
    "# Utilização de LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc86a87a-482f-4303-8d9d-d0df123f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\")\n",
    "\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b702c-fa21-4c82-a531-2538593ad3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.1\n",
      "peft version: 0.11.1\n",
      "accelerate version: 0.31.0\n",
      "datasets version: 2.19.2\n",
      "trl version: 0.9.6\n",
      "transformers version: 4.44.0\n",
      "Device name: 'NVIDIA GeForce GTX 1650'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce GTX 1650', major=7, minor=5, total_memory=3903MB, multi_processor_count=14)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import trl\n",
    "import warnings\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\n",
    "print(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25699ee-09f7-4ee2-952d-adda0691d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513a330f-7957-435a-92f8-6ccf87a66926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867fb4c8-6542-4d96-84de-92bf5859ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs['attention_mask'],\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09761d40-2d18-4f3e-ae49-e44aee9f3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# if torch.cuda.is_bf16_supported():\n",
    "#     compute_dtype = torch.bfloat16\n",
    "#     # attn_implementation = 'flash_attention_2'\n",
    "#     attn_implementation = 'eager'\n",
    "# else:\n",
    "compute_dtype = torch.float16\n",
    "attn_implementation = 'eager'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf36c5-f3ed-451b-b8ea-652574320e21",
   "metadata": {},
   "source": [
    "# Versão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97dec8e-3ae1-4df6-894f-338166c64908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aa14fc034d486cb4088f0eb3c337e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174f422ca3284acfb3412c53e68e512a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6014a843bde6467ba2794292b7008a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b9358289384e1e97af93e5d2ea454c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad2cff6aca4395b78643204c4ab6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186e1b0b54e4fbca3fb3aa6d5d31177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be4d8a7a8e84252aebfb05b0a4378b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc586617e2445c38b20caf0b23b5920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898ad82809d40c7b0bc094461fe767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de44cfc9a2344ce8206f7d53ea362dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe4374f51784907a019a56e01884246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52821c9f5054dac82984715508654d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ab64b40c704ea49368664f3051924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f38e34be5d4872ae3f021de4e73b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326c771ff366458689588cb26621dfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5d083703ba4c989de1e18cb4b9795a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a3dcfa9874c099f61fd5aea4f5032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/question-generator\"\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# # adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return tokenizer\n",
    "tokenizer = set_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f46b2-7908-4742-bfe7-ddbf2b441803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e6f782-9dda-4627-bbec-c5730c322f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.08 s, sys: 1.41 s, total: 4.49 s\n",
      "Wall time: 4.49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Em que ano Pablo Marçal recebeu o registro de candidatura?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "O Ministério Público Eleitoral entrou com uma ação contra o candidato do PRTB à Prefeitura de São Paulo, Pablo Marçal, para pedir a suspensão do registro de candidatura do coach e a abertura de uma investigação por abuso de poder econômico.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    result = llm.generate(messages)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c3336c-a3f0-4bdd-9a25-1f97d6d8af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = Quem saiu correndo do bar após ser preso?\n",
      "response = O jovem de 19 anos, que havia disparado a arma e repassado a arma a um segundo indivíduo, foi quem saiu correndo em direção à Rua Engenheiro Ubirajara Machado de Moraes após os disparos. No entanto, ele foi localizado e abordado pela Polícia Militar. Portanto, ninguém \"saiu correndo do bar após ser preso\", já que após serem abordados, os dois jovens foram presos.\n",
      "CPU times: user 10.2 s, sys: 6.15 s, total: 16.3 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "Dois jovens de 19 e 23 anos foram presos na noite desta sexta-feira,16, após disparos de arma de fogo próximo a um bar em Poços de Caldas.\n",
    "\n",
    "Segundo a Polícia Militar, durante uma operação na Avenida Marechal Castelo Branco, no bairro Jardim São Paulo, os policiais ouviram dois disparos de arma de fogo.\n",
    "\n",
    "Clique aqui e participe do grupo de notícias da Onda Poços no WhatsApp\n",
    "\n",
    "Em seguida foi recebida uma denúncia, via 190, relatando que o jovem de 19 anos trajando camiseta do flamengo havia efetuado dois disparos de arma de fogo próximo a um bar e que ele havia repassado a arma a um segundo indivíduo que trajava blusa com capuz preta e bermuda de cor preta, que após pegar arma saiu correndo em direção à Rua Engenheiro Ubirajara Machado de Moraes.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, o jovem de 19 anos já conhecido no meio policial por posse ilegal de arma e tráfico de drogas, foi localizado e abordado.\n",
    "\n",
    " \n",
    "\n",
    "Uma outra equipe da Polícia Militar recebeu denúncias, via 190, de que o segundo suspeito que havia fugido do local com a arma de fogo, a deixou em uma lixeira na Rua Abrieiro. A arma foi apreendida.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, após capturar o primeiro suspeito, a equipe continuou o rastreamento para localizar o segundo envolvido. O jovem foi encontrado ao final da Rua Engenheiro Ubirajara Machado de Moraes com as mesmas vestes repassadas pela denúncia.\n",
    "\n",
    " \n",
    "A arma de fogo se tratava de um revólver calibre 38 marca Taurus, que foi apreendida. Ainda segundo a PM, no local e nas adjacências não foram localizadas vítimas e não foi detectado o local exato em que o projétil atingiu.\n",
    "\n",
    "A dupla foi presa, encaminhada à UPA e em seguida levada para a Delegacia de Polícia Civil.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    question = llm.generate(messages)\n",
    "    print(\"question =\", question)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Dado o seguinte contexto ```{context}``` responda a pergunta: `{question}`.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51501907-a5a5-4013-8659-8c6e28cf6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = Há o que a Ciência de Dados e IA permitem?\n",
      "response = A Ciência de Dados e a Inteligência Artificial (IA) permitem:\n",
      "\n",
      "1. **Tomada de Decisões Mais Informadas e Precisas**: Elas possibilitam a análise de grandes volumes de dados, o que resulta em insights que fundamentam decisões mais acertadas.\n",
      "\n",
      "2. **Automação de Tarefas Repetitivas**: Muitas tarefas que são repetitivas e propensas a erros podem ser automatizadas, liberando os profissionais para se concentrarem em atividades mais estratégicas e criativas.\n",
      "\n",
      "3. **Identificação de Insights Valiosos**: Essas áreas ajudam na exploração de dados, permitindo a descoberta de padrões ocultos e insights que podem impulsionar a inovação e proporcionar vantagem competitiva para as organizações.\n",
      "\n",
      "Em suma, a Ciência de Dados e a IA oferecem oportunidades significativas para melhorar processos, aumentar a eficiência e fomentar a inovação nas indústrias.\n",
      "CPU times: user 6.57 s, sys: 3.83 s, total: 10.4 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "O Banco Central do Brasil (BC) acaba de criar um Centro de Excelência em Ciência de Dados e Inteligência Artificial (IA). Essa iniciativa demonstra a crescente importância dessas áreas não apenas para o BC, mas para diversas indústrias.\n",
    "\n",
    "\n",
    "Embora haja preocupações legítimas sobre o impacto da IA no mercado de trabalho, é importante destacar as enormes oportunidades que a Ciência de Dados e IA oferecem:\n",
    "*Tomada de decisões mais informadas e precisas com base em análises de grandes volumes de dados\n",
    "*Automação de tarefas repetitivas e propensas a erros, permitindo que os profissionais se concentrem em tarefas mais estratégicas\n",
    "*Identificação de insights valiosos e padrões ocultos nos dados que podem impulsionar a inovação e a vantagem competitiva\n",
    "\n",
    "\n",
    "A criação do Centro de Excelência em Ciência de Dados e IA pelo BC é um passo importante para aproveitar o potencial dessas tecnologias. À medida que outras indústrias seguem esse exemplo, é essencial que os profissionais se preparem para abraçar essa transformação e aproveitar as oportunidades que a Ciência de Dados e IA oferecem.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    question = llm.generate(messages)\n",
    "    print(\"question =\", question)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Dado o seguinte contexto ```{context}``` responda a pergunta: `{question}`.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39874dd7-0942-4af7-ab0b-cc1a15729a66",
   "metadata": {},
   "source": [
    "# Versão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61512dd0-3453-4aa5-bcfc-aaeea453ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85629d50e0b34ecf9bf058db6b6fbfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/733 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747e2e90e6784e64b08dd7501f5c6e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbb75a094b0472fbe47c9013827c007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac3947f1e804728b492664e563e537b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cc3bc6460543699e60dc1118c1a3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01547cd0fded49018b98ca48991a6c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d40bcac5c549d3b7a6f13461423cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468ed929ec1a42f2ad56193acbf85cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1f4a34a3a04f4384844a9b5fd1133f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c254fd016994376829e8b9c467a0d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516f4751e3be47798f06212b834c19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a408c4f553450eb3e87c92e51b05a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203a4527a6124f89b104c811d9dee02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26fd2e1f8904e7d94051e0035e97484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6850ebb6f841b593e180b517a83271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5461602775fa49978a7312c03c87f0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/question-generator-v2\"\n",
    "commit_hash = None\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    revision=commit_hash,\n",
    ")\n",
    "\n",
    "# # adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=commit_hash)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    return tokenizer\n",
    "tokenizer = set_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3a7fc-fbd9-47ae-8c76-62dd12988a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a64a980d-a443-4973-84de-f26631199b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"O que pode ser deduzido a partir do princípio teórico que leva às equações de Maxwell?\",\n",
      "        \"resposta\": \"A partir do mesmo procedimento estabelecido em teoria dos campos clássicos, é possível deduzir as equações de eletrodinâmica de Podolsky.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual é a consequência das equações de campo de Podolsky?\",\n",
      "        \"resposta\": \"A partir das equações de campo de Podolsky, é possível deduzir a equação de estado para a radiação de Podolsky.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Quais são os componentes da equação de estado de Podolsky?\",\n",
      "        \"resposta\": \"A equação de estado é do tipo P=w(a,T)ε, onde P é a pressão do gás fotônico, ε é a densidade de energia do gás fotônico, e w é o parâmetro da equação barotrópica que depende da temperatura T e da massa do fóton.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que a equação de estado de Podolsky permite resolver em relação à dinâmica cósmica?\",\n",
      "        \"resposta\": \"Usando a equação de estado na expressão de conservação do tensor energia-momento de fluido perfeito e na equação de Friedmann, é possível resolver a dinâmica cósmica para um universo preenchido pela radiação de Podolsky.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual é a relação entre a presença de fótons massivos e a dinâmica cósmica?\",\n",
      "        \"resposta\": \"A dinâmica é pouco afetada pela presença de fótons massivos, uma vez que 0,282 < wPodolsky < wMaxwell = 1/3 para qualquer valor de temperatura T, ou equivalentente, do tempo cosmológico t.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Como a correção de Podolsky para a lei de Stefan-Boltzmann é relevante?\",\n",
      "        \"resposta\": \"A correção de Podolsky para a lei de Stefan-Boltzmann é obtida para qualquer valor de temperatura, descrevendo potencialmente a dinâmica cósmica desde o universo primordial até o universo atual.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual é a máxima influência da massa do fóton na dinâmica de Podolsky?\",\n",
      "        \"resposta\": \"A máxima influência da massa do fóton acontece em ξref = 2,899, o que ocorre dentro de um intervalo de 0≦ξ≦8 para o parâmetro adimensional ξ=βm.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Para que valores do parâmetro ξ ocorre a máxima influência da massa do fóton?\",\n",
      "        \"resposta\": \"A máxima influência da massa do fóton ocorre para ξref = 2,899.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que acontece com a dinâmica de Podolsky quando a máxima influência da massa do fóton ocorre?\",\n",
      "        \"resposta\": \"Fora do intervalo de 0≦ξ≦8, a dinâmica de Podolsky tende à de Maxwell: nos limites de universo primordial (ξ≪1) e universo atual/futuro (ξ≫1), wPodolsky → wMaxwell e o fator de escala de Podoslsky vai com √t, de maneira consistente com um gás de fótons não massivos.\"\n",
      "    }\n",
      "]\n",
      "CPU times: user 1min 33s, sys: 16.1 ms, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "O mesmo procedimento estabelecido em teoria dos campos clássicos que nos leva a deduzir as equações de Maxwell, também conduz à eletrodinâmica de Podolsky, desde que a Lagrangia envolva derivadas do tensor intensidade de campo. A partir das equações de campo para Podolsky, que apresenta uma constante de acoplamento a associada à massa do fóton, é possível deduzir a equação de estado para a radiação de Podolsky. Essa equação é do tipo P=w(a,T)ε, em que P é pressão do gás fotônico; ε, a sua densidade de energia e w é o parâmetro da equação barotrópica que depende da temperatura T, além da massa do fóton. Usando essa equação de estado na expressão de conservação do tensor energia-momento de fluido perfeito e na equação de Friedmann, é possível resolver a dinâmica cósmica para um universo preenchido pela radiação de Podolsky. Mostramos que a dinâmica é pouco afetada pela presença de fótons massivos, uma vez que 0,282<wPodolsky<wMaxwell=1/3 para qualquer valor de T, ou equivalentente, do tempo cosmológico t. A correção de Podolsky para a lei de Stefan-Boltzmann é obtida para qualquer valor de temperatura, descrevendo potencialmente desde o universo primordial até o universo atual. Essa correção é relevante no intervalo 0≲ξ≲8 para o parâmetro adimensional ξ=βm. A máxima influência da massa do fóton acontece em ξref=2,899. Fora do intervalo referido intervalo de ξ, a dinâmica cosmológica de Podolsky tende à de Maxwell: nos limites de universo primordial (ξ≪1) e universo atual/futuro (ξ≫1), wPodolsky → wMaxwell e o fator de escala de Podoslsky vai com √t, de maneira consistente com um gás de fótons não massivos.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": f\"{context}\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    result = llm.generate(messages)\n",
    "\n",
    "\n",
    "try:\n",
    "    print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))\n",
    "except:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e6776d4-c617-42cc-af9a-d45bdc2b6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"pergunta\": \"Qual é a necessidade para se adentrar nos estudos cosmológicos?\",\n",
      "    \"resposta\": \"É necessário entender os desafios que tal jornada propicia.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Como é caracterizada a cosmologia em relação à natureza?\",\n",
      "    \"resposta\": \"A cosmologia é uma ciência que aborda aspectos da natureza do universo.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Quais são os desafios enfrentados na cosmologia?\",\n",
      "    \"resposta\": \"Destaca-se um enorme abismo entre a cosmologia e as demais ciências no que tange a escalas onde os fenômenos físicos ocorrem.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Quais são alguns dos fenômenos que a cosmologia estuda?\",\n",
      "    \"resposta\": \"Galáxias, aglomerados galácticos e todos objetos tais como esses.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Quais são as dimensões dos objetos estudados na cosmologia?\",\n",
      "    \"resposta\": \"Esses objetos apresentam dimensões tão superiores as escalas com que estamos habituados.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Como deve ser realizada a estudo dos objetos cosmológicos?\",\n",
      "    \"resposta\": \"Quadra estudá-los deve ser uma tarefa a ser realizada com extremo cuidado.\"\n",
      "  },\n",
      "  {\n",
      "    \"pergunta\": \"Qual método deve ser aplicado em estudos cosmológicos?\",\n",
      "    \"resposta\": \"O método científico deve ser aplicado de modo se obter resultados que independentem do senso comum humano.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(json.dumps(json.loads(result), indent=2, ensure_ascii=False))\n",
    "except:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e649f465-4f1a-4075-81b0-4876396e993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pergunta': 'Quem foi flagrado furtando um celular?',\n",
       "  'resposta': 'Um idoso.'},\n",
       " {'pergunta': 'Onde o furto ocorreu?',\n",
       "  'resposta': 'Dentro de um estúdio de tatuagem na Rua prefeito Chagas, no centro de Poços de Caldas.'},\n",
       " {'pergunta': 'Quando o furto aconteceu?',\n",
       "  'resposta': 'Na tarde desta terça-feira, dia 20 de agosto.'},\n",
       " {'pergunta': 'Qual era a razão pela qual o idoso frequentava o estúdio regularmente?',\n",
       "  'resposta': 'O idoso costuma ir ao estúdio pedir ajuda para comprar leite para o neto.'},\n",
       " {'pergunta': 'Além de pedir ajuda para leite, o que mais o idoso de aproximadamente 70 anos fazia?',\n",
       "  'resposta': 'Afirmava que pediam ajuda financeira aos comerciantes vizinhos.'},\n",
       " {'pergunta': 'O que os comerciantes faziam no momento que a ajuda foi solicitada?',\n",
       "  'resposta': 'Eles ajudavam sempre que podiam.'},\n",
       " {'pergunta': 'O que o tatuador estava fazendo quando o celular foi desaparecido?',\n",
       "  'resposta': 'O tatuador estava trabalhando e precisou usar o celular para entrar em contato com um cliente.'},\n",
       " {'pergunta': 'O que o tatuador fez após perder o celular?',\n",
       "  'resposta': 'Verificou as imagens das câmeras de segurança.'},\n",
       " {'pergunta': 'Como o idoso furtou o celular?',\n",
       "  'resposta': 'Aproveitou a situação em que uma funcionária estava servindo café e deixou o celular da empresa em cima do balcão.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5d404-3cfb-4344-9226-b0619e0bb0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df18c2d-1c95-4042-aec4-ecdf058cbc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e49774-6d3d-487d-9c0a-ddb374a6df19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca7c32-038b-4673-9d4f-8c01e345fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c892492-8799-488b-8279-68a183cb4efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93923fdc-b92c-4b84-b371-ff630d9ebcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b312b5-e66e-4d47-99cf-0909acf76a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5cbf0-43d8-4126-bd0c-b0a972942e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6fc9890-25f9-4d79-8771-02ec2153dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question = Quem saiu correndo do bar após ser preso?\n",
      "response = O jovem de 19 anos, que havia disparado a arma e repassado a arma a um segundo indivíduo, foi quem saiu correndo em direção à Rua Engenheiro Ubirajara Machado de Moraes após os disparos. No entanto, ele foi localizado e abordado pela Polícia Militar. Portanto, ninguém \"saiu correndo do bar após ser preso\", já que após serem abordados, os dois jovens foram presos.\n",
      "CPU times: user 10.2 s, sys: 6.15 s, total: 16.3 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "context = \"\"\"\n",
    "Dois jovens de 19 e 23 anos foram presos na noite desta sexta-feira,16, após disparos de arma de fogo próximo a um bar em Poços de Caldas.\n",
    "\n",
    "Segundo a Polícia Militar, durante uma operação na Avenida Marechal Castelo Branco, no bairro Jardim São Paulo, os policiais ouviram dois disparos de arma de fogo.\n",
    "\n",
    "Clique aqui e participe do grupo de notícias da Onda Poços no WhatsApp\n",
    "\n",
    "Em seguida foi recebida uma denúncia, via 190, relatando que o jovem de 19 anos trajando camiseta do flamengo havia efetuado dois disparos de arma de fogo próximo a um bar e que ele havia repassado a arma a um segundo indivíduo que trajava blusa com capuz preta e bermuda de cor preta, que após pegar arma saiu correndo em direção à Rua Engenheiro Ubirajara Machado de Moraes.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, o jovem de 19 anos já conhecido no meio policial por posse ilegal de arma e tráfico de drogas, foi localizado e abordado.\n",
    "\n",
    " \n",
    "\n",
    "Uma outra equipe da Polícia Militar recebeu denúncias, via 190, de que o segundo suspeito que havia fugido do local com a arma de fogo, a deixou em uma lixeira na Rua Abrieiro. A arma foi apreendida.\n",
    "\n",
    " \n",
    "\n",
    "Ainda segundo a PM, após capturar o primeiro suspeito, a equipe continuou o rastreamento para localizar o segundo envolvido. O jovem foi encontrado ao final da Rua Engenheiro Ubirajara Machado de Moraes com as mesmas vestes repassadas pela denúncia.\n",
    "\n",
    " \n",
    "A arma de fogo se tratava de um revólver calibre 38 marca Taurus, que foi apreendida. Ainda segundo a PM, no local e nas adjacências não foram localizadas vítimas e não foi detectado o local exato em que o projétil atingiu.\n",
    "\n",
    "A dupla foi presa, encaminhada à UPA e em seguida levada para a Delegacia de Polícia Civil.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Você é um assistente especializado em interpretação de texto\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": f\"Gere uma pergunta para o seguinte contexto:\\n```\\n{context}\\n```\\nPergunta:\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # model.generate(**tokenizer(\"test\", return_tensors=\"pt\").to(\"cuda\"))\n",
    "    question = llm.generate(messages)\n",
    "    print(\"question =\", question)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Dado o seguinte contexto ```{context}``` responda a pergunta: `{question}`.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fbd99d-e4ad-4791-b4f3-6ff8b2e526c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00c0e518-8837-4c5b-917c-0fc01a572f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.messages.system import SystemMessage\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def get_list(texto):\n",
    "    resultado = re.search(r'\\[([\\s\\S]*)\\]', texto)\n",
    "    if resultado:\n",
    "        conteudo = resultado.group(1)\n",
    "        return json.loads(f\"[{conteudo}]\")\n",
    "    else:\n",
    "        return \"Nenhum conteúdo encontrado entre os colchetes.\"\n",
    "\n",
    "\n",
    "\n",
    "def get_questions(context):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    prompt_text = f\"\"\"\n",
    "    Dado o seguinte contexto:\n",
    "    ```\n",
    "    {context}\n",
    "    ```\n",
    "    Escreva um conjunto de perguntas e respostas sobre o contexto.\n",
    "    As perguntas devem ser tais que, concatenando as respostas, seja possível recuperar o conteúdo completo do contexto.\n",
    "    As perguntas deve ser autocontidas; ou seja, um leitor deve entendê-la sem neessariamente conhecer o contexto\n",
    "\n",
    "    \n",
    "    \n",
    "    # Formato de saída:\n",
    "    O resultado deve ser parseavel em json:\n",
    "\n",
    "    ```\n",
    "    [\n",
    "     {{\"pergunta\": \"<<pergunta 1>>\", \"pergunta\": \"<<resposta 1>>\"}},\n",
    "     {{\"pergunta\": \"<<pergunta 2>>\", \"pergunta\": \"<<resposta 2>>\"}},\n",
    "     ...\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=prompt_text)\n",
    "    ])\n",
    "    \n",
    "    chain = chat_prompt | llm\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.invoke(dict(context=context))\n",
    "        preco_real = 5.7 * (0.15 * cb.prompt_tokens+ 0.6*cb.completion_tokens) / 1_000_000\n",
    "        print(f\"preco = {preco_real}\")\n",
    "        \n",
    "    return get_list(response.content), preco_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3da1a6-cc48-449d-8afe-c2b7505f6467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1155-450b-47b2-b1a0-839f39e3f003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c95fa-8293-4a86-bc79-c8581cd54a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cc9b34e-1fc8-4a9b-9af2-b342e725295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response = [\n",
      " {\"Qual foi o maior obstáculo para o estudo dos fenômenos cósmicos até algumas décadas atrás?\": \"A ausência de dados.\"},\n",
      " {\"O que ditava as características dos modelos elaborados antes das profundas alterações na cosmologia?\": \"Aspectos filosóficos.\"},\n",
      " {\"Qual foi o objetivo de Einstein ao adicionar a constante cosmológica em suas equações da Relatividade Geral?\": \"Descrever um universo estático, em equilíbrio (instável).\"},\n",
      " {\"Quem abandonou a premissa de um universo estático e qual foi sua contribuição?\": \"Alexander Friedmann, que encontrou a descrição analítica de um universo dinâmico.\"},\n",
      " {\"O que as soluções das equações de campo da Relatividade Geral de Friedmann descrevem?\": \"A evolução de um universo proveniente de uma singularidade.\"},\n",
      " {\"Quem cunhou o termo 'átomo primordial' para descrever o modelo dinâmico do universo de Friedmann?\": \"Georges Lemaître.\"},\n",
      " {\"Em que ano Georges Lemaître encontrou uma solução dinâmica para a evolução do universo?\": \"1927.\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "A ciência da cosmologia sofreu profundas alterações nas últimas décadas. Até então,\n",
    "a ausência de dados era o maior obstáculo para o estudo dos fenômenos cósmicos. Muitas\n",
    "das vezes, eram os aspectos filosóficos que ditavam as características de modelos elaborados.\n",
    "Como exemplo, temos o acréscimo da constante cosmológica por Einstein em suas equações\n",
    "da Relatividade Geral com o objetivo de descrição de um universo estático, em equilíbrio\n",
    "(instável)(EINSTEIN, 1918). Em contrapartida, Alexander Friedmann abandonou a premissa de um universo estático e encontrou a descrição analítica de um universo dinâmico\n",
    "perante a solução das equações de campo da Relatividade Geral (FRIEDMANN, 1922;\n",
    "FRIEDMANN, 1924). Tal modelo descreve a evolução de um universo proveniente de uma\n",
    "singularidade, que posteriormente seria cunhado por Lemaître como o átomo primordial.\n",
    "Uma solução dinâmica para a evolução do universo foi encontrada também por Georges\n",
    "Lemaître em 1927 (LEMAÎTRE, 1927).\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Dado o seguinte contexto:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "Escreva um conjunto de perguntas e respostas sobre o contexto. As perguntas devem ser tais que, concatenando as respostas, seja possível recuperar o conteúdo completo do contexto\n",
    "\n",
    "# Formato de saída:\n",
    "\n",
    "[\n",
    " {{\"pergunta_1\": \"resposta_1\"}},\n",
    " {{\"pergunta_2\": \"resposta_2\"}},\n",
    " {{\"pergunta_3\": \"resposta_3\"}},\n",
    " ...\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\"response =\", completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e5ce2dd-11d5-4702-9a6b-4228ea15408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preco = 0.0015313049999999997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pergunta': 'Quais foram as principais mudanças na ciência da cosmologia nas últimas décadas?',\n",
       "  'resposta': 'A ciência da cosmologia sofreu profundas alterações, com a superação da ausência de dados que antes era o maior obstáculo para o estudo dos fenômenos cósmicos.'},\n",
       " {'pergunta': 'Como os aspectos filosóficos influenciaram os modelos cosmológicos anteriormente?',\n",
       "  'resposta': 'Os aspectos filosóficos muitas vezes ditavam as características de modelos elaborados, como no caso do acréscimo da constante cosmológica por Einstein para descrever um universo estático.'},\n",
       " {'pergunta': 'Qual foi a contribuição de Einstein para a cosmologia?',\n",
       "  'resposta': 'Einstein acrescentou a constante cosmológica em suas equações da Relatividade Geral com o objetivo de descrever um universo estático em equilíbrio.'},\n",
       " {'pergunta': 'Quem foi Alexander Friedmann e qual foi sua contribuição para a cosmologia?',\n",
       "  'resposta': 'Alexander Friedmann abandonou a ideia de um universo estático e encontrou a descrição analítica de um universo dinâmico com base nas equações de campo da Relatividade Geral.'},\n",
       " {'pergunta': 'Como Friedmann descreveu a evolução do universo?',\n",
       "  'resposta': 'Friedmann descreveu a evolução do universo como proveniente de uma singularidade, que mais tarde seria denominado por Lemaître como o átomo primordial.'},\n",
       " {'pergunta': 'Qual outra solução dinâmica para a evolução do universo foi encontrada e por quem?',\n",
       "  'resposta': 'Georges Lemaître também encontrou uma solução dinâmica para a evolução do universo em 1927.'}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "context = \"\"\"\n",
    "A ciência da cosmologia sofreu profundas alterações nas últimas décadas. Até então,\n",
    "a ausência de dados era o maior obstáculo para o estudo dos fenômenos cósmicos. Muitas\n",
    "das vezes, eram os aspectos filosóficos que ditavam as características de modelos elaborados.\n",
    "Como exemplo, temos o acréscimo da constante cosmológica por Einstein em suas equações\n",
    "da Relatividade Geral com o objetivo de descrição de um universo estático, em equilíbrio\n",
    "(instável)(EINSTEIN, 1918). Em contrapartida, Alexander Friedmann abandonou a premissa de um universo estático e encontrou a descrição analítica de um universo dinâmico\n",
    "perante a solução das equações de campo da Relatividade Geral (FRIEDMANN, 1922;\n",
    "FRIEDMANN, 1924). Tal modelo descreve a evolução de um universo proveniente de uma\n",
    "singularidade, que posteriormente seria cunhado por Lemaître como o átomo primordial.\n",
    "Uma solução dinâmica para a evolução do universo foi encontrada também por Georges\n",
    "Lemaître em 1927 (LEMAÎTRE, 1927).\n",
    "\"\"\"\n",
    "\n",
    "response, preco_real = get_questions(context)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "620cb098-156a-4b4f-9410-1c700d18d217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3226fc34bcbc470391e19ec50b7631a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd41b8fd51054a65b3fe04fa0e23878f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56398aad63a4ac0a761cffbf5a90247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.95M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100c4a09bbe940759eee87ada1a6ef6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf84f6483d7456ba43c1099e3582046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7a431ae5ed42938060465b4f1fa91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08cbf4c0ffd43b8a7861ee80a6bec7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "columns = ['id', 'context', 'question','response']\n",
    "train_dataset = dataset[\"train\"].shuffle(42).select(range(5000)).select_columns(columns)\n",
    "test_dataset = dataset[\"validation\"].shuffle(42).select(range(100)).select_columns(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19610fbb-fbf7-42e0-85b6-396c2dd5baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f59e7ab2-d718-4574-ae0f-20a5ceef747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como nas outras línguas românicas ocidentais, a principal expressão plural é o sufixo -s, que pode criar alternâncias morfológicas semelhantes às encontradas na inflexão de gênero, embora mais raramente. O mais importante é o acréscimo, antes de certos grupos de consoantes, a um fenómeno fonético que não afeta as formas femininas: el pols / els polsos (&quot;o pulso&quot; / &quot;os pulsos&quot;) vs. la pols / les pols ( &quot;a poeira&quot; / &quot;as poeiras&quot;).\n",
      "\n",
      "preco = 0.0010225800000000001\n",
      "\n",
      "[{'pergunta': 'Qual é a principal expressão plural nas línguas românicas ocidentais?', 'resposta': 'O sufixo -s.'}, {'pergunta': 'O que o sufixo -s pode criar nas línguas românicas ocidentais?', 'resposta': 'Alternâncias morfológicas semelhantes às encontradas na inflexão de gênero.'}, {'pergunta': 'O fenômeno fonético que afeta a formação do plural nas línguas românicas ocidentais tem impacto nas formas femininas?', 'resposta': 'Não, ele não afeta as formas femininas.'}, {'pergunta': 'Dê um exemplo de como o sufixo -s é aplicado a palavras masculinas e femininas.', 'resposta': \"Por exemplo, 'el pols' se torna 'els polsos' (o pulso / os pulsos) e 'la pols' se torna 'les pols' (a poeira / as poeiras).\"}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e012ccff-9ce9-460c-9faa-aa5874918756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in test_dataset:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ce232-3a82-4990-993d-65191bae29de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a563f31-35be-4da1-bfd7-5d73a0b28d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 17\n",
      "\tPrompt Tokens: 8\n",
      "\tCompletion Tokens: 9\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54f10658-6046-49c9-b918-1577b2e1f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
