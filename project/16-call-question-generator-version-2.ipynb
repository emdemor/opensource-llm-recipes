{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8976cac-8c2d-44f7-955d-40069317fb18",
   "metadata": {},
   "source": [
    "# Utilização de LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc86a87a-482f-4303-8d9d-d0df123f017c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:03.447117Z",
     "iopub.status.busy": "2024-12-22T22:32:03.445155Z",
     "iopub.status.idle": "2024-12-22T22:32:03.461265Z",
     "shell.execute_reply": "2024-12-22T22:32:03.460735Z",
     "shell.execute_reply.started": "2024-12-22T22:32:03.446998Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=FutureWarning, message=\".*TRANSFORMERS_CACHE.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*use_reentrant parameter should be passed explicitly.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b702c-fa21-4c82-a531-2538593ad3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:05.469664Z",
     "iopub.status.busy": "2024-12-22T22:32:05.467133Z",
     "iopub.status.idle": "2024-12-22T22:32:09.135598Z",
     "shell.execute_reply": "2024-12-22T22:32:09.134958Z",
     "shell.execute_reply.started": "2024-12-22T22:32:05.469547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.3\n",
      "peft version: 0.12.0\n",
      "accelerate version: 0.34.2\n",
      "datasets version: 2.21.0\n",
      "trl version: 0.10.1\n",
      "transformers version: 4.46.1\n",
      "Device name: 'NVIDIA GeForce RTX 4060 Ti'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Ti', major=8, minor=9, total_memory=16059MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import datasets\n",
    "import peft\n",
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(\n",
    "    f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\"\n",
    ")\n",
    "print(\n",
    "    \"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25699ee-09f7-4ee2-952d-adda0691d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513a330f-7957-435a-92f8-6ccf87a66926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:09.136928Z",
     "iopub.status.busy": "2024-12-22T22:32:09.136585Z",
     "iopub.status.idle": "2024-12-22T22:32:09.830632Z",
     "shell.execute_reply": "2024-12-22T22:32:09.830103Z",
     "shell.execute_reply.started": "2024-12-22T22:32:09.136914Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, PeftModel, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867fb4c8-6542-4d96-84de-92bf5859ec99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:09.831504Z",
     "iopub.status.busy": "2024-12-22T22:32:09.831221Z",
     "iopub.status.idle": "2024-12-22T22:32:09.836073Z",
     "shell.execute_reply": "2024-12-22T22:32:09.835581Z",
     "shell.execute_reply.started": "2024-12-22T22:32:09.831489Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages, max_new_tokens=2000, **kwargs):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs[\"attention_mask\"] = model_inputs[\"attention_mask\"].to(\n",
    "            model_inputs[\"input_ids\"].device\n",
    "        )\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs[\"attention_mask\"],\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09761d40-2d18-4f3e-ae49-e44aee9f3838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:09.837207Z",
     "iopub.status.busy": "2024-12-22T22:32:09.836999Z",
     "iopub.status.idle": "2024-12-22T22:32:09.840310Z",
     "shell.execute_reply": "2024-12-22T22:32:09.839873Z",
     "shell.execute_reply.started": "2024-12-22T22:32:09.837192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_bf16_supported():\n",
    "    # compute_dtype = torch.bfloat16\n",
    "    # attn_implementation = \"flash_attention_2\"\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    # attn_implementation = 'eager'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61512dd0-3453-4aa5-bcfc-aaeea453ccb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:32:09.841008Z",
     "iopub.status.busy": "2024-12-22T22:32:09.840852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176bb2876faa43a99467402545d18584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f718fc2deebc4fcd9ea81563d496e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "model_id = \"emdemor/question-generator-v2\"\n",
    "commit_hash = None\n",
    "\n",
    "\n",
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    revision=commit_hash,\n",
    ")\n",
    "\n",
    "# # adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "def set_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=commit_hash)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = set_tokenizer(model_id)\n",
    "\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39874dd7-0942-4af7-ab0b-cc1a15729a66",
   "metadata": {},
   "source": [
    "# Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3a7fc-fbd9-47ae-8c76-62dd12988a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def generate_question(context, temperature=0.01):\n",
    "    return llm.generate([{\"content\": f\"{context}\", \"role\": \"user\"}], temperature=temperature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcce1c32-d07c-4e8e-94fb-c015e6486e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:08:02.373375Z",
     "iopub.status.busy": "2024-09-14T05:08:02.372982Z",
     "iopub.status.idle": "2024-09-14T05:08:22.059123Z",
     "shell.execute_reply": "2024-09-14T05:08:22.058614Z",
     "shell.execute_reply.started": "2024-09-14T05:08:02.373358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"Onde o narrador estava indo na noite em questão?\",\n",
      "        \"resposta\": \"O narrador estava indo do Engenho Novo para a cidade.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o narrador encontrou no trem da Central?\",\n",
      "        \"resposta\": \"O narrador encontrou um rapaz do bairro que conhecia de vista e de chapéu.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Como o rapaz se apresentou ao narrador?\",\n",
      "        \"resposta\": \"O rapaz cumprimentou o narrador, sentou-se ao seu lado e falou sobre a Lua e os ministros.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o rapaz fez após falar sobre a Lua e os ministros?\",\n",
      "        \"resposta\": \"O rapaz acabou recitando versos para o narrador.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual era a duração da viagem e como se relacionava com a leitura dos versos?\",\n",
      "        \"resposta\": \"A viagem era curta, e os versos poderiam ser que não fossem inteiramente maus.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu quando o narrador se fechou os olhos durante a leitura dos versos?\",\n",
      "        \"resposta\": \"Quando o narrador se fechou os olhos três ou quatro vezes, o rapaz interrompeu a leitura e meteu os versos no bolso.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Esses interlocutores avaliam que transferir o vice-presidente Geraldo Alckmin do Ministério do Desenvolvimento, Indústria, Comércio e Serviços para a vaga, como cogitado, seria colocar seu vice numa posição de intermediar constantes conflitos com o governo e com o partido do presidente, o PT.\n",
    "\"\"\"\n",
    "\n",
    "result = generate_question(context, temperature=0.01)\n",
    "print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f9ac52-ee55-47ac-9fcd-dad54581747a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:08:39.889143Z",
     "iopub.status.busy": "2024-09-14T05:08:39.888495Z",
     "iopub.status.idle": "2024-09-14T05:09:00.928424Z",
     "shell.execute_reply": "2024-09-14T05:09:00.927869Z",
     "shell.execute_reply.started": "2024-09-14T05:08:39.889111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"Onde o narrador estava indo na noite em questão?\",\n",
      "        \"resposta\": \"O narrador estava indo do Engenho Novo.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o narrador encontrou no trem da Central?\",\n",
      "        \"resposta\": \"O narrador encontrou um rapaz do bairro que ele conhece de vista e de chapéu.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Como o rapaz se apresentou ao narrador?\",\n",
      "        \"resposta\": \"O rapaz cumprimentou o narrador, sentou-se ao seu lado e falou sobre a Lua e os ministros.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o rapaz fez depois de falar sobre a Lua e os ministros?\",\n",
      "        \"resposta\": \"O rapaz acabou recitando versos para o narrador.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual era a duração da viagem e como se relacionava com os versos?\",\n",
      "        \"resposta\": \"A viagem era curta, e os versos poderiam ser que não fossem inteiramente maus.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu quando o narrador se sentiu cansado durante a viagem?\",\n",
      "        \"resposta\": \"Quando o narrador se sentiu cansado, ele fechou os olhos três ou quatro vezes.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual foi a consequência do narrador fechar os olhos?\",\n",
      "        \"resposta\": \"O rapaz interrompeu a leitura e meteu os versos no bolso.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result = generate_question(context, temperature=0.3)\n",
    "print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2234b55-e1b6-4c19-b951-b3d0143f8f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:09:00.929435Z",
     "iopub.status.busy": "2024-09-14T05:09:00.929237Z",
     "iopub.status.idle": "2024-09-14T05:09:19.772152Z",
     "shell.execute_reply": "2024-09-14T05:09:19.771659Z",
     "shell.execute_reply.started": "2024-09-14T05:09:00.929419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"Onde o narrador estava vindo de uma cidade?\",\n",
      "        \"resposta\": \"O narrador estava vindo da cidade para o Engenho Novo.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o narrador encontrou no trem da Central?\",\n",
      "        \"resposta\": \"O narrador encontrou um rapaz aqui do bairro, que ele conhece de vista e de chapéu.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Como o rapaz se apresentou ao narrador?\",\n",
      "        \"resposta\": \"O rapaz cumprimentou-se, sentou-se ao pé de mim, falou da Lua e dos ministros, e acabou recitando versos.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual era a duração da viagem mencionada no contexto?\",\n",
      "        \"resposta\": \"A viagem era curta.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu quando o narrador se fechou os olhos durante a leitura?\",\n",
      "        \"resposta\": \"Quando o narrador se fechou os olhos três ou quatro vezes, o rapaz interrompeu a leitura e meteu os versos no bolso.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o narrador pensava sobre os versos que o rapaz recitou?\",\n",
      "        \"resposta\": \"O narrador pensou que os versos poderiam ser que não fossem inteiramente maus.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result = generate_question(context, temperature=0.7)\n",
    "print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "906f795a-5a8e-454c-bcc8-2d799d1ce79d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:09:19.772843Z",
     "iopub.status.busy": "2024-09-14T05:09:19.772692Z",
     "iopub.status.idle": "2024-09-14T05:09:35.952067Z",
     "shell.execute_reply": "2024-09-14T05:09:35.951528Z",
     "shell.execute_reply.started": "2024-09-14T05:09:19.772829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu na noite mencionada?\",\n",
      "        \"resposta\": \"Encontrei um rapaz no trem da Central, que eu conheço de vista e de chapéu.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o rapaz fez ao encontrar o narrador?\",\n",
      "        \"resposta\": \"O rapaz cumprimentou o narrador, sentou-se ao seu lado e falou sobre a Lua e os ministros.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o rapaz fez ao narrador durante a viagem?\",\n",
      "        \"resposta\": \"Ele começou a recitar versos ao narrador.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual foi a razão pela qual o narrador suspeitou dos versos?\",\n",
      "        \"resposta\": \"O narrador suspeitou que os versos não fossem inteiramente maus, pois a viagem era curta.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu quando o narrador se fechou os olhos?\",\n",
      "        \"resposta\": \"Quando o narrador se fechou os olhos três ou quatro vezes, o rapaz interrompeu a leitura e colocou os versos no bolso.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result = generate_question(context, temperature=0.9)\n",
    "print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbfab2b4-13fd-4f5b-a75c-d50b7cd5d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:12:07.046951Z",
     "iopub.status.busy": "2024-09-14T05:12:07.046057Z",
     "iopub.status.idle": "2024-09-14T05:12:24.900263Z",
     "shell.execute_reply": "2024-09-14T05:12:24.899779Z",
     "shell.execute_reply.started": "2024-09-14T05:12:07.046887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"pergunta\": \"Onde eu encontra o rapaz que cumprem apresentação no trem da Central?\",\n",
      "        \"resposta\": \"No bairro do Engenho Novo.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Com quem eu já conheci, além do rapaz mencionado?\",\n",
      "        \"resposta\": \"Pela vista e pela sua faixa.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que o rapaz falava enquanto cumprimentava o autor?\",\n",
      "        \"resposta\": \"- Da Lua e dos ministros.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"O que aconteceu quando o autor se fechou os olhos no trem?\",\n",
      "        \"resposta\": \"- Os versos recitados a ele foram colocados no bolso.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual foi a consequência de me fechar os olhos no trajeto do trem?\",\n",
      "        \"resposta\": \"- O rapaz interrompeu a leitura e guardou seus versos.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"A viagem do trem era de que características?\",\n",
      "        \"resposta\": \"- A viagem foi curta.\"\n",
      "    },\n",
      "    {\n",
      "        \"pergunta\": \"Qual era a impressão que o autor teve sobre os versos do rapaz, considerando a ocorrência do relato?\",\n",
      "        \"resposta\": \"- Não se sabe se os versos eram inteiramente maus ou não.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result = generate_question(context, temperature=1.2)\n",
    "print(json.dumps(json.loads(result), indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a01a8a-5fa6-4b61-9c27-b1a80312db44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
