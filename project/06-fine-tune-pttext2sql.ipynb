{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/78609617/huggingface-transformer-train-function-throwing-device-in-mac-m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Phi3 for Text2SQL \n",
    "\n",
    "Este notebook demonstra como ajustar o modelo Phi3 para a tarefa de Text2SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação de Dependências\n",
    "\n",
    "Nesta célula, instalamos todas as bibliotecas necessárias para o ajuste fino do modelo. As bibliotecas incluem bitsandbytes, transformers, peft, accelerate, datasets, trl, entre outras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação das Versões das Bibliotecas\n",
    "\n",
    "Após a instalação, verificamos as versões das bibliotecas para garantir que foram instaladas corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:43:18.768074Z",
     "iopub.status.busy": "2024-12-22T21:43:18.767856Z",
     "iopub.status.idle": "2024-12-22T21:43:18.772468Z",
     "shell.execute_reply": "2024-12-22T21:43:18.772075Z",
     "shell.execute_reply.started": "2024-12-22T21:43:18.768058Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:43:21.319486Z",
     "iopub.status.busy": "2024-12-22T21:43:21.319262Z",
     "iopub.status.idle": "2024-12-22T21:43:25.445321Z",
     "shell.execute_reply": "2024-12-22T21:43:25.444830Z",
     "shell.execute_reply.started": "2024-12-22T21:43:21.319471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.3\n",
      "peft version: 0.12.0\n",
      "accelerate version: 0.34.2\n",
      "datasets version: 2.21.0\n",
      "trl version: 0.10.1\n",
      "Device name: 'NVIDIA GeForce RTX 4060 Ti'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Ti', major=8, minor=9, total_memory=16059MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\n",
    "print(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:54:27.877020Z",
     "iopub.status.busy": "2024-12-22T21:54:27.876595Z",
     "iopub.status.idle": "2024-12-22T21:54:28.225685Z",
     "shell.execute_reply": "2024-12-22T21:54:28.223944Z",
     "shell.execute_reply.started": "2024-12-22T21:54:27.877002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 22 21:54:28 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P8              5W /  165W |     541MiB /  16380MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de Bibliotecas e Configuração de Diretórios\n",
    "\n",
    "Esta célula importa as bibliotecas necessárias para a análise de dados e configura os diretórios de entrada e saída. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T21:54:30.201697Z",
     "iopub.status.busy": "2024-12-22T21:54:30.201462Z",
     "iopub.status.idle": "2024-12-22T21:54:35.220133Z",
     "shell.execute_reply": "2024-12-22T21:54:35.219515Z",
     "shell.execute_reply.started": "2024-12-22T21:54:30.201679Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações\n",
    "\n",
    "Vamos setar algumas variáveis de ambiente, algumas secret keys e o model id à ser utilizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:54:37.521532Z",
     "iopub.status.busy": "2024-12-22T21:54:37.520762Z",
     "iopub.status.idle": "2024-12-22T21:54:42.762018Z",
     "shell.execute_reply": "2024-12-22T21:54:42.761507Z",
     "shell.execute_reply.started": "2024-12-22T21:54:37.521513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "LOCAL_MODELPATH = \"data/\" + model_name.lower().replace(\"/\",\"-\").replace(\".\",\"_\")\n",
    "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:54:48.926308Z",
     "iopub.status.busy": "2024-12-22T21:54:48.925983Z",
     "iopub.status.idle": "2024-12-22T21:54:48.930635Z",
     "shell.execute_reply": "2024-12-22T21:54:48.930131Z",
     "shell.execute_reply.started": "2024-12-22T21:54:48.926282Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:54:51.680467Z",
     "iopub.status.busy": "2024-12-22T21:54:51.680285Z",
     "iopub.status.idle": "2024-12-22T21:54:51.683679Z",
     "shell.execute_reply": "2024-12-22T21:54:51.683268Z",
     "shell.execute_reply.started": "2024-12-22T21:54:51.680453Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    try:\n",
    "        checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        return latest_checkpoint\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:55:06.647208Z",
     "iopub.status.busy": "2024-12-22T21:55:06.644922Z",
     "iopub.status.idle": "2024-12-22T21:55:06.654352Z",
     "shell.execute_reply": "2024-12-22T21:55:06.653950Z",
     "shell.execute_reply.started": "2024-12-22T21:55:06.647121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_name() == 'NVIDIA GeForce RTX 2060 SUPER':\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'eager'\n",
    "elif torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'eager'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:55:09.036333Z",
     "iopub.status.busy": "2024-12-22T21:55:09.036079Z",
     "iopub.status.idle": "2024-12-22T21:55:17.307599Z",
     "shell.execute_reply": "2024-12-22T21:55:17.307081Z",
     "shell.execute_reply.started": "2024-12-22T21:55:09.036315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bbbc7c4d424182bc4134b0a575c595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6a61cb3711491ea8431b904d953c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfb162b492148d5a2fd92480860ae49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced588fb18004a9d85d8d8118427dfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True, device_map=\"auto\")\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T21:55:30.668768Z",
     "iopub.status.busy": "2024-12-22T21:55:30.668517Z",
     "iopub.status.idle": "2024-12-22T22:02:41.043906Z",
     "shell.execute_reply": "2024-12-22T22:02:41.043346Z",
     "shell.execute_reply.started": "2024-12-22T21:55:30.668753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73da3e5467e64043925afbdb013af0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93320ff16545679313c21eeb561750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9c4fd1619d4265948646ba01c90147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:11:50.710340Z",
     "iopub.status.busy": "2024-12-22T22:11:50.710000Z",
     "iopub.status.idle": "2024-12-22T22:11:50.714426Z",
     "shell.execute_reply": "2024-12-22T22:11:50.714037Z",
     "shell.execute_reply.started": "2024-12-22T22:11:50.710318Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs['attention_mask'],\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:04:26.643276Z",
     "iopub.status.busy": "2024-12-22T22:04:26.642811Z",
     "iopub.status.idle": "2024-12-22T22:04:49.188156Z",
     "shell.execute_reply": "2024-12-22T22:04:49.187650Z",
     "shell.execute_reply.started": "2024-12-22T22:04:26.643259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 0 ns, total: 22.6 s\n",
      "Wall time: 22.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pelé is an Italian-American professional boxer who represented Brazil in the 1980 Olympics, where he represented Brazil at its biggest sporting event in his native country. Pelé is one of the biggest names in the world of professional boxing, having fought in several tournaments around the world since he was born on February 3, 1966. He is also known for representing Brazil in the Olympics, where he won the silver medal in the super-heavyweight division at the 1980 Summer Olympics in Moscow. Pelé\\'s impressive career includes a record of more than 28 international fights, 182 wins and two losses, multiple knockout finishes, and many successful defenses. His victories are widely regarded as some of the greatest fights in professional boxing history. Pelé has been inducted into the International Boxing Hall of Fame and the Ring of Honor Hall of Fame, among other professional boxing bodies. \\n\\nOne of the most talked-about topics surrounding Pelé in recent years has been his ongoing feud with American boxer Joe Lewis, the 2008 Olympic gold medal winner, who was said to be considering fighting back against Pelé. Although recent news has not mentioned the situation, some sources suggest that both of them have expressed their disagreement at one point.\\n\\nPelé\\'s boxing career has been marred with several notable events throughout its history. The first and most famous fought in 1969 in Mexico, where he fought against former world super-middleweight champion and undefeated boxing champion Oscar Jovarian. In this bout, Pelé won by decision from a 12-8 score in the first round. However, his career took a turn for the worse in 1970 as he faced off against legendary Italian boxer Carlos \"Los Colubro\" Vidal. Pelé managed to score the first knockout of his career, earning 10-8, before being knocked out in round seven. Pelé would go on to defeat Vidal two years later in the finals of the World Boxing Union\\'s championship.\\n\\nSince then, Pelé has become a prominent figure not only in Brazilian boxing but also around the world. In 2018, he announced he had been cleared of any legal issues concerning a lawsuit he initially filed against the World Boxing Organization. The lawsuit stemmed from Pelé\\'s own alleged involvement in a failed drug case that was shut down by the World Anti-Doping Agency.\\n\\nIn 1980, Pelé fought against fellow Brazilian Beto Moutinho but never'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "prompt = \"Quem é Pelé?\"\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"Olá. Você é um expert em geografia e vai me ajudar a responder algumas questões.\"},\n",
    "    # {\"role\": \"assistent\", \"content\": \"Tudo bem! Como posso ajudar?\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "llm.generate(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:11:54.216122Z",
     "iopub.status.busy": "2024-12-22T22:11:54.215637Z",
     "iopub.status.idle": "2024-12-22T22:12:03.605689Z",
     "shell.execute_reply": "2024-12-22T22:12:03.605323Z",
     "shell.execute_reply.started": "2024-12-22T22:11:54.216104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b36e8b5e38543dfaf9c4aa91652ba94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f4e0c01e5d4073917432690745bdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffa6874b5cd44b38f8d9ea7e556b609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fb79bb9c4f45b8b024ba0836dd78fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_dataset_chatml(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Pergunta: {row['pergunta']}\\nContexto: {row['contexto']}\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"{row['resposta']}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(\"emdemor/sql-create-context-pt\", split=\"train\").shuffle(seed=42).select(range(200))\n",
    "dataset_chatml = dataset.map(format_dataset_chatml).train_test_split(test_size=0.10, seed=1234)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:12:19.140939Z",
     "iopub.status.busy": "2024-12-22T22:12:19.140740Z",
     "iopub.status.idle": "2024-12-22T22:12:19.144451Z",
     "shell.execute_reply": "2024-12-22T22:12:19.144047Z",
     "shell.execute_reply.started": "2024-12-22T22:12:19.140925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pergunta': 'Que equipa visitante terminou com um resultado final de 18-40?',\n",
       " 'contexto': 'CREATE TABLE table_name_77 (visiting_team VARCHAR, final_score VARCHAR)',\n",
       " 'resposta': 'SELECT visiting_team FROM table_name_77 WHERE final_score = \"18-40\"',\n",
       " 'text': '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\nPergunta: Que equipa visitante terminou com um resultado final de 18-40?\\nContexto: CREATE TABLE table_name_77 (visiting_team VARCHAR, final_score VARCHAR)<|im_end|>\\n<|im_start|>assistant\\nSELECT visiting_team FROM table_name_77 WHERE final_score = \"18-40\"<|im_end|>\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chatml[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFTTrainer` é uma classe que simplifica o processo de fine-tuning de modelos de linguagem. Ela gerencia o treinamento e avaliação do modelo, tratando de diversos detalhes técnicos que normalmente exigiriam muito código para serem configurados manualmente.\n",
    "\n",
    "### Principais Funcionalidades\n",
    "\n",
    "1. **Configuração do Treinamento**:\n",
    "   - Facilita a configuração de hiperparâmetros como taxa de aprendizado, tamanho do lote, número de épocas, etc.\n",
    "   - Permite a integração com frameworks de logging como WandB, TensorBoard, entre outros.\n",
    "\n",
    "2. **Gerenciamento de Dados**:\n",
    "   - Gerencia o carregamento e pré-processamento dos dados de treinamento e validação.\n",
    "   - Suporta diferentes formatos de dados e tokenizadores.\n",
    "\n",
    "3. **Treinamento e Avaliação**:\n",
    "   - Implementa loops de treinamento e avaliação, com suporte a múltiplas GPUs e distribuído.\n",
    "   - Realiza checkpoints automáticos do modelo, salvando os melhores resultados com base em métricas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Configuração do `SFTConfig`\n",
    "A classe `SFTConfig` é usada para definir diversas configurações para o treinamento do modelo. Aqui estão os parâmetros principais:\n",
    "\n",
    "- `dataset_text_field`: Campo de texto do dataset.\n",
    "- `max_seq_length`: Comprimento máximo da sequência.\n",
    "- `output_dir`: Diretório onde o modelo treinado será salvo.\n",
    "- `eval_strategy`: Estratégia de avaliação, como \"steps\" (passos) ou \"epoch\" (épocas).\n",
    "- `do_eval`: Se a avaliação deve ser feita durante o treinamento.\n",
    "- `optim`: Otimizador a ser usado, neste caso `adamw_torch`.\n",
    "- `per_device_train_batch_size`: Tamanho do lote de treinamento por dispositivo.\n",
    "- `gradient_accumulation_steps`: Número de passos de acumulação de gradientes antes de realizar uma atualização do modelo.\n",
    "- `per_device_eval_batch_size`: Tamanho do lote de avaliação por dispositivo.\n",
    "- `log_level`: Nível de log, neste caso \"debug\".\n",
    "- `save_strategy`: Estratégia de salvamento, como \"steps\" ou \"epoch\".\n",
    "- `logging_steps`: Número de passos entre registros de log.\n",
    "- `learning_rate`: Taxa de aprendizado.\n",
    "- `fp16`: Se deve usar precisão mista (16 bits).\n",
    "- `bf16`: Se deve usar bfloat16, dependendo do suporte CUDA.\n",
    "- `eval_steps`: Número de passos entre avaliações.\n",
    "- `num_train_epochs`: Número de épocas de treinamento.\n",
    "- `warmup_ratio`: Proporção de warmup (aquecimento) da taxa de aprendizado.\n",
    "- `lr_scheduler_type`: Tipo de agendador de taxa de aprendizado, neste caso \"linear\". <a href=\"#01\">[Apêndice A]</a>\n",
    "- `report_to`: Onde reportar os resultados do treinamento, como \"none\" (nenhum).\n",
    "- `seed`: Semente para replicabilidade.\n",
    "\n",
    "#### Configuração do `LoraConfig`\n",
    "A classe `LoraConfig` é usada para definir configurações específicas de Low-Rank Adaptation (LoRA):\n",
    "\n",
    "- `r`: Parâmetro de rank da decomposição.\n",
    "- `lora_alpha`: Parâmetro de escala de LoRA.\n",
    "- `lora_dropout`: Dropout aplicado nas camadas LoRA.\n",
    "- `task_type`: Tipo de tarefa, como `TaskType.CAUSAL_LM`.\n",
    "- `target_modules`: Módulos alvo para LoRA.\n",
    "\n",
    "#### Inicialização do `SFTTrainer`\n",
    "A classe `SFTTrainer` é inicializada com as seguintes configurações:\n",
    "\n",
    "- `model`: O modelo a ser treinado.\n",
    "- `train_dataset`: Dataset de treinamento.\n",
    "- `eval_dataset`: Dataset de avaliação.\n",
    "- `args`: Configurações de treinamento (instância de `SFTConfig`).\n",
    "- `peft_config`: Configurações de LoRA (instância de `LoraConfig`).\n",
    "- `tokenizer`: Tokenizador a ser usado.\n",
    "\n",
    "### Parâmetros para Mudar a Frequência de Checkpoints\n",
    "\n",
    "Para alterar a frequência de checkpoints, você pode ajustar os seguintes parâmetros em `SFTConfig`:\n",
    "\n",
    "1. **Salvar Checkpoints por Passos**:\n",
    "   - **Parâmetro**: `save_strategy`\n",
    "   - **Valor**: \"steps\"\n",
    "   - **Parâmetro Adicional**: `save_steps`\n",
    "   - **Descrição**: Define a frequência de salvamento em passos.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         save_strategy=\"steps\",\n",
    "         save_steps=100,  # Salvando a cada 100 passos\n",
    "         ...\n",
    "     )\n",
    "     ```\n",
    "\n",
    "2. **Salvar Checkpoints por Épocas**:\n",
    "   - **Parâmetro**: `save_strategy`\n",
    "   - **Valor**: \"epoch\"\n",
    "   - **Descrição**: Define que o salvamento será feito ao final de cada época.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "         ...\n",
    "     )\n",
    "     ```\n",
    "\n",
    "3. **Frequência de Avaliação**:\n",
    "   - **Parâmetro**: `eval_steps`\n",
    "   - **Descrição**: Define a frequência de avaliações em passos.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         eval_steps=50,  # Avaliando a cada 50 passos\n",
    "         ...\n",
    "     )\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:14:15.838217Z",
     "iopub.status.busy": "2024-12-22T22:14:15.837892Z",
     "iopub.status.idle": "2024-12-22T22:14:15.855260Z",
     "shell.execute_reply": "2024-12-22T22:14:15.854745Z",
     "shell.execute_reply.started": "2024-12-22T22:14:15.838196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 ms, sys: 0 ns, total: 2.19 ms\n",
      "Wall time: 13.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_dir = \"models/FT-01\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias? TensorBoard?\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T22:14:48.651250Z",
     "iopub.status.busy": "2024-12-22T22:14:48.650883Z",
     "iopub.status.idle": "2024-12-22T22:15:24.125998Z",
     "shell.execute_reply": "2024-12-22T22:15:24.125414Z",
     "shell.execute_reply.started": "2024-12-22T22:14:48.651233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72f2ec1d49b497b946e710fe3701462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239e6b1eeb4b45fd9ada0d096063366e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.962500</td>\n",
       "      <td>1.232542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.084500</td>\n",
       "      <td>1.069918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.007000</td>\n",
       "      <td>1.057704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 20\n",
      "Saving checkpoint at step 30\n",
      "Saving checkpoint at step 40\n",
      "Saving checkpoint at step 50\n",
      "Saving checkpoint at step 60\n",
      "Saving checkpoint at step 69\n"
     ]
    }
   ],
   "source": [
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento com EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias? TensorBoard?\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/69 00:30 < 01:38, 0.52 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.968371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.711381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.366761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.010422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.728735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.505082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.397608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.329260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.278901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 10\n",
      "Early stopping at step 18 with best eval_loss = 1.505082368850708\n"
     ]
    }
   ],
   "source": [
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=3, early_stopping_threshold=0.02):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.best_metric = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n",
    "        current_metric = metrics.get(\"eval_loss\")  # Use the relevant metric for your task\n",
    "\n",
    "        if current_metric is None:\n",
    "            return\n",
    "\n",
    "        if self.best_metric is None or current_metric < self.best_metric - self.early_stopping_threshold:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.early_stopping_patience:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Early stopping at step {state.global_step} with best eval_loss = {self.best_metric}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback(), EarlyStoppingCallback( early_stopping_threshold=0.3)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuar a partir de um checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"models/FT-01\"\n",
    "latest_checkpoint = get_latest_checkpoint(output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444afffc3c0d4fbd9e463ab1d9992585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_dataset_chatml(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Pergunta: {row['pergunta']}\\nContexto: {row['contexto']}\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"{row['resposta']}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(\"emdemor/sql-create-context-pt\", split=\"train\").shuffle(seed=42).select(range(200))\n",
    "dataset_chatml = dataset.map(format_dataset_chatml).train_test_split(test_size=0.10, seed=1234)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:174: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f7823d50d041ab9e590945d3e04761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1548bd3d847f8b8d1995cdf9432cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias?\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")\n",
    "\n",
    "\n",
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    latest_checkpoint,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"#01\"></a>\n",
    "\n",
    "## A. Parâmetro `lr_scheduler_type`\n",
    "\n",
    "O parâmetro `lr_scheduler_type` é usado para especificar o tipo de agendador de taxa de aprendizado (Learning Rate Scheduler) a ser utilizado durante o treinamento do modelo. Um agendador de taxa de aprendizado ajusta a taxa de aprendizado ao longo do treinamento, o que pode ajudar a melhorar a convergência e o desempenho do modelo.\n",
    "\n",
    "\n",
    "\n",
    "A taxa de aprendizado é um dos hiperparâmetros mais importantes no treinamento de modelos de aprendizado de máquina. Ajustá-la dinamicamente durante o treinamento pode ajudar o modelo a aprender mais rapidamente no início e a estabilizar conforme se aproxima do mínimo da função de perda.\n",
    "\n",
    "Existem vários tipos de agendadores de taxa de aprendizado, e cada um tem uma estratégia diferente para ajustar a taxa de aprendizado ao longo do tempo. Abaixo, descrevo alguns dos tipos comuns de agendadores que podem ser especificados pelo parâmetro `lr_scheduler_type`:\n",
    "\n",
    "1. **Linear**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado de forma linear ao longo do treinamento.\n",
    "   - **Uso**: `linear`\n",
    "   - **Comportamento**: Começa com a taxa de aprendizado inicial e a diminui gradualmente até zero ao final do treinamento.\n",
    "   - **Exemplo**: Ideal para cenários onde se deseja uma diminuição constante da taxa de aprendizado.\n",
    "\n",
    "2. **Cosine**:\n",
    "   - **Descrição**: Usa uma função cosseno para diminuir a taxa de aprendizado.\n",
    "   - **Uso**: `cosine`\n",
    "   - **Comportamento**: A taxa de aprendizado diminui de forma não-linear, com uma curva suave.\n",
    "   - **Exemplo**: Útil para evitar grandes mudanças bruscas na taxa de aprendizado, promovendo uma convergência mais suave.\n",
    "\n",
    "3. **Cosine with Restarts**:\n",
    "   - **Descrição**: Variante do agendador cosseno, mas com reinicializações periódicas.\n",
    "   - **Uso**: `cosine_with_restarts`\n",
    "   - **Comportamento**: A taxa de aprendizado segue uma função cosseno, mas reinicia para a taxa inicial em intervalos definidos.\n",
    "   - **Exemplo**: Eficaz para evitar ficar preso em mínimos locais.\n",
    "\n",
    "4. **Polynomial**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado de acordo com uma função polinomial.\n",
    "   - **Uso**: `polynomial`\n",
    "   - **Comportamento**: A taxa de aprendizado decai de acordo com uma potência especificada.\n",
    "   - **Exemplo**: Fornece um controle mais preciso sobre o decaimento da taxa de aprendizado.\n",
    "\n",
    "5. **Constant**:\n",
    "   - **Descrição**: Mantém a taxa de aprendizado constante ao longo do treinamento.\n",
    "   - **Uso**: `constant`\n",
    "   - **Comportamento**: Não ajusta a taxa de aprendizado.\n",
    "   - **Exemplo**: Útil em cenários onde a taxa de aprendizado não precisa mudar.\n",
    "\n",
    "6. **Constant with Warmup**:\n",
    "   - **Descrição**: Mantém a taxa de aprendizado constante após um período inicial de aquecimento.\n",
    "   - **Uso**: `constant_with_warmup`\n",
    "   - **Comportamento**: Começa com uma taxa de aprendizado baixa e a aumenta gradualmente até a taxa desejada, mantendo-a constante após o aquecimento.\n",
    "   - **Exemplo**: Bom para estabilizar o treinamento no início.\n",
    "\n",
    "7. **Exponential**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado exponencialmente.\n",
    "   - **Uso**: `exponential`\n",
    "   - **Comportamento**: A taxa de aprendizado decresce de forma exponencial.\n",
    "   - **Exemplo**: Útil quando se deseja uma diminuição rápida da taxa de aprendizado.\n",
    "\n",
    "### Exemplo de Uso\n",
    "\n",
    "Aqui está um exemplo de configuração do `SFTConfig` usando o agendador de taxa de aprendizado linear:\n",
    "\n",
    "```python\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    output_dir=LOCAL_MODELPATH,\n",
    "    eval_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    log_level=\"debug\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    eval_steps=50,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",  # Tipo de agendador de taxa de aprendizado\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "```\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "O `lr_scheduler_type` permite controlar como a taxa de aprendizado será ajustada durante o treinamento, o que pode impactar significativamente a eficiência e o desempenho do modelo. Escolher o agendador correto depende da natureza do problema e das preferências específicas de treinamento."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5235696,
     "sourceId": 8724462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
