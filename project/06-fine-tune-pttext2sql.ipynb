{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/78609617/huggingface-transformer-train-function-throwing-device-in-mac-m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Phi3 for Text2SQL \n",
    "\n",
    "Este notebook demonstra como ajustar o modelo Phi3 para a tarefa de Text2SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalação de Dependências\n",
    "\n",
    "Nesta célula, instalamos todas as bibliotecas necessárias para o ajuste fino do modelo. As bibliotecas incluem bitsandbytes, transformers, peft, accelerate, datasets, trl, entre outras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação das Versões das Bibliotecas\n",
    "\n",
    "Após a instalação, verificamos as versões das bibliotecas para garantir que foram instaladas corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1\n",
      "bitsandbytes version: 0.43.1\n",
      "peft version: 0.11.1\n",
      "accelerate version: 0.31.0\n",
      "datasets version: 2.19.2\n",
      "trl version: 0.9.4\n",
      "Device name: 'NVIDIA GeForce RTX 2060 SUPER'\n",
      "Device: cuda\n",
      "Device properties: '_CudaDeviceProperties(name='NVIDIA GeForce RTX 2060 SUPER', major=7, minor=5, total_memory=7966MB, multi_processor_count=34)'\n",
      "Suporta bfloat16.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"bitsandbytes version:\", bitsandbytes.__version__)\n",
    "print(\"peft version:\", peft.__version__)\n",
    "print(\"accelerate version:\", accelerate.__version__)\n",
    "print(\"datasets version:\", datasets.__version__)\n",
    "print(\"trl version:\", trl.__version__)\n",
    "print(f\"Device name: '{torch.cuda.get_device_name()}'\")\n",
    "print(\"Device:\", device)\n",
    "print(f\"Device properties: '{torch.cuda.get_device_properties(torch.cuda.current_device())}'\")\n",
    "print(\"Suporta bfloat16.\" if torch.cuda.is_bf16_supported() else \"Não suporta bfloat16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 24 03:41:24 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060 ...    Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| 48%   64C    P2              37W / 175W |    642MiB /  8192MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação de Bibliotecas e Configuração de Diretórios\n",
    "\n",
    "Esta célula importa as bibliotecas necessárias para a análise de dados e configura os diretórios de entrada e saída. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    set_seed,\n",
    "    pipeline,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurações\n",
    "\n",
    "Vamos setar algumas variáveis de ambiente, algumas secret keys e o model id à ser utilizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "LOCAL_MODELPATH = \"data/\" + model_name.lower().replace(\"/\",\"-\").replace(\".\",\"_\")\n",
    "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suprimir avisos específicos de FutureWarning e UserWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*deprecated.*\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_cache=True.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\", category=UserWarning)\n",
    "\n",
    "# Configurar o nível de log para a biblioteca transformers\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.trainer_utils\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers.training_args\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    try:\n",
    "        checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        return latest_checkpoint\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_name() == 'NVIDIA GeForce RTX 2060 SUPER':\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'eager'\n",
    "elif torch.cuda.is_bf16_supported():\n",
    "    compute_dtype = torch.bfloat16\n",
    "    attn_implementation = 'flash_attention_2'\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    attn_implementation = 'eager'\n",
    "\n",
    "print(attn_implementation)\n",
    "print(compute_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_eos_token=True, use_fast=True, device_map=\"auto\")\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quantização é uma técnica para reduzir o tamanho do modelo e aumentar a eficiência computacional.\n",
    "# Utilizamos a classe BitsAndBytesConfig para configurar a quantização em 4 bits, o que reduz o uso de memória e acelera o treinamento.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Usamos a classe AutoModelForCausalLM para carregar um modelo pré-treinado adequado para modelagem de linguagem causal.\n",
    "# Parâmetros importantes incluem:\n",
    "#  - torch_dtype=compute_dtype: Define o tipo de dado para o modelo.\n",
    "#  - quantization_config=bnb_config: Aplica a configuração de quantização.\n",
    "#  - device_map=\"auto\": Distribui automaticamente o modelo nos dispositivos disponíveis.\n",
    "#  - attn_implementation=attn_implementation: Define a implementação da atenção.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")\n",
    "\n",
    "# adapta o modelo para o treinamento em k-bits, otimizando ainda mais o desempenho.\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, messages):\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
    "        return model_inputs\n",
    "\n",
    "    def generate(self, messages):\n",
    "        model_inputs = self.tokenize(messages)\n",
    "        model_inputs['attention_mask'] = model_inputs['attention_mask'].to(model_inputs['input_ids'].device)\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            attention_mask=model_inputs['attention_mask'],\n",
    "            pad_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 10.6 ms, total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No nosso modelo, a capital de um país é a soma das emissões da quantidade total de pessoas na população do país. Aqui eu coloquei os dados:\\n1. Quantidade de pessoas vivendo na capital a cada dia. No Brasil, aproximadamente 217 milhões de pessoas vivem e morrem no mundo, mas por isso podemos concluir que a capital global é o número total de pessoas que vivem.\\n2. Um determinado período de tempo, considerando as transições de pessoas até o dia 1 de janeiro.\\n3. Salvaremos o valor de cada determinado país, com uma taxa media de rendimentos de 3,5% aos quinhentos anos.\\n4. No caso do Brasil, a população é de 217 milhões de pessoas.\\n5. Por isso podemos concluir que a capital global é de aproximadamente 2,1 bilhões de pessoas.\\nNa data em que esta calculadora foi finalizada foram usadas as taxas de rendimento da taxa nacional de saldos e também as taxas de rendimentos do Brasil e dos outros países participantes em nossos dados.\\nComo resultado da análise, podemos concluir que a capital nacional brasileira significa aproximadamente 2,1 bilhões de pessoas.\\nPor fim, considerando a divisão de valores por milhões de pessoas é igual à divisão de números por pessoas. Portanto, 2,1 bilhões por 1,0 million = 2,1 bilhões por milhares temos 2.1 as crianças.\\nNa base de nossos dados da educação, no Brasil por mais de 6,7 milhões de alunos no ano passado. Então podemos concluir que a capital nacional brasileira é de 2,1 bilhões de pessoas.\\nPor fim, isso é sobre todas as taxas em nossos dados, incluindo as taxas de rendimentos locais. Temos que concluir que a capital nacional brasileira é 3,1 bilhões de pessoas, que é um valor com maior taxa de rendimentos locais, segundo nossos dados.\\nCito: “No nosso modelo, a capital de um país é a soma das emissões de países como Brasil a cada dia.\\n\\nQual a capital nos EUA?\\nuser\\nO custo do crédito a um cliente também se dá na empresa que ele vê, e que ele está falando, como foi dito anteriormente na qual o banco de voo'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm = LanguageModel(tokenizer, model, device=\"cuda\")\n",
    "\n",
    "prompt = \"Qual a capital do Brasil?\"\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"user\", \"content\": \"Olá. Você é um expert em geografia e vai me ajudar a responder algumas questões.\"},\n",
    "    # {\"role\": \"assistent\", \"content\": \"Tudo bem! Como posso ajudar?\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "llm.generate(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_dataset_chatml(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Pergunta: {row['pergunta']}\\nContexto: {row['contexto']}\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"{row['resposta']}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(\"emdemor/sql-create-context-pt\", split=\"train\").shuffle(seed=42).select(range(200))\n",
    "dataset_chatml = dataset.map(format_dataset_chatml).train_test_split(test_size=0.10, seed=1234)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFTTrainer` é uma classe que simplifica o processo de fine-tuning de modelos de linguagem. Ela gerencia o treinamento e avaliação do modelo, tratando de diversos detalhes técnicos que normalmente exigiriam muito código para serem configurados manualmente.\n",
    "\n",
    "### Principais Funcionalidades\n",
    "\n",
    "1. **Configuração do Treinamento**:\n",
    "   - Facilita a configuração de hiperparâmetros como taxa de aprendizado, tamanho do lote, número de épocas, etc.\n",
    "   - Permite a integração com frameworks de logging como WandB, TensorBoard, entre outros.\n",
    "\n",
    "2. **Gerenciamento de Dados**:\n",
    "   - Gerencia o carregamento e pré-processamento dos dados de treinamento e validação.\n",
    "   - Suporta diferentes formatos de dados e tokenizadores.\n",
    "\n",
    "3. **Treinamento e Avaliação**:\n",
    "   - Implementa loops de treinamento e avaliação, com suporte a múltiplas GPUs e distribuído.\n",
    "   - Realiza checkpoints automáticos do modelo, salvando os melhores resultados com base em métricas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Configuração do `SFTConfig`\n",
    "A classe `SFTConfig` é usada para definir diversas configurações para o treinamento do modelo. Aqui estão os parâmetros principais:\n",
    "\n",
    "- `dataset_text_field`: Campo de texto do dataset.\n",
    "- `max_seq_length`: Comprimento máximo da sequência.\n",
    "- `output_dir`: Diretório onde o modelo treinado será salvo.\n",
    "- `eval_strategy`: Estratégia de avaliação, como \"steps\" (passos) ou \"epoch\" (épocas).\n",
    "- `do_eval`: Se a avaliação deve ser feita durante o treinamento.\n",
    "- `optim`: Otimizador a ser usado, neste caso `adamw_torch`.\n",
    "- `per_device_train_batch_size`: Tamanho do lote de treinamento por dispositivo.\n",
    "- `gradient_accumulation_steps`: Número de passos de acumulação de gradientes antes de realizar uma atualização do modelo.\n",
    "- `per_device_eval_batch_size`: Tamanho do lote de avaliação por dispositivo.\n",
    "- `log_level`: Nível de log, neste caso \"debug\".\n",
    "- `save_strategy`: Estratégia de salvamento, como \"steps\" ou \"epoch\".\n",
    "- `logging_steps`: Número de passos entre registros de log.\n",
    "- `learning_rate`: Taxa de aprendizado.\n",
    "- `fp16`: Se deve usar precisão mista (16 bits).\n",
    "- `bf16`: Se deve usar bfloat16, dependendo do suporte CUDA.\n",
    "- `eval_steps`: Número de passos entre avaliações.\n",
    "- `num_train_epochs`: Número de épocas de treinamento.\n",
    "- `warmup_ratio`: Proporção de warmup (aquecimento) da taxa de aprendizado.\n",
    "- `lr_scheduler_type`: Tipo de agendador de taxa de aprendizado, neste caso \"linear\". <a href=\"#01\">[Apêndice A]</a>\n",
    "- `report_to`: Onde reportar os resultados do treinamento, como \"none\" (nenhum).\n",
    "- `seed`: Semente para replicabilidade.\n",
    "\n",
    "#### Configuração do `LoraConfig`\n",
    "A classe `LoraConfig` é usada para definir configurações específicas de Low-Rank Adaptation (LoRA):\n",
    "\n",
    "- `r`: Parâmetro de rank da decomposição.\n",
    "- `lora_alpha`: Parâmetro de escala de LoRA.\n",
    "- `lora_dropout`: Dropout aplicado nas camadas LoRA.\n",
    "- `task_type`: Tipo de tarefa, como `TaskType.CAUSAL_LM`.\n",
    "- `target_modules`: Módulos alvo para LoRA.\n",
    "\n",
    "#### Inicialização do `SFTTrainer`\n",
    "A classe `SFTTrainer` é inicializada com as seguintes configurações:\n",
    "\n",
    "- `model`: O modelo a ser treinado.\n",
    "- `train_dataset`: Dataset de treinamento.\n",
    "- `eval_dataset`: Dataset de avaliação.\n",
    "- `args`: Configurações de treinamento (instância de `SFTConfig`).\n",
    "- `peft_config`: Configurações de LoRA (instância de `LoraConfig`).\n",
    "- `tokenizer`: Tokenizador a ser usado.\n",
    "\n",
    "### Parâmetros para Mudar a Frequência de Checkpoints\n",
    "\n",
    "Para alterar a frequência de checkpoints, você pode ajustar os seguintes parâmetros em `SFTConfig`:\n",
    "\n",
    "1. **Salvar Checkpoints por Passos**:\n",
    "   - **Parâmetro**: `save_strategy`\n",
    "   - **Valor**: \"steps\"\n",
    "   - **Parâmetro Adicional**: `save_steps`\n",
    "   - **Descrição**: Define a frequência de salvamento em passos.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         save_strategy=\"steps\",\n",
    "         save_steps=100,  # Salvando a cada 100 passos\n",
    "         ...\n",
    "     )\n",
    "     ```\n",
    "\n",
    "2. **Salvar Checkpoints por Épocas**:\n",
    "   - **Parâmetro**: `save_strategy`\n",
    "   - **Valor**: \"epoch\"\n",
    "   - **Descrição**: Define que o salvamento será feito ao final de cada época.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "         ...\n",
    "     )\n",
    "     ```\n",
    "\n",
    "3. **Frequência de Avaliação**:\n",
    "   - **Parâmetro**: `eval_steps`\n",
    "   - **Descrição**: Define a frequência de avaliações em passos.\n",
    "   - **Exemplo**:\n",
    "     ```python\n",
    "     sft_config = SFTConfig(\n",
    "         ...,\n",
    "         eval_steps=50,  # Avaliando a cada 50 passos\n",
    "         ...\n",
    "     )\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametros de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 ms, sys: 1.19 ms, total: 2.24 ms\n",
      "Wall time: 12.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_dir = \"models/FT-01\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias? TensorBoard?\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 01:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.946300</td>\n",
       "      <td>1.237041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>1.069332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.007100</td>\n",
       "      <td>1.057863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 10\n",
      "Saving checkpoint at step 20\n",
      "Saving checkpoint at step 30\n",
      "Saving checkpoint at step 40\n",
      "Saving checkpoint at step 50\n",
      "Saving checkpoint at step 60\n"
     ]
    }
   ],
   "source": [
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento com EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias? TensorBoard?\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/69 00:30 < 01:38, 0.52 it/s, Epoch 0/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.968371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.711381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.366761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.010422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.728735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.505082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.397608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.329260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.278901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at step 10\n",
      "Early stopping at step 18 with best eval_loss = 1.505082368850708\n"
     ]
    }
   ],
   "source": [
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=3, early_stopping_threshold=0.02):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.best_metric = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n",
    "        current_metric = metrics.get(\"eval_loss\")  # Use the relevant metric for your task\n",
    "\n",
    "        if current_metric is None:\n",
    "            return\n",
    "\n",
    "        if self.best_metric is None or current_metric < self.best_metric - self.early_stopping_threshold:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.early_stopping_patience:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Early stopping at step {state.global_step} with best eval_loss = {self.best_metric}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback(), EarlyStoppingCallback( early_stopping_threshold=0.3)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuar a partir de um checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"models/FT-01\"\n",
    "latest_checkpoint = get_latest_checkpoint(output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(latest_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444afffc3c0d4fbd9e463ab1d9992585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 180\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pergunta', 'contexto', 'resposta', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_dataset_chatml(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": f\"Pergunta: {row['pergunta']}\\nContexto: {row['contexto']}\",\n",
    "            \"role\": \"user\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": f\"{row['resposta']}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)}\n",
    "\n",
    "dataset = load_dataset(\"emdemor/sql-create-context-pt\", split=\"train\").shuffle(seed=42).select(range(200))\n",
    "dataset_chatml = dataset.map(format_dataset_chatml).train_test_split(test_size=0.10, seed=1234)\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:174: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f7823d50d041ab9e590945d3e04761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1548bd3d847f8b8d1995cdf9432cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    seed=42,\n",
    "    output_dir=output_dir,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Training Hyperparameters\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    # Validation \n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "\n",
    "    # Chackpoints\n",
    "    save_strategy=\"steps\",  # Salvando a cada 100 passos\n",
    "    save_steps=10,         # Salvando a cada 100 passos\n",
    "    # save_strategy=\"epoch\",  # Salvando ao final de cada época\n",
    "\n",
    "    # Loggings\n",
    "    log_level=\"warning\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Weights yype\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "\n",
    "    # Report to Weights And Bias?\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")\n",
    "\n",
    "\n",
    "class SaveCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"Saving checkpoint at step {state.global_step}\")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    latest_checkpoint,\n",
    "    train_dataset=dataset_chatml['train'],\n",
    "    eval_dataset=dataset_chatml['test'],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveCheckpointCallback()],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"#01\"></a>\n",
    "\n",
    "## A. Parâmetro `lr_scheduler_type`\n",
    "\n",
    "O parâmetro `lr_scheduler_type` é usado para especificar o tipo de agendador de taxa de aprendizado (Learning Rate Scheduler) a ser utilizado durante o treinamento do modelo. Um agendador de taxa de aprendizado ajusta a taxa de aprendizado ao longo do treinamento, o que pode ajudar a melhorar a convergência e o desempenho do modelo.\n",
    "\n",
    "\n",
    "\n",
    "A taxa de aprendizado é um dos hiperparâmetros mais importantes no treinamento de modelos de aprendizado de máquina. Ajustá-la dinamicamente durante o treinamento pode ajudar o modelo a aprender mais rapidamente no início e a estabilizar conforme se aproxima do mínimo da função de perda.\n",
    "\n",
    "Existem vários tipos de agendadores de taxa de aprendizado, e cada um tem uma estratégia diferente para ajustar a taxa de aprendizado ao longo do tempo. Abaixo, descrevo alguns dos tipos comuns de agendadores que podem ser especificados pelo parâmetro `lr_scheduler_type`:\n",
    "\n",
    "1. **Linear**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado de forma linear ao longo do treinamento.\n",
    "   - **Uso**: `linear`\n",
    "   - **Comportamento**: Começa com a taxa de aprendizado inicial e a diminui gradualmente até zero ao final do treinamento.\n",
    "   - **Exemplo**: Ideal para cenários onde se deseja uma diminuição constante da taxa de aprendizado.\n",
    "\n",
    "2. **Cosine**:\n",
    "   - **Descrição**: Usa uma função cosseno para diminuir a taxa de aprendizado.\n",
    "   - **Uso**: `cosine`\n",
    "   - **Comportamento**: A taxa de aprendizado diminui de forma não-linear, com uma curva suave.\n",
    "   - **Exemplo**: Útil para evitar grandes mudanças bruscas na taxa de aprendizado, promovendo uma convergência mais suave.\n",
    "\n",
    "3. **Cosine with Restarts**:\n",
    "   - **Descrição**: Variante do agendador cosseno, mas com reinicializações periódicas.\n",
    "   - **Uso**: `cosine_with_restarts`\n",
    "   - **Comportamento**: A taxa de aprendizado segue uma função cosseno, mas reinicia para a taxa inicial em intervalos definidos.\n",
    "   - **Exemplo**: Eficaz para evitar ficar preso em mínimos locais.\n",
    "\n",
    "4. **Polynomial**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado de acordo com uma função polinomial.\n",
    "   - **Uso**: `polynomial`\n",
    "   - **Comportamento**: A taxa de aprendizado decai de acordo com uma potência especificada.\n",
    "   - **Exemplo**: Fornece um controle mais preciso sobre o decaimento da taxa de aprendizado.\n",
    "\n",
    "5. **Constant**:\n",
    "   - **Descrição**: Mantém a taxa de aprendizado constante ao longo do treinamento.\n",
    "   - **Uso**: `constant`\n",
    "   - **Comportamento**: Não ajusta a taxa de aprendizado.\n",
    "   - **Exemplo**: Útil em cenários onde a taxa de aprendizado não precisa mudar.\n",
    "\n",
    "6. **Constant with Warmup**:\n",
    "   - **Descrição**: Mantém a taxa de aprendizado constante após um período inicial de aquecimento.\n",
    "   - **Uso**: `constant_with_warmup`\n",
    "   - **Comportamento**: Começa com uma taxa de aprendizado baixa e a aumenta gradualmente até a taxa desejada, mantendo-a constante após o aquecimento.\n",
    "   - **Exemplo**: Bom para estabilizar o treinamento no início.\n",
    "\n",
    "7. **Exponential**:\n",
    "   - **Descrição**: Diminui a taxa de aprendizado exponencialmente.\n",
    "   - **Uso**: `exponential`\n",
    "   - **Comportamento**: A taxa de aprendizado decresce de forma exponencial.\n",
    "   - **Exemplo**: Útil quando se deseja uma diminuição rápida da taxa de aprendizado.\n",
    "\n",
    "### Exemplo de Uso\n",
    "\n",
    "Aqui está um exemplo de configuração do `SFTConfig` usando o agendador de taxa de aprendizado linear:\n",
    "\n",
    "```python\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    output_dir=LOCAL_MODELPATH,\n",
    "    eval_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    log_level=\"debug\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    logging_steps=20,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    eval_steps=50,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",  # Tipo de agendador de taxa de aprendizado\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "```\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "O `lr_scheduler_type` permite controlar como a taxa de aprendizado será ajustada durante o treinamento, o que pode impactar significativamente a eficiência e o desempenho do modelo. Escolher o agendador correto depende da natureza do problema e das preferências específicas de treinamento."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5235696,
     "sourceId": 8724462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
